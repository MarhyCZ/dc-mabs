{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo3xMe1_OGq0"
      },
      "source": [
        "## Install deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks0Y6NTkLv6n",
        "outputId": "af736d39-3726-4294-8a12-7d8b4f22a17a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/site-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/site-packages (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (2.32.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/site-packages (3.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /root/.local/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/site-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/site-packages (from matplotlib) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /root/.local/lib/python3.11/site-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/site-packages (from matplotlib) (10.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/site-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /root/.local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: tf-agents in /usr/local/lib/python3.11/site-packages (0.19.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.11/site-packages (from tf-agents) (2.1.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.11/site-packages (from tf-agents) (3.0.0)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.11/site-packages (from tf-agents) (0.5.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.11/site-packages (from tf-agents) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/site-packages (from tf-agents) (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/site-packages (from tf-agents) (10.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /root/.local/lib/python3.11/site-packages (from tf-agents) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.11/site-packages (from tf-agents) (4.25.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.11/site-packages (from tf-agents) (1.16.0)\n",
            "Collecting typing-extensions==4.5.0 (from tf-agents)\n",
            "  Using cached typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.11/site-packages (from tf-agents) (2.1.3)\n",
            "Requirement already satisfied: tensorflow-probability~=0.23.0 in /usr/local/lib/python3.11/site-packages (from tf-agents) (0.23.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/site-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\n",
            "Requirement already satisfied: decorator in /root/.local/lib/python3.11/site-packages (from tensorflow-probability~=0.23.0->tf-agents) (5.1.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.11/site-packages (from tensorflow-probability~=0.23.0->tf-agents) (0.5.4)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/site-packages (from tensorflow-probability~=0.23.0->tf-agents) (0.1.8)\n",
            "Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: typing-extensions\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 8.25.0 requires typing-extensions>=4.6; python_version < \"3.12\", but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed typing-extensions-4.5.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: tf-keras in /usr/local/lib/python3.11/site-packages (2.16.0)\n",
            "Requirement already satisfied: tensorflow<2.17,>=2.16 in /usr/local/lib/python3.11/site-packages (from tf-keras) (2.16.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.3.0)\n",
            "Requirement already satisfied: packaging in /root/.local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (4.25.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (69.0.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /root/.local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.3.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.37.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tf-keras) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2024.6.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/.local/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.1.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy requests matplotlib\n",
        "!pip install tf-agents\n",
        "!pip install tf-keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "# Keep using keras-2 (tf-keras) rather than keras-3 (keras).\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmgd_l0kMaY_"
      },
      "source": [
        "### Download the dataset from my netlify deployment. As Collab is ephemeral\n",
        "And I am lazy to spin up new nginx on my traefik host at home"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Onra9Id-MZvd"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "base_url = \"https://dc-bandits-ab-06e963.netlify.app/dataset-\"\n",
        "files = [\"zakladni.csv\",\"discovery.csv\",\"segment.csv\"]\n",
        "\n",
        "for file in files:\n",
        "  response = requests.get(base_url+file, stream=True)\n",
        "  with open(file,'wb') as output:\n",
        "    output.write(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7cNJJ20Og0J"
      },
      "source": [
        "## Data preparation\n",
        "Aggregate the data by 30 minutes intervals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RWHIp_gLryU",
        "outputId": "7805e9c9-c767-4901-a9cd-3e261be6c85a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "215\n",
            "215\n",
            "215\n",
            "                     visits  clicks   variant\n",
            "interval                                     \n",
            "2023-03-06 16:00:00     283       1  zakladni\n",
            "2023-03-06 18:00:00     373       7  zakladni\n",
            "2023-03-06 20:00:00     276       3  zakladni\n",
            "2023-03-06 22:00:00     136       5  zakladni\n",
            "2023-03-07 00:00:00      71       3  zakladni\n",
            "                     visits  clicks  variant\n",
            "interval                                    \n",
            "2023-03-24 04:00:00      79       1  segment\n",
            "2023-03-24 06:00:00     173       3  segment\n",
            "2023-03-24 08:00:00     299       6  segment\n",
            "2023-03-24 10:00:00     403      11  segment\n",
            "2023-03-24 12:00:00     245      14  segment\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_6868/1509006101.py:9: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  variant = variant.resample('2H').agg({'visits':'sum', 'clicks':'sum'}).dropna()\n",
            "/tmp/ipykernel_6868/1509006101.py:9: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  variant = variant.resample('2H').agg({'visits':'sum', 'clicks':'sum'}).dropna()\n",
            "/tmp/ipykernel_6868/1509006101.py:9: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  variant = variant.resample('2H').agg({'visits':'sum', 'clicks':'sum'}).dropna()\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.DataFrame()\n",
        "for file in files:\n",
        "  variant = pd.read_csv(file)\n",
        "  variant['minute'] = pd.to_datetime(variant['minute'])\n",
        "  variant.set_index('minute', inplace=True)\n",
        "  variant = variant.resample('2H').agg({'visits':'sum', 'clicks':'sum'}).dropna()\n",
        "\n",
        "  variant.index.rename(\"interval\", inplace=True)\n",
        "  variant['variant'] = file.replace(\".csv\", \"\")\n",
        "  print(len(variant))\n",
        "  data = pd.concat([data, variant])\n",
        "\n",
        "print(data.head())\n",
        "print(data.tail())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HENfkL8ECALv"
      },
      "source": [
        "## Prepare Tensorflow Bandit Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "01srm6QcC-9r"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-06-14 18:07:15.708165: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-06-14 18:07:15.708785: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-06-14 18:07:15.711070: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-06-14 18:07:15.737077: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-14 18:07:16.179195: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.bandits.environments.bandit_py_environment import BanditPyEnvironment\n",
        "from tf_agents.specs import array_spec\n",
        "\n",
        "\n",
        "class ABCTestBanditEnv(BanditPyEnvironment):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.current = 0\n",
        "\n",
        "        # Num of A/B variants (actions)\n",
        "        self.variants = data['variant'].unique()\n",
        "        action_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(), dtype=np.int32, minimum=0, maximum=len(self.variants) - 1, name='action')\n",
        "\n",
        "        # Define observation as a vector of the day of the week (0-6) and hour of the day (0-23)\n",
        "        observation_spec = array_spec.ArraySpec(\n",
        "            shape=(2,), dtype=np.int32, name='observation')\n",
        "\n",
        "        super(ABCTestBanditEnv, self).__init__(observation_spec, action_spec)\n",
        "\n",
        "        # Precompute CTR for each variant\n",
        "        self.data['CTR'] = self.data['clicks'] / self.data['visits']\n",
        "\n",
        "        # Normalize day and hour for observations\n",
        "        self.data['day_of_week'] = self.data.index.dayofweek\n",
        "        self.data['hour_of_day'] = self.data.index.hour\n",
        "\n",
        "    def _observe(self):\n",
        "        if self.current >= len(self.data):\n",
        "            self.current = 0  # reset to loop through the data\n",
        "        # Construct the observation from the current interval\n",
        "        obs_data = self.data.iloc[self.current]\n",
        "        self._observation = np.array([obs_data['day_of_week'], obs_data['hour_of_day']], dtype='int32')\n",
        "        return self._observation\n",
        "\n",
        "    def _apply_action(self, action):\n",
        "        # Get the current time interval data\n",
        "        obs_data = self.data.iloc[self.current]\n",
        "        current_time = obs_data.name\n",
        "\n",
        "        current_interval_data = self.data.loc[current_time]\n",
        "        total_visits = current_interval_data['visits'].sum()\n",
        "\n",
        "        # Allocate traffic based on action taken\n",
        "        chosen_variant = self.variants[action]\n",
        "        chosen_variant_data = current_interval_data[current_interval_data['variant'] == chosen_variant]\n",
        "\n",
        "        # Calculate the new visits and clicks based on the chosen variant's CTR\n",
        "        new_clicks = total_visits * chosen_variant_data['CTR'].values[0]\n",
        "\n",
        "        # Calculate reward as the CTR\n",
        "        reward = new_clicks / total_visits if total_visits > 0 else 0\n",
        "        print(reward)\n",
        "        self.current += 1\n",
        "        return reward\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyUvUqz1NNYi",
        "outputId": "d333a57c-51b7-47e6-d9a6-d1573264d0c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0035335689045936395\n",
            "WARNING:tensorflow:From /tmp/ipykernel_6868/960868158.py:50: ReplayBuffer.gather_all (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=True)` instead.\n",
            "Step: 0, Loss: 1.2486108971643262e-05\n",
            "0.034210526315789476\n",
            "Step: 1, Loss: 0.001170360017567873\n",
            "0.015723270440251572\n",
            "Step: 2, Loss: 0.00024722126545384526\n",
            "0.03676470588235294\n",
            "Step: 3, Loss: 0.0010192027548328042\n",
            "0.042253521126760556\n",
            "Step: 4, Loss: 0.0017853599274531007\n",
            "0.08333333333333333\n",
            "Step: 5, Loss: 0.006327225361019373\n",
            "0.046511627906976744\n",
            "Step: 6, Loss: 0.0018813745118677616\n",
            "0.04861111111111111\n",
            "Step: 7, Loss: 1.4028770237928256e-05\n",
            "0.01929260450160772\n",
            "Step: 8, Loss: 0.00012492098903749138\n",
            "0.014388489208633094\n",
            "Step: 9, Loss: 0.00028350844513624907\n",
            "0.03571428571428571\n",
            "Step: 10, Loss: 0.0007774344994686544\n",
            "0.0030303030303030303\n",
            "Step: 11, Loss: 0.002862703986465931\n",
            "0.010554089709762533\n",
            "Step: 12, Loss: 0.00035286095226183534\n",
            "0.0226628895184136\n",
            "Step: 13, Loss: 0.00021294796897564083\n",
            "0.027700831024930747\n",
            "Step: 14, Loss: 0.00029404647648334503\n",
            "0.00847457627118644\n",
            "Step: 15, Loss: 0.00027981348102912307\n",
            "0.014492753623188408\n",
            "Step: 16, Loss: 0.00015842750144656748\n",
            "0.0\n",
            "Step: 17, Loss: 0.0033667024690657854\n",
            "0.016666666666666666\n",
            "Step: 18, Loss: 0.0002953289949800819\n",
            "0.0064516129032258064\n",
            "Step: 19, Loss: 0.0004181385738775134\n",
            "0.017142857142857144\n",
            "Step: 20, Loss: 3.230291622458026e-05\n",
            "0.009876543209876543\n",
            "Step: 21, Loss: 0.0007749128271825612\n",
            "0.026634382566585957\n",
            "Step: 22, Loss: 1.917845082743952e-07\n",
            "0.008714596949891068\n",
            "Step: 23, Loss: 0.000679834047332406\n",
            "0.031630170316301706\n",
            "Step: 24, Loss: 5.437812433228828e-05\n",
            "0.009836065573770493\n",
            "Step: 25, Loss: 0.0005130894714966416\n",
            "0.03557312252964427\n",
            "Step: 26, Loss: 1.7904500282384106e-07\n",
            "0.030303030303030304\n",
            "Step: 27, Loss: 2.4911228138080332e-06\n",
            "0.018867924528301886\n",
            "Step: 28, Loss: 5.723377967115084e-07\n",
            "0.5263157894736842\n",
            "Step: 29, Loss: 0.24644985795021057\n",
            "0.08333333333333333\n",
            "Step: 30, Loss: 0.02260592207312584\n",
            "0.013513513513513514\n",
            "Step: 31, Loss: 0.02914297580718994\n",
            "0.015337423312883436\n",
            "Step: 32, Loss: 6.814490916440263e-05\n",
            "0.011961722488038277\n",
            "Step: 33, Loss: 0.01568613573908806\n",
            "0.020942408376963352\n",
            "Step: 34, Loss: 0.008226573467254639\n",
            "0.022727272727272728\n",
            "Step: 35, Loss: 1.2798390343959909e-05\n",
            "0.025495750708215296\n",
            "Step: 36, Loss: 0.00014101239503361285\n",
            "0.0353356890459364\n",
            "Step: 37, Loss: 0.0021634958684444427\n",
            "0.030075187969924814\n",
            "Step: 38, Loss: 7.476729706468177e-07\n",
            "0.006172839506172839\n",
            "Step: 39, Loss: 0.001260325196199119\n",
            "0.012345679012345678\n",
            "Step: 40, Loss: 0.026206770911812782\n",
            "0.03571428571428571\n",
            "Step: 41, Loss: 0.0002943698491435498\n",
            "0.019230769230769232\n",
            "Step: 42, Loss: 1.319192392656987e-06\n",
            "0.007575757575757577\n",
            "Step: 43, Loss: 0.012240001931786537\n",
            "0.017341040462427744\n",
            "Step: 44, Loss: 0.00016350844816770405\n",
            "0.04455445544554455\n",
            "Step: 45, Loss: 0.0003634509921539575\n",
            "0.008955223880597015\n",
            "Step: 46, Loss: 0.006676391698420048\n",
            "0.009836065573770493\n",
            "Step: 47, Loss: 0.004533675499260426\n",
            "0.013157894736842105\n",
            "Step: 48, Loss: 0.00046755970106460154\n",
            "0.025210084033613446\n",
            "Step: 49, Loss: 0.00011372421431588009\n",
            "0.014925373134328358\n",
            "Step: 50, Loss: 0.0020605772733688354\n",
            "0.007142857142857143\n",
            "Step: 51, Loss: 0.0008411280578002334\n",
            "0.029850746268656712\n",
            "Step: 52, Loss: 0.0064002033323049545\n",
            "0.0\n",
            "Step: 53, Loss: 0.0006349613540805876\n",
            "0.05555555555555555\n",
            "Step: 54, Loss: 0.0011093871435150504\n",
            "0.08620689655172414\n",
            "Step: 55, Loss: 4.069666829309426e-06\n",
            "0.021621621621621623\n",
            "Step: 56, Loss: 0.00014298483438324183\n",
            "0.015267175572519083\n",
            "Step: 57, Loss: 0.00010890996782109141\n",
            "0.02510460251046025\n",
            "Step: 58, Loss: 0.0027715303003787994\n",
            "0.024054982817869417\n",
            "Step: 59, Loss: 0.002093337941914797\n",
            "0.012195121951219513\n",
            "Step: 60, Loss: 0.0003254544280935079\n",
            "0.011583011583011582\n",
            "Step: 61, Loss: 0.0006125649670138955\n",
            "0.02127659574468085\n",
            "Step: 62, Loss: 0.0013932286528870463\n",
            "0.0457516339869281\n",
            "Step: 63, Loss: 0.0001388239033985883\n",
            "0.0945945945945946\n",
            "Step: 64, Loss: 0.004253237042576075\n",
            "0.054054054054054064\n",
            "Step: 65, Loss: 0.00147730833850801\n",
            "0.023809523809523808\n",
            "Step: 66, Loss: 3.1705116271041334e-05\n",
            "0.021739130434782608\n",
            "Step: 67, Loss: 0.0006865880568511784\n",
            "0.020512820512820513\n",
            "Step: 68, Loss: 0.003455973928794265\n",
            "0.03018867924528302\n",
            "Step: 69, Loss: 0.00017976890376303345\n",
            "0.033707865168539325\n",
            "Step: 70, Loss: 3.227076376788318e-05\n",
            "0.013452914798206279\n",
            "Step: 71, Loss: 0.0028869984671473503\n",
            "0.019083969465648856\n",
            "Step: 72, Loss: 0.0005413879989646375\n",
            "0.036290322580645164\n",
            "Step: 73, Loss: 1.2715925095108105e-06\n",
            "0.036\n",
            "Step: 74, Loss: 0.00044268317287787795\n",
            "0.04225352112676057\n",
            "Step: 75, Loss: 4.385477950563654e-06\n",
            "0.0\n",
            "Step: 76, Loss: 0.0\n",
            "0.0\n",
            "Step: 77, Loss: 4.460390755411936e-06\n",
            "0.0\n",
            "Step: 78, Loss: 1.777142460923642e-05\n",
            "0.006993006993006993\n",
            "Step: 79, Loss: 5.169484325051599e-07\n",
            "0.010101010101010102\n",
            "Step: 80, Loss: 2.955155650852248e-06\n",
            "0.015037593984962405\n",
            "Step: 81, Loss: 2.0215378754073754e-05\n",
            "0.016166281755196306\n",
            "Step: 82, Loss: 1.073597013601102e-05\n",
            "0.014150943396226417\n",
            "Step: 83, Loss: 1.248145508725429e-06\n",
            "0.07235142118863049\n",
            "Step: 84, Loss: 0.003024867968633771\n",
            "0.017341040462427744\n",
            "Step: 85, Loss: 0.0002584308967925608\n",
            "0.03619909502262444\n",
            "Step: 86, Loss: 6.922413740539923e-05\n",
            "0.05217391304347826\n",
            "Step: 87, Loss: 0.0004152843321207911\n",
            "0.043478260869565216\n",
            "Step: 88, Loss: 0.0009789420291781425\n",
            "0.02857142857142857\n",
            "Step: 89, Loss: 0.00032791486592032015\n",
            "0.03773584905660377\n",
            "Step: 90, Loss: 0.0009083434706553817\n",
            "0.013986013986013986\n",
            "Step: 91, Loss: 4.4607193558476865e-05\n",
            "0.016722408026755852\n",
            "Step: 92, Loss: 8.0505182268098e-05\n",
            "0.009925558312655087\n",
            "Step: 93, Loss: 2.604514065751573e-06\n",
            "0.011764705882352941\n",
            "Step: 94, Loss: 8.843085197440814e-06\n",
            "0.018604651162790697\n",
            "Step: 95, Loss: 8.547951438231394e-05\n",
            "0.020594965675057208\n",
            "Step: 96, Loss: 0.00010497847688384354\n",
            "0.0215633423180593\n",
            "Step: 97, Loss: 6.737819057889283e-05\n",
            "0.0165016501650165\n",
            "Step: 98, Loss: 0.0005268632667139173\n",
            "0.029411764705882353\n",
            "Step: 99, Loss: 0.00027348060393705964\n",
            "0.047619047619047616\n",
            "Step: 100, Loss: 0.0005505596636794508\n",
            "0.05128205128205128\n",
            "Step: 101, Loss: 0.0008050950709730387\n",
            "0.0\n",
            "Step: 102, Loss: 0.00046326962183229625\n",
            "0.02097902097902098\n",
            "Step: 103, Loss: 7.711220678174868e-05\n",
            "0.019830028328611898\n",
            "Step: 104, Loss: 1.9608496586442925e-05\n",
            "0.0234192037470726\n",
            "Step: 105, Loss: 2.3602211513207294e-05\n",
            "0.036931818181818184\n",
            "Step: 106, Loss: 0.0005052036140114069\n",
            "0.015625\n",
            "Step: 107, Loss: 4.688862190960208e-06\n",
            "0.018666666666666668\n",
            "Step: 108, Loss: 8.647487993584946e-05\n",
            "0.0462046204620462\n",
            "Step: 109, Loss: 0.00024125240452121943\n",
            "0.01694915254237288\n",
            "Step: 110, Loss: 6.684272375423461e-05\n",
            "0.01910828025477707\n",
            "Step: 111, Loss: 0.00034551965654827654\n",
            "0.015873015873015872\n",
            "Step: 112, Loss: 0.0004379339807201177\n",
            "0.0\n",
            "Step: 113, Loss: 0.001197268022224307\n",
            "0.031746031746031744\n",
            "Step: 114, Loss: 3.333880442824011e-07\n",
            "0.0\n",
            "Step: 115, Loss: 0.0009622576762922108\n",
            "0.027700831024930747\n",
            "Step: 116, Loss: 0.00011620982695603743\n",
            "0.017326732673267328\n",
            "Step: 117, Loss: 8.378164238820318e-06\n",
            "0.005063291139240506\n",
            "Step: 118, Loss: 0.00029268546495586634\n",
            "0.01977401129943503\n",
            "Step: 119, Loss: 3.135509541607462e-05\n",
            "0.021604938271604937\n",
            "Step: 120, Loss: 2.712600462473347e-06\n",
            "0.006779661016949152\n",
            "Step: 121, Loss: 0.00025108447880484164\n",
            "0.0875\n",
            "Step: 122, Loss: 0.003940093796700239\n",
            "0.0\n",
            "Step: 123, Loss: 0.000876487756613642\n",
            "0.0\n",
            "Step: 124, Loss: 0.002091098576784134\n",
            "0.058823529411764705\n",
            "Step: 125, Loss: 0.0002863612899091095\n",
            "0.012987012987012988\n",
            "Step: 126, Loss: 0.00013381594908423722\n",
            "0.023622047244094488\n",
            "Step: 127, Loss: 6.0857721109641716e-05\n",
            "0.026706231454005934\n",
            "Step: 128, Loss: 5.614084147964604e-05\n",
            "0.005361930294906166\n",
            "Step: 129, Loss: 0.0002947291068267077\n",
            "0.023026315789473683\n",
            "Step: 130, Loss: 0.00018727639690041542\n",
            "0.09763313609467456\n",
            "Step: 131, Loss: 0.004875369369983673\n",
            "0.007194244604316547\n",
            "Step: 132, Loss: 0.0006999678444117308\n",
            "0.011811023622047244\n",
            "Step: 133, Loss: 0.00043449358781799674\n",
            "0.02066115702479339\n",
            "Step: 134, Loss: 0.0003215438628103584\n",
            "0.007194244604316547\n",
            "Step: 135, Loss: 0.0006687487475574017\n",
            "0.08695652173913043\n",
            "Step: 136, Loss: 0.001065235584974289\n",
            "0.0\n",
            "Step: 137, Loss: 0.003071790561079979\n",
            "0.0\n",
            "Step: 138, Loss: 0.0008398821810260415\n",
            "0.0\n",
            "Step: 139, Loss: 0.0007663836004212499\n",
            "0.00975609756097561\n",
            "Step: 140, Loss: 0.001474037766456604\n",
            "0.09704641350210971\n",
            "Step: 141, Loss: 0.005190703086555004\n",
            "0.00904977375565611\n",
            "Step: 142, Loss: 0.0005273258429951966\n",
            "0.024390243902439025\n",
            "Step: 143, Loss: 8.556822285754606e-05\n",
            "0.00796812749003984\n",
            "Step: 144, Loss: 0.001148541341535747\n",
            "0.023696682464454975\n",
            "Step: 145, Loss: 0.00022972897568251938\n",
            "0.0\n",
            "Step: 146, Loss: 0.001142158405855298\n",
            "0.06422018348623854\n",
            "Step: 147, Loss: 0.0007589150918647647\n",
            "0.05454545454545454\n",
            "Step: 148, Loss: 3.7673176848329604e-05\n",
            "0.0\n",
            "Step: 149, Loss: 0.0007192574557848275\n",
            "0.03333333333333333\n",
            "Step: 150, Loss: 0.0006068802322261035\n",
            "0.0\n",
            "Step: 151, Loss: 0.0005682687042281032\n",
            "0.02617801047120419\n",
            "Step: 152, Loss: 0.0007963007665239275\n",
            "0.04878048780487805\n",
            "Step: 153, Loss: 0.0004031471034977585\n",
            "0.027027027027027032\n",
            "Step: 154, Loss: 1.2084812624379992e-05\n",
            "0.015503875968992248\n",
            "Step: 155, Loss: 0.0002856182982213795\n",
            "0.0053475935828877\n",
            "Step: 156, Loss: 0.0019388577202335\n",
            "0.026905829596412554\n",
            "Step: 157, Loss: 0.00014785330859012902\n",
            "0.008888888888888889\n",
            "Step: 158, Loss: 0.0013393204426392913\n",
            "0.008130081300813009\n",
            "Step: 159, Loss: 0.001303646364249289\n",
            "0.0\n",
            "Step: 160, Loss: 0.0\n",
            "0.025\n",
            "Step: 161, Loss: 0.0004905824898742139\n",
            "0.047619047619047616\n",
            "Step: 162, Loss: 0.0017540064873173833\n",
            "0.04347826086956521\n",
            "Step: 163, Loss: 0.001201695529744029\n",
            "0.018518518518518517\n",
            "Step: 164, Loss: 4.13494162785355e-05\n",
            "0.0170316301703163\n",
            "Step: 165, Loss: 3.306036205685814e-06\n",
            "0.015659955257270694\n",
            "Step: 166, Loss: 6.96395409249817e-06\n",
            "0.017412935323383085\n",
            "Step: 167, Loss: 1.4830489817541093e-05\n",
            "0.01907356948228883\n",
            "Step: 168, Loss: 2.569646676420234e-05\n",
            "0.00980392156862745\n",
            "Step: 169, Loss: 5.836764103150927e-07\n",
            "0.03319502074688797\n",
            "Step: 170, Loss: 1.0901288078457583e-05\n",
            "0.03684210526315789\n",
            "Step: 171, Loss: 1.3843697161064483e-05\n",
            "0.017543859649122806\n",
            "Step: 172, Loss: 7.213773642433807e-05\n",
            "0.07894736842105263\n",
            "Step: 173, Loss: 0.005461916793137789\n",
            "0.058823529411764705\n",
            "Step: 174, Loss: 0.002757042646408081\n",
            "0.0392156862745098\n",
            "Step: 175, Loss: 0.0009993902640417218\n",
            "0.015306122448979593\n",
            "Step: 176, Loss: 4.031453499919735e-05\n",
            "0.009324009324009324\n",
            "Step: 177, Loss: 8.511474334227387e-07\n",
            "0.005050505050505051\n",
            "Step: 178, Loss: 4.124391853110865e-05\n",
            "0.00847457627118644\n",
            "Step: 179, Loss: 1.669431549089495e-05\n",
            "0.015151515151515152\n",
            "Step: 180, Loss: 2.2731353510607732e-06\n",
            "0.02864583333333333\n",
            "Step: 181, Loss: 4.644130768838295e-08\n",
            "0.024793388429752067\n",
            "Step: 182, Loss: 7.552279566880316e-05\n",
            "0.032467532467532464\n",
            "Step: 183, Loss: 6.0329884945531376e-06\n",
            "0.034482758620689655\n",
            "Step: 184, Loss: 0.00026709382655099034\n",
            "0.09523809523809523\n",
            "Step: 185, Loss: 0.006093689240515232\n",
            "0.0\n",
            "Step: 186, Loss: 0.00011126343451905996\n",
            "0.017543859649122806\n",
            "Step: 187, Loss: 3.367440149304457e-05\n",
            "0.02425876010781671\n",
            "Step: 188, Loss: 0.00010444330837344751\n",
            "0.006651884700665189\n",
            "Step: 189, Loss: 3.9453869248973206e-05\n",
            "0.011037527593818985\n",
            "Step: 190, Loss: 3.9181608713079186e-07\n",
            "0.01882845188284519\n",
            "Step: 191, Loss: 7.014621223788708e-05\n",
            "0.017142857142857144\n",
            "Step: 192, Loss: 5.855631752638146e-05\n",
            "0.02072538860103627\n",
            "Step: 193, Loss: 1.8594447510622558e-06\n",
            "0.01444043321299639\n",
            "Step: 194, Loss: 0.0003567434614524245\n",
            "0.0661764705882353\n",
            "Step: 195, Loss: 0.0019561341032385826\n",
            "0.042253521126760556\n",
            "Step: 196, Loss: 0.00019930553389713168\n",
            "0.041666666666666664\n",
            "Step: 197, Loss: 0.0002079631231026724\n",
            "0.052631578947368425\n",
            "Step: 198, Loss: 0.0014634287217631936\n",
            "0.02262443438914027\n",
            "Step: 199, Loss: 3.774947253987193e-05\n",
            "0.010610079575596816\n",
            "Step: 200, Loss: 5.496270387084223e-05\n",
            "0.015748031496062992\n",
            "Step: 201, Loss: 1.3195900464779697e-05\n",
            "0.0055248618784530384\n",
            "Step: 202, Loss: 0.00025425225612707436\n",
            "0.016867469879518072\n",
            "Step: 203, Loss: 9.776646038517356e-06\n",
            "0.022151898734177215\n",
            "Step: 204, Loss: 1.198181871586712e-05\n",
            "0.0\n",
            "Step: 205, Loss: 0.0006323140114545822\n",
            "0.027491408934707903\n",
            "Step: 206, Loss: 0.00012470311776269227\n",
            "0.017857142857142856\n",
            "Step: 207, Loss: 0.00037675132625736296\n",
            "0.01694915254237288\n",
            "Step: 208, Loss: 0.000443712982814759\n",
            "0.10638297872340426\n",
            "Step: 209, Loss: 0.004922998603433371\n",
            "0.012658227848101266\n",
            "Step: 210, Loss: 0.000580851046834141\n",
            "0.006329113924050633\n",
            "Step: 211, Loss: 0.00019293520017527044\n",
            "0.005934718100890208\n",
            "Step: 212, Loss: 0.0002343541127629578\n",
            "0.03291139240506329\n",
            "Step: 213, Loss: 0.00014393443416338414\n",
            "0.019704433497536946\n",
            "Step: 214, Loss: 1.9001558030140586e-05\n",
            "0.01893939393939394\n",
            "Step: 215, Loss: 1.658393921388779e-05\n",
            "0.01876675603217158\n",
            "Step: 216, Loss: 3.660624133772217e-05\n",
            "0.023728813559322035\n",
            "Step: 217, Loss: 2.370348920521792e-05\n",
            "0.03676470588235294\n",
            "Step: 218, Loss: 0.0004346324421931058\n",
            "0.04054054054054055\n",
            "Step: 219, Loss: 0.0009520708117634058\n",
            "0.02702702702702703\n",
            "Step: 220, Loss: 0.0003440913569647819\n",
            "0.046511627906976744\n",
            "Step: 221, Loss: 0.001543400576338172\n",
            "0.0078125\n",
            "Step: 222, Loss: 3.134436383334105e-06\n",
            "0.008823529411764706\n",
            "Step: 223, Loss: 1.613804852240719e-05\n",
            "0.014388489208633094\n",
            "Step: 224, Loss: 0.00011635781993390992\n",
            "0.03571428571428571\n",
            "Step: 225, Loss: 0.0002876438375096768\n",
            "0.0030303030303030303\n",
            "Step: 226, Loss: 0.000352883362211287\n",
            "0.026881720430107527\n",
            "Step: 227, Loss: 0.00011369633284630254\n",
            "0.014749262536873156\n",
            "Step: 228, Loss: 0.00015423189324792475\n",
            "0.0034482758620689655\n",
            "Step: 229, Loss: 0.0002653577830642462\n",
            "0.026845637583892617\n",
            "Step: 230, Loss: 3.0747163691557944e-05\n",
            "0.014492753623188408\n",
            "Step: 231, Loss: 2.4956851120805368e-05\n",
            "0.0\n",
            "Step: 232, Loss: 0.00033326068660244346\n",
            "0.016666666666666666\n",
            "Step: 233, Loss: 4.0181796066462994e-05\n",
            "0.0064516129032258064\n",
            "Step: 234, Loss: 2.9749789973720908e-05\n",
            "0.017142857142857144\n",
            "Step: 235, Loss: 6.305894203251228e-06\n",
            "0.03209876543209877\n",
            "Step: 236, Loss: 0.0003468989743851125\n",
            "0.012594458438287154\n",
            "Step: 237, Loss: 7.076305763575874e-09\n",
            "0.01204819277108434\n",
            "Step: 238, Loss: 4.526178543073911e-07\n",
            "0.031630170316301706\n",
            "Step: 239, Loss: 0.0004569060110952705\n",
            "0.013468013468013467\n",
            "Step: 240, Loss: 1.3692889297090005e-05\n",
            "0.014285714285714284\n",
            "Step: 241, Loss: 7.015053415670991e-05\n",
            "0.03333333333333333\n",
            "Step: 242, Loss: 4.3628674006868096e-07\n",
            "0.19117647058823528\n",
            "Step: 243, Loss: 0.026374489068984985\n",
            "0.5263157894736842\n",
            "Step: 244, Loss: 0.24621881544589996\n",
            "0.08333333333333333\n",
            "Step: 245, Loss: 0.002417574170976877\n",
            "0.013513513513513514\n",
            "Step: 246, Loss: 0.00035464949905872345\n",
            "0.02064896755162242\n",
            "Step: 247, Loss: 8.53110323077999e-05\n",
            "0.011961722488038277\n",
            "Step: 248, Loss: 0.00024156732251867652\n",
            "0.020942408376963352\n",
            "Step: 249, Loss: 1.674648046900984e-05\n",
            "0.03359173126614987\n",
            "Step: 250, Loss: 0.00011952064232900739\n",
            "0.031161473087818695\n",
            "Step: 251, Loss: 0.00011297124729026109\n",
            "0.09342560553633218\n",
            "Step: 252, Loss: 0.004746666643768549\n",
            "0.030075187969924814\n",
            "Step: 253, Loss: 5.0693374760157894e-06\n",
            "0.025806451612903226\n",
            "Step: 254, Loss: 1.4172167539072689e-05\n",
            "0.012345679012345678\n",
            "Step: 255, Loss: 0.001563232159242034\n",
            "0.07407407407407407\n",
            "Step: 256, Loss: 0.0006483871256932616\n",
            "0.0\n",
            "Step: 257, Loss: 0.002206449629738927\n",
            "0.007575757575757577\n",
            "Step: 258, Loss: 0.0013262578286230564\n",
            "0.013986013986013986\n",
            "Step: 259, Loss: 0.000748765713069588\n",
            "0.02\n",
            "Step: 260, Loss: 0.00035708508221432567\n",
            "0.013477088948787063\n",
            "Step: 261, Loss: 0.0001079127105185762\n",
            "0.0136986301369863\n",
            "Step: 262, Loss: 0.00016245919687207788\n",
            "0.02464788732394366\n",
            "Step: 263, Loss: 2.0344084987300448e-05\n",
            "0.021929824561403508\n",
            "Step: 264, Loss: 7.000644109211862e-05\n",
            "0.025380710659898477\n",
            "Step: 265, Loss: 8.434682968072593e-05\n",
            "0.007142857142857143\n",
            "Step: 266, Loss: 0.0006602460634894669\n",
            "0.029850746268656712\n",
            "Step: 267, Loss: 0.0009924168698489666\n",
            "0.0\n",
            "Step: 268, Loss: 0.0033752203453332186\n",
            "0.04255319148936171\n",
            "Step: 269, Loss: 0.00013980187941342592\n",
            "0.08620689655172414\n",
            "Step: 270, Loss: 0.001153069781139493\n",
            "0.0\n",
            "Step: 271, Loss: 0.0026164159644395113\n",
            "0.01968503937007874\n",
            "Step: 272, Loss: 3.9847054722486064e-05\n",
            "0.016233766233766232\n",
            "Step: 273, Loss: 7.878108590375632e-05\n",
            "0.014545454545454545\n",
            "Step: 274, Loss: 0.00020912225591018796\n",
            "0.012195121951219513\n",
            "Step: 275, Loss: 0.00032834350713528693\n",
            "0.026515151515151516\n",
            "Step: 276, Loss: 0.00021149551321286708\n",
            "0.02127659574468085\n",
            "Step: 277, Loss: 0.0003086678043473512\n",
            "0.0457516339869281\n",
            "Step: 278, Loss: 5.992539445287548e-05\n",
            "0.02127659574468085\n",
            "Step: 279, Loss: 0.0022499901242554188\n",
            "0.054054054054054064\n",
            "Step: 280, Loss: 0.00011209741205675527\n",
            "0.0\n",
            "Step: 281, Loss: 0.003909614868462086\n",
            "0.021739130434782608\n",
            "Step: 282, Loss: 1.4753335563000292e-05\n",
            "0.04245283018867924\n",
            "Step: 283, Loss: 0.00023920807871036232\n",
            "0.03018867924528302\n",
            "Step: 284, Loss: 7.247490998452122e-07\n",
            "0.033707865168539325\n",
            "Step: 285, Loss: 5.8105953939957544e-05\n",
            "0.17167381974248927\n",
            "Step: 286, Loss: 0.020307281985878944\n",
            "0.010830324909747292\n",
            "Step: 287, Loss: 0.0007146245916374028\n",
            "0.036290322580645164\n",
            "Step: 288, Loss: 8.319242624565959e-06\n",
            "0.036\n",
            "Step: 289, Loss: 0.00014757599274162203\n",
            "0.021276595744680847\n",
            "Step: 290, Loss: 0.0005303961806930602\n",
            "0.0\n",
            "Step: 291, Loss: 0.0\n",
            "0.0\n",
            "Step: 292, Loss: 6.7659339038073085e-06\n",
            "0.0\n",
            "Step: 293, Loss: 2.704082362470217e-05\n",
            "0.006993006993006993\n",
            "Step: 294, Loss: 6.30303645721142e-07\n",
            "0.010101010101010102\n",
            "Step: 295, Loss: 7.705517646172666e-08\n",
            "0.015037593984962405\n",
            "Step: 296, Loss: 4.271136276656762e-06\n",
            "0.016166281755196306\n",
            "Step: 297, Loss: 3.3111297170762555e-07\n",
            "0.014150943396226417\n",
            "Step: 298, Loss: 1.638819776417222e-05\n",
            "0.07235142118863049\n",
            "Step: 299, Loss: 0.0026670219376683235\n",
            "0.18\n",
            "Step: 300, Loss: 0.024103302508592606\n",
            "0.03619909502262444\n",
            "Step: 301, Loss: 1.1676348549372051e-05\n",
            "0.05217391304347826\n",
            "Step: 302, Loss: 0.00025525150704197586\n",
            "0.043478260869565216\n",
            "Step: 303, Loss: 0.001084015821106732\n",
            "0.0\n",
            "Step: 304, Loss: 2.663127816049382e-05\n",
            "0.015151515151515152\n",
            "Step: 305, Loss: 5.933639840804972e-05\n",
            "0.013986013986013986\n",
            "Step: 306, Loss: 3.19346618198324e-05\n",
            "0.016722408026755852\n",
            "Step: 307, Loss: 4.5972901716595516e-05\n",
            "0.009925558312655087\n",
            "Step: 308, Loss: 2.7161265734321205e-06\n",
            "0.022675736961451247\n",
            "Step: 309, Loss: 1.5658981737942668e-06\n",
            "0.02\n",
            "Step: 310, Loss: 2.2925249140826054e-05\n",
            "0.020594965675057208\n",
            "Step: 311, Loss: 1.79196686076466e-05\n",
            "0.0215633423180593\n",
            "Step: 312, Loss: 9.725426934892312e-05\n",
            "0.021875\n",
            "Step: 313, Loss: 4.83091525893542e-06\n",
            "0.034482758620689655\n",
            "Step: 314, Loss: 1.149426771007711e-05\n",
            "0.047619047619047616\n",
            "Step: 315, Loss: 0.0006983302300795913\n",
            "0.05128205128205128\n",
            "Step: 316, Loss: 0.000995759735815227\n",
            "0.0\n",
            "Step: 317, Loss: 0.00010648445459082723\n",
            "0.0234375\n",
            "Step: 318, Loss: 0.00013315198884811252\n",
            "0.008403361344537815\n",
            "Step: 319, Loss: 4.309463838580996e-05\n",
            "0.013605442176870748\n",
            "Step: 320, Loss: 8.631661074787189e-08\n",
            "0.036931818181818184\n",
            "Step: 321, Loss: 0.0006373013602569699\n",
            "0.018518518518518517\n",
            "Step: 322, Loss: 4.659984842447784e-09\n",
            "0.02570694087403599\n",
            "Step: 323, Loss: 3.169347473885864e-05\n",
            "0.0462046204620462\n",
            "Step: 324, Loss: 0.00018742840620689094\n",
            "0.035211267605633804\n",
            "Step: 325, Loss: 7.061208862069179e-07\n",
            "0.03496503496503497\n",
            "Step: 326, Loss: 9.768822201294824e-05\n",
            "0.015873015873015872\n",
            "Step: 327, Loss: 0.00026320689357817173\n",
            "0.0\n",
            "Step: 328, Loss: 0.0009222350199706852\n",
            "0.031746031746031744\n",
            "Step: 329, Loss: 9.84835423878394e-06\n",
            "0.03355704697986577\n",
            "Step: 330, Loss: 0.00032567812013439834\n",
            "0.013422818791946308\n",
            "Step: 331, Loss: 1.5432711734320037e-05\n",
            "0.01488833746898263\n",
            "Step: 332, Loss: 8.470211469102651e-05\n",
            "0.024128686327077747\n",
            "Step: 333, Loss: 2.6411200906295562e-06\n",
            "0.01977401129943503\n",
            "Step: 334, Loss: 1.5050234196678502e-06\n",
            "0.015384615384615385\n",
            "Step: 335, Loss: 1.664049159444403e-05\n",
            "0.015444015444015444\n",
            "Step: 336, Loss: 0.0003462239692453295\n",
            "0.0875\n",
            "Step: 337, Loss: 0.003624811302870512\n",
            "0.0\n",
            "Step: 338, Loss: 0.0009256104240193963\n",
            "0.0\n",
            "Step: 339, Loss: 0.001779942656867206\n",
            "0.058823529411764705\n",
            "Step: 340, Loss: 0.0003566686064004898\n",
            "0.030303030303030304\n",
            "Step: 341, Loss: 7.143381662899628e-05\n",
            "0.06329113924050633\n",
            "Step: 342, Loss: 0.001926581608131528\n",
            "0.018072289156626505\n",
            "Step: 343, Loss: 1.4666337847302202e-05\n",
            "0.020618556701030927\n",
            "Step: 344, Loss: 8.361674190382473e-06\n",
            "0.028011204481792718\n",
            "Step: 345, Loss: 7.89873502071714e-06\n",
            "0.09763313609467456\n",
            "Step: 346, Loss: 0.004772294778376818\n",
            "0.007194244604316547\n",
            "Step: 347, Loss: 0.0006599139887839556\n",
            "0.011811023622047244\n",
            "Step: 348, Loss: 0.0002765586832538247\n",
            "0.014218009478672983\n",
            "Step: 349, Loss: 0.00015331378381233662\n",
            "0.020979020979020983\n",
            "Step: 350, Loss: 0.00045976575347594917\n",
            "0.08695652173913043\n",
            "Step: 351, Loss: 0.0012200282653793693\n",
            "0.0\n",
            "Step: 352, Loss: 0.0026418319903314114\n",
            "0.047619047619047616\n",
            "Step: 353, Loss: 1.1446613825683016e-06\n",
            "0.0\n",
            "Step: 354, Loss: 0.0005709246033802629\n",
            "0.11111111111111112\n",
            "Step: 355, Loss: 0.0074350363574922085\n",
            "0.0228310502283105\n",
            "Step: 356, Loss: 3.532450136845e-05\n",
            "0.019801980198019802\n",
            "Step: 357, Loss: 0.00010898231266764924\n",
            "0.024390243902439025\n",
            "Step: 358, Loss: 3.5541102988645434e-05\n",
            "0.024271844660194174\n",
            "Step: 359, Loss: 8.517620881320909e-05\n",
            "0.0055248618784530384\n",
            "Step: 360, Loss: 0.0008600339060649276\n",
            "0.01932367149758454\n",
            "Step: 361, Loss: 0.0002964047307614237\n",
            "0.05\n",
            "Step: 362, Loss: 4.693894152296707e-05\n",
            "0.05454545454545454\n",
            "Step: 363, Loss: 5.5634009186178446e-05\n",
            "0.0\n",
            "Step: 364, Loss: 0.003622672287747264\n",
            "0.03333333333333333\n",
            "Step: 365, Loss: 0.0005557061522267759\n",
            "0.034482758620689655\n",
            "Step: 366, Loss: 3.4309141483390704e-05\n",
            "0.020512820512820513\n",
            "Step: 367, Loss: 9.863493323791772e-05\n",
            "0.04878048780487805\n",
            "Step: 368, Loss: 0.00029229899519123137\n",
            "0.027027027027027032\n",
            "Step: 369, Loss: 1.6270340665869298e-06\n",
            "0.015503875968992248\n",
            "Step: 370, Loss: 0.00025742792058736086\n",
            "0.00909090909090909\n",
            "Step: 371, Loss: 0.0007759729051031172\n",
            "0.022727272727272728\n",
            "Step: 372, Loss: 0.0005834363983012736\n",
            "0.01507537688442211\n",
            "Step: 373, Loss: 0.000670351495500654\n",
            "0.014814814814814815\n",
            "Step: 374, Loss: 0.0008284831419587135\n",
            "0.0\n",
            "Step: 375, Loss: 0.0\n",
            "0.025\n",
            "Step: 376, Loss: 0.00047092503518797457\n",
            "0.047619047619047616\n",
            "Step: 377, Loss: 0.0016815848648548126\n",
            "0.04347826086956521\n",
            "Step: 378, Loss: 0.0011211712844669819\n",
            "0.018518518518518517\n",
            "Step: 379, Loss: 2.5685620130388997e-05\n",
            "0.0170316301703163\n",
            "Step: 380, Loss: 3.506739076897247e-08\n",
            "0.015659955257270694\n",
            "Step: 381, Loss: 2.074875374091789e-05\n",
            "0.017412935323383085\n",
            "Step: 382, Loss: 3.7371639336925e-05\n",
            "0.01907356948228883\n",
            "Step: 383, Loss: 5.946323653915897e-05\n",
            "0.014925373134328358\n",
            "Step: 384, Loss: 0.00022635087952949107\n",
            "0.03319502074688797\n",
            "Step: 385, Loss: 7.951811653583718e-08\n",
            "0.03684210526315789\n",
            "Step: 386, Loss: 3.956789385028969e-07\n",
            "0.017543859649122806\n",
            "Step: 387, Loss: 6.0498401580844074e-05\n",
            "0.07894736842105263\n",
            "Step: 388, Loss: 0.005393418483436108\n",
            "0.058823529411764705\n",
            "Step: 389, Loss: 0.0026720822788774967\n",
            "0.0392156862745098\n",
            "Step: 390, Loss: 0.0009277793578803539\n",
            "0.015306122448979593\n",
            "Step: 391, Loss: 2.3984863219084218e-05\n",
            "0.009324009324009324\n",
            "Step: 392, Loss: 7.313999049074482e-06\n",
            "0.005050505050505051\n",
            "Step: 393, Loss: 7.328849460463971e-05\n",
            "0.00847457627118644\n",
            "Step: 394, Loss: 4.412555063026957e-05\n",
            "0.015151515151515152\n",
            "Step: 395, Loss: 2.1001897039241157e-06\n",
            "0.011049723756906077\n",
            "Step: 396, Loss: 5.02898583363276e-05\n",
            "0.029411764705882353\n",
            "Step: 397, Loss: 2.2534906747750938e-05\n",
            "0.032467532467532464\n",
            "Step: 398, Loss: 2.384222170803696e-05\n",
            "0.034482758620689655\n",
            "Step: 399, Loss: 0.00022307752806227654\n",
            "0.09523809523809523\n",
            "Step: 400, Loss: 0.005942474585026503\n",
            "0.0\n",
            "Step: 401, Loss: 0.0001263226440642029\n",
            "0.017543859649122806\n",
            "Step: 402, Loss: 2.2959624402574264e-05\n",
            "0.02425876010781671\n",
            "Step: 403, Loss: 0.00010800804739119485\n",
            "0.006651884700665189\n",
            "Step: 404, Loss: 3.3277345210080966e-05\n",
            "0.016666666666666666\n",
            "Step: 405, Loss: 5.585418421105715e-07\n",
            "0.00804289544235925\n",
            "Step: 406, Loss: 0.00011912309855688363\n",
            "0.008571428571428572\n",
            "Step: 407, Loss: 0.0001395917497575283\n",
            "0.02072538860103627\n",
            "Step: 408, Loss: 1.0418583542559645e-06\n",
            "0.012578616352201259\n",
            "Step: 409, Loss: 0.00011358497431501746\n",
            "0.041666666666666664\n",
            "Step: 410, Loss: 1.0128167559741996e-05\n",
            "0.042253521126760556\n",
            "Step: 411, Loss: 0.0001559714146424085\n",
            "0.041666666666666664\n",
            "Step: 412, Loss: 0.00017691808170638978\n",
            "0.014285714285714284\n",
            "Step: 413, Loss: 0.00015986427024472505\n",
            "0.02262443438914027\n",
            "Step: 414, Loss: 3.442407978582196e-05\n",
            "0.009523809523809525\n",
            "Step: 415, Loss: 5.3525516705121845e-05\n",
            "0.0103359173126615\n",
            "Step: 416, Loss: 9.472214878769591e-05\n",
            "0.0055248618784530384\n",
            "Step: 417, Loss: 0.00023354418226517737\n",
            "0.016867469879518072\n",
            "Step: 418, Loss: 5.2004670578753576e-06\n",
            "0.022151898734177215\n",
            "Step: 419, Loss: 2.0865598344244063e-05\n",
            "0.012698412698412698\n",
            "Step: 420, Loss: 1.17230047180783e-05\n",
            "0.024390243902439025\n",
            "Step: 421, Loss: 7.5757211561722215e-06\n",
            "0.017857142857142856\n",
            "Step: 422, Loss: 0.000476191402412951\n",
            "0.01694915254237288\n",
            "Step: 423, Loss: 0.0005249395035207272\n",
            "0.10638297872340426\n",
            "Step: 424, Loss: 0.0046739596873521805\n",
            "0.012658227848101266\n",
            "Step: 425, Loss: 0.0006042023305781186\n",
            "0.017341040462427744\n",
            "Step: 426, Loss: 0.00032753802952356637\n",
            "0.005934718100890208\n",
            "Step: 427, Loss: 0.00027058616979047656\n",
            "0.012376237623762377\n",
            "Step: 428, Loss: 0.00012737236102111638\n",
            "0.019704433497536946\n",
            "Step: 429, Loss: 2.1212517822277732e-05\n",
            "0.01893939393939394\n",
            "Step: 430, Loss: 4.948931382386945e-05\n",
            "0.034210526315789476\n",
            "Step: 431, Loss: 2.6163745133089833e-05\n",
            "0.023728813559322035\n",
            "Step: 432, Loss: 7.589065353386104e-05\n",
            "0.006535947712418301\n",
            "Step: 433, Loss: 0.0008364274981431663\n",
            "0.04054054054054055\n",
            "Step: 434, Loss: 0.0009332340559922159\n",
            "0.06779661016949153\n",
            "Step: 435, Loss: 0.0038793638814240694\n",
            "0.046511627906976744\n",
            "Step: 436, Loss: 0.0015708447899669409\n",
            "0.0078125\n",
            "Step: 437, Loss: 6.028551524650538e-06\n",
            "0.01929260450160772\n",
            "Step: 438, Loss: 8.659686136525124e-05\n",
            "0.01288659793814433\n",
            "Step: 439, Loss: 1.926712911881623e-06\n",
            "0.0199501246882793\n",
            "Step: 440, Loss: 4.842651105718687e-05\n",
            "0.02997275204359673\n",
            "Step: 441, Loss: 0.00023810558195691556\n",
            "0.026881720430107527\n",
            "Step: 442, Loss: 0.0001132542165578343\n",
            "0.0226628895184136\n",
            "Step: 443, Loss: 2.222730290668551e-05\n",
            "0.0034482758620689655\n",
            "Step: 444, Loss: 0.0002608719514682889\n",
            "0.026845637583892617\n",
            "Step: 445, Loss: 8.386576519114897e-05\n",
            "0.014492753623188408\n",
            "Step: 446, Loss: 3.078235386055894e-05\n",
            "0.0\n",
            "Step: 447, Loss: 0.000341240840498358\n",
            "0.016666666666666666\n",
            "Step: 448, Loss: 3.065220880671404e-05\n",
            "0.0\n",
            "Step: 449, Loss: 0.00014760038175154477\n",
            "0.017142857142857144\n",
            "Step: 450, Loss: 1.1100717529188842e-05\n",
            "0.03209876543209877\n",
            "Step: 451, Loss: 0.00039255546289496124\n",
            "0.012594458438287154\n",
            "Step: 452, Loss: 2.933639279945055e-06\n",
            "0.01204819277108434\n",
            "Step: 453, Loss: 7.085075139912078e-06\n",
            "0.029490616621983913\n",
            "Step: 454, Loss: 8.566259930375963e-05\n",
            "0.009836065573770493\n",
            "Step: 455, Loss: 0.0001447211834602058\n",
            "0.03557312252964427\n",
            "Step: 456, Loss: 2.36851906265656e-06\n",
            "0.025974025974025976\n",
            "Step: 457, Loss: 1.6022439695007051e-06\n",
            "0.19117647058823528\n",
            "Step: 458, Loss: 0.026003876700997353\n",
            "0.5263157894736842\n",
            "Step: 459, Loss: 0.24680204689502716\n",
            "0.08333333333333333\n",
            "Step: 460, Loss: 0.002808423014357686\n",
            "0.013513513513513514\n",
            "Step: 461, Loss: 0.00022196059580892324\n",
            "0.02064896755162242\n",
            "Step: 462, Loss: 3.060061499127187e-05\n",
            "0.011961722488038277\n",
            "Step: 463, Loss: 0.00014454039046540856\n",
            "0.020942408376963352\n",
            "Step: 464, Loss: 6.306090085672622e-07\n",
            "0.03359173126614987\n",
            "Step: 465, Loss: 0.00019722268916666508\n",
            "0.031161473087818695\n",
            "Step: 466, Loss: 0.00018584537610877305\n",
            "0.043137254901960784\n",
            "Step: 467, Loss: 0.00011792681470979005\n",
            "0.038314176245210725\n",
            "Step: 468, Loss: 7.750048098387197e-06\n",
            "0.006172839506172839\n",
            "Step: 469, Loss: 0.0010586567223072052\n",
            "0.012345679012345678\n",
            "Step: 470, Loss: 0.001158134313300252\n",
            "0.07407407407407407\n",
            "Step: 471, Loss: 0.0009153083083219826\n",
            "0.0\n",
            "Step: 472, Loss: 0.0017672599060460925\n",
            "0.007575757575757577\n",
            "Step: 473, Loss: 0.0010248800972476602\n",
            "0.013986013986013986\n",
            "Step: 474, Loss: 0.0005429559387266636\n",
            "0.02\n",
            "Step: 475, Loss: 0.0002269447868457064\n",
            "0.008955223880597015\n",
            "Step: 476, Loss: 0.0005728747346438468\n",
            "0.038348082595870206\n",
            "Step: 477, Loss: 0.0001242368743987754\n",
            "0.02952029520295203\n",
            "Step: 478, Loss: 9.893410606309772e-07\n",
            "0.021929824561403508\n",
            "Step: 479, Loss: 2.0471818061196245e-05\n",
            "0.025380710659898477\n",
            "Step: 480, Loss: 0.00012682453962042928\n",
            "0.007142857142857143\n",
            "Step: 481, Loss: 0.0006616113823838532\n",
            "0.029850746268656712\n",
            "Step: 482, Loss: 0.0007110582082532346\n",
            "0.0\n",
            "Step: 483, Loss: 0.002907397458329797\n",
            "0.04255319148936171\n",
            "Step: 484, Loss: 7.226859452202916e-05\n",
            "0.08620689655172414\n",
            "Step: 485, Loss: 0.001384723698720336\n",
            "0.0\n",
            "Step: 486, Loss: 0.0022525659296661615\n",
            "0.01968503937007874\n",
            "Step: 487, Loss: 6.288822623901069e-05\n",
            "0.015822784810126583\n",
            "Step: 488, Loss: 0.0001721681037452072\n",
            "0.0035087719298245615\n",
            "Step: 489, Loss: 0.0006286384887062013\n",
            "0.012195121951219513\n",
            "Step: 490, Loss: 0.0003628623380791396\n",
            "0.026515151515151516\n",
            "Step: 491, Loss: 0.00011445098789408803\n",
            "0.02127659574468085\n",
            "Step: 492, Loss: 0.00019057781901210546\n",
            "0.0457516339869281\n",
            "Step: 493, Loss: 3.0341654564836062e-05\n",
            "0.02127659574468085\n",
            "Step: 494, Loss: 0.001972678815945983\n",
            "0.054054054054054064\n",
            "Step: 495, Loss: 7.237618410727009e-05\n",
            "0.0\n",
            "Step: 496, Loss: 0.003659115405753255\n",
            "0.0\n",
            "Step: 497, Loss: 0.003310030559077859\n",
            "0.04245283018867924\n",
            "Step: 498, Loss: 0.00016211775073315948\n",
            "0.03018867924528302\n",
            "Step: 499, Loss: 1.9027456801268272e-06\n",
            "0.012295081967213113\n",
            "Step: 500, Loss: 0.00042853952618315816\n",
            "0.17167381974248927\n",
            "Step: 501, Loss: 0.02031952328979969\n",
            "0.010830324909747292\n",
            "Step: 502, Loss: 0.0006223535165190697\n",
            "0.036290322580645164\n",
            "Step: 503, Loss: 3.674392473840271e-06\n",
            "0.0034722222222222225\n",
            "Step: 504, Loss: 0.001422438072040677\n",
            "0.007936507936507936\n",
            "Step: 505, Loss: 0.001216115546412766\n",
            "0.0\n",
            "Step: 506, Loss: 0.0\n",
            "0.0\n",
            "Step: 507, Loss: 9.14618794922717e-06\n",
            "0.0\n",
            "Step: 508, Loss: 3.6568879295373335e-05\n",
            "0.006993006993006993\n",
            "Step: 509, Loss: 4.284253918740433e-06\n",
            "0.010101010101010102\n",
            "Step: 510, Loss: 3.910140094376402e-06\n",
            "0.015037593984962405\n",
            "Step: 511, Loss: 2.6800737185084245e-09\n",
            "0.016166281755196306\n",
            "Step: 512, Loss: 3.766429472307209e-06\n",
            "0.014150943396226417\n",
            "Step: 513, Loss: 4.83923613501247e-05\n",
            "0.07235142118863049\n",
            "Step: 514, Loss: 0.0023339868057519197\n",
            "0.18\n",
            "Step: 515, Loss: 0.023173240944743156\n",
            "0.03619909502262444\n",
            "Step: 516, Loss: 6.379472324624658e-06\n",
            "0.05217391304347826\n",
            "Step: 517, Loss: 0.00022732959769200534\n",
            "0.043478260869565216\n",
            "Step: 518, Loss: 0.0011022592661902308\n",
            "0.0\n",
            "Step: 519, Loss: 2.9328080927371047e-05\n",
            "0.03773584905660377\n",
            "Step: 520, Loss: 0.0008927662274800241\n",
            "0.013986013986013986\n",
            "Step: 521, Loss: 3.146447488688864e-05\n",
            "0.016722408026755852\n",
            "Step: 522, Loss: 4.696528412750922e-05\n",
            "0.009925558312655087\n",
            "Step: 523, Loss: 2.1072760318929795e-06\n",
            "0.022675736961451247\n",
            "Step: 524, Loss: 1.3758871091340552e-06\n",
            "0.02\n",
            "Step: 525, Loss: 2.4165399736375548e-05\n",
            "0.009389671361502348\n",
            "Step: 526, Loss: 0.00035716366255655885\n",
            "0.0215633423180593\n",
            "Step: 527, Loss: 9.896748815663159e-05\n",
            "0.021875\n",
            "Step: 528, Loss: 9.366553058498539e-06\n",
            "0.034482758620689655\n",
            "Step: 529, Loss: 1.3343110367713962e-05\n",
            "0.047619047619047616\n",
            "Step: 530, Loss: 0.0007297833799384534\n",
            "0.05128205128205128\n",
            "Step: 531, Loss: 0.0010506893740966916\n",
            "0.0\n",
            "Step: 532, Loss: 0.0001172594929812476\n",
            "0.0234375\n",
            "Step: 533, Loss: 0.00012407223402988166\n",
            "0.019830028328611898\n",
            "Step: 534, Loss: 1.724066532915458e-05\n",
            "0.0234192037470726\n",
            "Step: 535, Loss: 1.910715945996344e-05\n",
            "0.02127659574468085\n",
            "Step: 536, Loss: 1.982019966817461e-05\n",
            "0.018518518518518517\n",
            "Step: 537, Loss: 2.793415809776434e-08\n",
            "0.02570694087403599\n",
            "Step: 538, Loss: 3.4230044548166916e-05\n",
            "0.016181229773462782\n",
            "Step: 539, Loss: 2.7508287530508824e-05\n",
            "0.035211267605633804\n",
            "Step: 540, Loss: 4.2978555825357034e-07\n",
            "0.03496503496503497\n",
            "Step: 541, Loss: 0.00011236119462409988\n",
            "0.015873015873015872\n",
            "Step: 542, Loss: 0.00023281927860807627\n",
            "0.0\n",
            "Step: 543, Loss: 0.0008538680267520249\n",
            "0.031746031746031744\n",
            "Step: 544, Loss: 1.981913737836294e-05\n",
            "0.03355704697986577\n",
            "Step: 545, Loss: 0.00029924779664725065\n",
            "0.013422818791946308\n",
            "Step: 546, Loss: 1.9883023924194276e-05\n",
            "0.017326732673267328\n",
            "Step: 547, Loss: 8.173423339030705e-06\n",
            "0.013440860215053764\n",
            "Step: 548, Loss: 0.00010180657409364358\n",
            "0.01977401129943503\n",
            "Step: 549, Loss: 2.021128238993697e-06\n",
            "0.026143790849673203\n",
            "Step: 550, Loss: 1.6398695152020082e-05\n",
            "0.015444015444015444\n",
            "Step: 551, Loss: 0.0003268354630563408\n",
            "0.0875\n",
            "Step: 552, Loss: 0.003662524512037635\n",
            "0.0\n",
            "Step: 553, Loss: 0.0008661889587529004\n",
            "0.0\n",
            "Step: 554, Loss: 0.0016965337563306093\n",
            "0.058823529411764705\n",
            "Step: 555, Loss: 0.00039337383350357413\n",
            "0.030303030303030304\n",
            "Step: 556, Loss: 5.0543567340355366e-05\n",
            "0.06329113924050633\n",
            "Step: 557, Loss: 0.0018423465080559254\n",
            "0.018072289156626505\n",
            "Step: 558, Loss: 1.8733191609499045e-05\n",
            "0.020618556701030927\n",
            "Step: 559, Loss: 1.0664780347724445e-05\n",
            "0.04216867469879518\n",
            "Step: 560, Loss: 0.00028176847263239324\n",
            "0.09763313609467456\n",
            "Step: 561, Loss: 0.004878834821283817\n",
            "0.007194244604316547\n",
            "Step: 562, Loss: 0.0006044181645847857\n",
            "0.011811023622047244\n",
            "Step: 563, Loss: 0.0001774823758751154\n",
            "0.014218009478672983\n",
            "Step: 564, Loss: 8.016515494091436e-05\n",
            "0.020979020979020983\n",
            "Step: 565, Loss: 0.0004238997644279152\n",
            "0.08695652173913043\n",
            "Step: 566, Loss: 0.0012838654220104218\n",
            "0.0\n",
            "Step: 567, Loss: 0.0024890678469091654\n",
            "0.047619047619047616\n",
            "Step: 568, Loss: 4.187656799103934e-08\n",
            "0.0\n",
            "Step: 569, Loss: 0.0006268350407481194\n",
            "0.11111111111111112\n",
            "Step: 570, Loss: 0.0072243730537593365\n",
            "0.0228310502283105\n",
            "Step: 571, Loss: 3.840853241854347e-05\n",
            "0.019801980198019802\n",
            "Step: 572, Loss: 0.00011278214515186846\n",
            "0.024390243902439025\n",
            "Step: 573, Loss: 2.3178477931651287e-05\n",
            "0.024271844660194174\n",
            "Step: 574, Loss: 6.705865962430835e-05\n",
            "0.023696682464454975\n",
            "Step: 575, Loss: 0.00014345429372042418\n",
            "0.01932367149758454\n",
            "Step: 576, Loss: 0.00019122501544188708\n",
            "0.02586206896551724\n",
            "Step: 577, Loss: 0.00014076389197725803\n",
            "0.05454545454545454\n",
            "Step: 578, Loss: 4.401693513500504e-05\n",
            "0.0\n",
            "Step: 579, Loss: 0.0035082148388028145\n",
            "0.03333333333333333\n",
            "Step: 580, Loss: 0.0005307986866682768\n",
            "0.03225806451612903\n",
            "Step: 581, Loss: 0.00048691677511669695\n",
            "0.020512820512820513\n",
            "Step: 582, Loss: 0.00011909758177353069\n",
            "0.04878048780487805\n",
            "Step: 583, Loss: 0.000260221742792055\n",
            "0.0043859649122807015\n",
            "Step: 584, Loss: 0.0009056521812453866\n",
            "0.015503875968992248\n",
            "Step: 585, Loss: 0.00039502649451605976\n",
            "0.01485148514851485\n",
            "Step: 586, Loss: 0.00034025911008939147\n",
            "0.026905829596412554\n",
            "Step: 587, Loss: 8.744193473830819e-05\n",
            "0.008888888888888889\n",
            "Step: 588, Loss: 0.0011274325661361217\n",
            "0.008130081300813009\n",
            "Step: 589, Loss: 0.001198556856252253\n",
            "0.0\n",
            "Step: 590, Loss: 0.0\n",
            "0.025\n",
            "Step: 591, Loss: 0.0004694806702900678\n",
            "0.047619047619047616\n",
            "Step: 592, Loss: 0.0016765770269557834\n",
            "0.04347826086956521\n",
            "Step: 593, Loss: 0.0011171282967552543\n",
            "0.018518518518518517\n",
            "Step: 594, Loss: 2.5383928004885092e-05\n",
            "0.0170316301703163\n",
            "Step: 595, Loss: 2.6452566359580487e-08\n",
            "0.015659955257270694\n",
            "Step: 596, Loss: 2.1009909687563777e-05\n",
            "0.017412935323383085\n",
            "Step: 597, Loss: 3.8064405089244246e-05\n",
            "0.01907356948228883\n",
            "Step: 598, Loss: 6.10854840488173e-05\n",
            "0.014925373134328358\n",
            "Step: 599, Loss: 0.00023179585696198046\n",
            "0.03319502074688797\n",
            "Step: 600, Loss: 4.202122205754222e-09\n",
            "0.03684210526315789\n",
            "Step: 601, Loss: 6.63328592054313e-08\n",
            "0.017543859649122806\n",
            "Step: 602, Loss: 5.984615927445702e-05\n",
            "0.07894736842105263\n",
            "Step: 603, Loss: 0.005382105708122253\n",
            "0.047619047619047616\n",
            "Step: 604, Loss: 0.001697640516795218\n",
            "0.0392156862745098\n",
            "Step: 605, Loss: 0.000936153344810009\n",
            "0.015306122448979593\n",
            "Step: 606, Loss: 2.6437190172146074e-05\n",
            "0.009324009324009324\n",
            "Step: 607, Loss: 5.596095888904529e-06\n",
            "0.005050505050505051\n",
            "Step: 608, Loss: 6.628197297686711e-05\n",
            "0.00847457627118644\n",
            "Step: 609, Loss: 3.80742312700022e-05\n",
            "0.015151515151515152\n",
            "Step: 610, Loss: 8.713701049600786e-07\n",
            "0.02864583333333333\n",
            "Step: 611, Loss: 4.881838776782388e-06\n",
            "0.029411764705882353\n",
            "Step: 612, Loss: 2.2530273781740107e-05\n",
            "0.032467532467532464\n",
            "Step: 613, Loss: 2.4520224542357028e-05\n",
            "0.034482758620689655\n",
            "Step: 614, Loss: 0.0002207665820606053\n",
            "0.09523809523809523\n",
            "Step: 615, Loss: 0.005970675032585859\n",
            "0.0\n",
            "Step: 616, Loss: 0.0001270599605049938\n",
            "0.017543859649122806\n",
            "Step: 617, Loss: 2.306375426996965e-05\n",
            "0.02425876010781671\n",
            "Step: 618, Loss: 0.00012819665425922722\n",
            "0.006651884700665189\n",
            "Step: 619, Loss: 2.120556018780917e-05\n",
            "0.016666666666666666\n",
            "Step: 620, Loss: 3.2005678463065124e-07\n",
            "0.00804289544235925\n",
            "Step: 621, Loss: 0.00011407072452129796\n",
            "0.008571428571428572\n",
            "Step: 622, Loss: 0.00013379417941905558\n",
            "0.02072538860103627\n",
            "Step: 623, Loss: 6.103562100179261e-07\n",
            "0.012578616352201259\n",
            "Step: 624, Loss: 0.00010785327322082594\n",
            "0.0661764705882353\n",
            "Step: 625, Loss: 0.0017568040639162064\n",
            "0.042253521126760556\n",
            "Step: 626, Loss: 0.0001573175541125238\n",
            "0.041666666666666664\n",
            "Step: 627, Loss: 0.00018528467626310885\n",
            "0.014285714285714284\n",
            "Step: 628, Loss: 0.00014614631072618067\n",
            "0.02262443438914027\n",
            "Step: 629, Loss: 3.3292446460109204e-05\n",
            "0.010610079575596816\n",
            "Step: 630, Loss: 6.08486843702849e-05\n",
            "0.00746268656716418\n",
            "Step: 631, Loss: 0.00018722501408774406\n",
            "0.0055248618784530384\n",
            "Step: 632, Loss: 0.00019107926345895976\n",
            "0.016867469879518072\n",
            "Step: 633, Loss: 4.171509146999597e-07\n",
            "0.022151898734177215\n",
            "Step: 634, Loss: 4.0938190068118274e-05\n",
            "0.0\n",
            "Step: 635, Loss: 0.0006771466578356922\n",
            "0.024390243902439025\n",
            "Step: 636, Loss: 8.335281563631725e-06\n",
            "0.017857142857142856\n",
            "Step: 637, Loss: 0.00045677495654672384\n",
            "0.01694915254237288\n",
            "Step: 638, Loss: 0.0005177273997105658\n",
            "0.10638297872340426\n",
            "Step: 639, Loss: 0.004708335734903812\n",
            "0.012658227848101266\n",
            "Step: 640, Loss: 0.0005685407668352127\n",
            "0.017341040462427744\n",
            "Step: 641, Loss: 0.00029771641129627824\n",
            "0.005934718100890208\n",
            "Step: 642, Loss: 0.0002697751915547997\n",
            "0.012376237623762377\n",
            "Step: 643, Loss: 0.00012878487177658826\n",
            "0.019704433497536946\n",
            "Step: 644, Loss: 1.4482322512776591e-05\n",
            "0.01893939393939394\n",
            "Step: 645, Loss: 5.450231037684716e-05\n",
            "0.034210526315789476\n",
            "Step: 646, Loss: 2.1937128622084856e-05\n",
            "0.023728813559322035\n",
            "Step: 647, Loss: 8.364436507690698e-05\n",
            "0.006535947712418301\n",
            "Step: 648, Loss: 0.0008684133063070476\n",
            "0.04054054054054055\n",
            "Step: 649, Loss: 0.0009359808173030615\n",
            "0.06779661016949153\n",
            "Step: 650, Loss: 0.003876791102811694\n",
            "0.046511627906976744\n",
            "Step: 651, Loss: 0.0016067809192463756\n",
            "0.0078125\n",
            "Step: 652, Loss: 9.709330697660334e-06\n",
            "0.01929260450160772\n",
            "Step: 653, Loss: 8.518924005329609e-05\n",
            "0.01288659793814433\n",
            "Step: 654, Loss: 1.6915249716475955e-06\n",
            "0.0199501246882793\n",
            "Step: 655, Loss: 4.697440090239979e-05\n",
            "0.02997275204359673\n",
            "Step: 656, Loss: 0.00023490296734962612\n",
            "0.026881720430107527\n",
            "Step: 657, Loss: 0.00011203305621165782\n",
            "0.0226628895184136\n",
            "Step: 658, Loss: 2.2147671188577078e-05\n",
            "0.027700831024930747\n",
            "Step: 659, Loss: 3.118856329820119e-05\n",
            "0.026845637583892617\n",
            "Step: 660, Loss: 9.207966650137678e-05\n",
            "0.014492753623188408\n",
            "Step: 661, Loss: 2.9532393455156125e-05\n",
            "0.0\n",
            "Step: 662, Loss: 0.00032991153420880437\n",
            "0.016666666666666666\n",
            "Step: 663, Loss: 3.077572910115123e-05\n",
            "0.0\n",
            "Step: 664, Loss: 0.00013844207569491118\n",
            "0.017142857142857144\n",
            "Step: 665, Loss: 1.7999140254687518e-05\n",
            "0.03209876543209877\n",
            "Step: 666, Loss: 0.00043807984911836684\n",
            "0.012594458438287154\n",
            "Step: 667, Loss: 9.377969035995193e-06\n",
            "0.028985507246376812\n",
            "Step: 668, Loss: 0.00010146586282644421\n",
            "0.029490616621983913\n",
            "Step: 669, Loss: 8.021450776141137e-05\n",
            "0.009836065573770493\n",
            "Step: 670, Loss: 0.0001523330429336056\n",
            "0.014285714285714284\n",
            "Step: 671, Loss: 8.701160550117493e-05\n",
            "0.025974025974025976\n",
            "Step: 672, Loss: 8.746872595111199e-07\n",
            "0.19117647058823528\n",
            "Step: 673, Loss: 0.026043415069580078\n",
            "0.5263157894736842\n",
            "Step: 674, Loss: 0.24753151834011078\n",
            "0.08333333333333333\n",
            "Step: 675, Loss: 0.0030034249648451805\n",
            "0.013513513513513514\n",
            "Step: 676, Loss: 0.00016914306615944952\n",
            "0.02064896755162242\n",
            "Step: 677, Loss: 1.3418412891041953e-05\n",
            "0.011961722488038277\n",
            "Step: 678, Loss: 0.00010334491526009515\n",
            "0.020942408376963352\n",
            "Step: 679, Loss: 1.0722493470893824e-06\n",
            "0.03359173126614987\n",
            "Step: 680, Loss: 0.0002514730440452695\n",
            "0.031161473087818695\n",
            "Step: 681, Loss: 0.0002391419402556494\n",
            "0.043137254901960784\n",
            "Step: 682, Loss: 0.00012334255734458566\n",
            "0.038314176245210725\n",
            "Step: 683, Loss: 8.993756637210026e-06\n",
            "0.006172839506172839\n",
            "Step: 684, Loss: 0.0010483384830877185\n",
            "0.012345679012345678\n",
            "Step: 685, Loss: 0.0009970426326617599\n",
            "0.07407407407407407\n",
            "Step: 686, Loss: 0.001059081288985908\n",
            "0.0\n",
            "Step: 687, Loss: 0.001571126515045762\n",
            "0.007575757575757577\n",
            "Step: 688, Loss: 0.0008839821093715727\n",
            "0.013986013986013986\n",
            "Step: 689, Loss: 0.00044442524085752666\n",
            "0.02\n",
            "Step: 690, Loss: 0.0001657285465626046\n",
            "0.008955223880597015\n",
            "Step: 691, Loss: 0.000473207444883883\n",
            "0.009836065573770493\n",
            "Step: 692, Loss: 0.0003470515657681972\n",
            "0.02952029520295203\n",
            "Step: 693, Loss: 1.1056742550863419e-05\n",
            "0.021929824561403508\n",
            "Step: 694, Loss: 4.70224222226534e-06\n",
            "0.025380710659898477\n",
            "Step: 695, Loss: 0.00011567852925509214\n",
            "0.0078125\n",
            "Step: 696, Loss: 0.0009862898150458932\n",
            "0.029850746268656712\n",
            "Step: 697, Loss: 0.0005865975399501622\n",
            "0.0\n",
            "Step: 698, Loss: 0.0026661083102226257\n",
            "0.04255319148936171\n",
            "Step: 699, Loss: 4.172442640992813e-05\n",
            "0.08620689655172414\n",
            "Step: 700, Loss: 0.0015434547094628215\n",
            "0.0\n",
            "Step: 701, Loss: 0.0020405887626111507\n",
            "0.01968503937007874\n",
            "Step: 702, Loss: 6.65982297505252e-05\n",
            "0.015822784810126583\n",
            "Step: 703, Loss: 0.00018140666361432523\n",
            "0.014545454545454545\n",
            "Step: 704, Loss: 0.0002605664194561541\n",
            "0.012195121951219513\n",
            "Step: 705, Loss: 0.00033291714498773217\n",
            "0.026515151515151516\n",
            "Step: 706, Loss: 6.606079114135355e-05\n",
            "0.02127659574468085\n",
            "Step: 707, Loss: 0.00012585568765643984\n",
            "0.0457516339869281\n",
            "Step: 708, Loss: 3.647407720563933e-05\n",
            "0.02127659574468085\n",
            "Step: 709, Loss: 0.0017941108671948314\n",
            "0.054054054054054064\n",
            "Step: 710, Loss: 4.580409949994646e-05\n",
            "0.0\n",
            "Step: 711, Loss: 0.0034485734067857265\n",
            "0.0\n",
            "Step: 712, Loss: 0.0031377749983221292\n",
            "0.020512820512820513\n",
            "Step: 713, Loss: 0.0010878584580495954\n",
            "0.03018867924528302\n",
            "Step: 714, Loss: 1.2635731536647654e-06\n",
            "0.012295081967213113\n",
            "Step: 715, Loss: 0.00042247114470228553\n",
            "0.04583333333333333\n",
            "Step: 716, Loss: 0.00013858005695510656\n",
            "0.010830324909747292\n",
            "Step: 717, Loss: 0.00041812448762357235\n",
            "0.036290322580645164\n",
            "Step: 718, Loss: 4.869762960879598e-06\n",
            "0.0034722222222222225\n",
            "Step: 719, Loss: 0.0011440471280366182\n",
            "0.007936507936507936\n",
            "Step: 720, Loss: 0.00103323336225003\n",
            "0.0\n",
            "Step: 721, Loss: 0.0\n",
            "0.0\n",
            "Step: 722, Loss: 1.009759034786839e-05\n",
            "0.0\n",
            "Step: 723, Loss: 4.0378759877057746e-05\n",
            "0.006993006993006993\n",
            "Step: 724, Loss: 6.416051292035263e-06\n",
            "0.010101010101010102\n",
            "Step: 725, Loss: 6.738511274306802e-06\n",
            "0.015037593984962405\n",
            "Step: 726, Loss: 6.820256430728477e-07\n",
            "0.016166281755196306\n",
            "Step: 727, Loss: 8.216069545596838e-06\n",
            "0.014150943396226417\n",
            "Step: 728, Loss: 6.457966810557991e-05\n",
            "0.07235142118863049\n",
            "Step: 729, Loss: 0.002214618492871523\n",
            "0.18\n",
            "Step: 730, Loss: 0.022819776087999344\n",
            "0.03619909502262444\n",
            "Step: 731, Loss: 4.509985046752263e-06\n",
            "0.05217391304347826\n",
            "Step: 732, Loss: 0.00021487804770004004\n",
            "0.043478260869565216\n",
            "Step: 733, Loss: 0.0011138682020828128\n",
            "0.0\n",
            "Step: 734, Loss: 2.9675175028387457e-05\n",
            "0.015151515151515152\n",
            "Step: 735, Loss: 7.888530672062188e-05\n",
            "0.013986013986013986\n",
            "Step: 736, Loss: 2.92346921924036e-05\n",
            "0.016722408026755852\n",
            "Step: 737, Loss: 4.315645855967887e-05\n",
            "0.009925558312655087\n",
            "Step: 738, Loss: 3.283009164078976e-06\n",
            "0.011764705882352941\n",
            "Step: 739, Loss: 2.3663151296204887e-06\n",
            "0.018604651162790697\n",
            "Step: 740, Loss: 1.3984365068608895e-05\n",
            "0.009389671361502348\n",
            "Step: 741, Loss: 0.0003402376896701753\n",
            "0.0215633423180593\n",
            "Step: 742, Loss: 9.183189104078338e-05\n",
            "0.015822784810126583\n",
            "Step: 743, Loss: 0.0003481672320049256\n",
            "0.034482758620689655\n",
            "Step: 744, Loss: 1.0294827006873675e-05\n",
            "0.047619047619047616\n",
            "Step: 745, Loss: 0.0007495756144635379\n",
            "0.05128205128205128\n",
            "Step: 746, Loss: 0.0010839389869943261\n",
            "0.0\n",
            "Step: 747, Loss: 0.00011866849672514945\n",
            "0.0234375\n",
            "Step: 748, Loss: 0.00012074453115928918\n",
            "0.008403361344537815\n",
            "Step: 749, Loss: 1.729735595290549e-05\n",
            "0.004796163069544364\n",
            "Step: 750, Loss: 0.00011717042070813477\n",
            "0.02127659574468085\n",
            "Step: 751, Loss: 1.6909567420952953e-05\n",
            "0.018518518518518517\n",
            "Step: 752, Loss: 5.5415430466609905e-08\n",
            "0.02570694087403599\n",
            "Step: 753, Loss: 2.894515091611538e-05\n",
            "0.016181229773462782\n",
            "Step: 754, Loss: 3.3216976589756086e-05\n",
            "0.024615384615384615\n",
            "Step: 755, Loss: 1.3260901141620707e-06\n",
            "0.03496503496503497\n",
            "Step: 756, Loss: 9.827460598899052e-05\n",
            "0.015873015873015872\n",
            "Step: 757, Loss: 0.0002144991303794086\n",
            "0.0\n",
            "Step: 758, Loss: 0.0008131488575600088\n",
            "0.031746031746031744\n",
            "Step: 759, Loss: 2.7497699193190783e-05\n",
            "0.03355704697986577\n",
            "Step: 760, Loss: 0.00029697915306314826\n",
            "0.013422818791946308\n",
            "Step: 761, Loss: 2.079026489809621e-05\n",
            "0.01488833746898263\n",
            "Step: 762, Loss: 3.4310440241824836e-05\n",
            "0.024128686327077747\n",
            "Step: 763, Loss: 2.840794695657678e-05\n",
            "0.01977401129943503\n",
            "Step: 764, Loss: 8.24142807687167e-06\n",
            "0.026143790849673203\n",
            "Step: 765, Loss: 5.021115157433087e-06\n",
            "0.028925619834710745\n",
            "Step: 766, Loss: 9.194452104566153e-06\n",
            "0.0875\n",
            "Step: 767, Loss: 0.0035993566270917654\n",
            "0.0\n",
            "Step: 768, Loss: 0.0008847142453305423\n",
            "0.0\n",
            "Step: 769, Loss: 0.0016379584558308125\n",
            "0.058823529411764705\n",
            "Step: 770, Loss: 0.0004223232390359044\n",
            "0.030303030303030304\n",
            "Step: 771, Loss: 3.848224150715396e-05\n",
            "0.06329113924050633\n",
            "Step: 772, Loss: 0.0018461124273017049\n",
            "0.018072289156626505\n",
            "Step: 773, Loss: 1.77040074049728e-05\n",
            "0.020618556701030927\n",
            "Step: 774, Loss: 1.0375870260759257e-05\n",
            "0.04216867469879518\n",
            "Step: 775, Loss: 0.00028086212114430964\n",
            "0.09763313609467456\n",
            "Step: 776, Loss: 0.005213380325585604\n",
            "0.007194244604316547\n",
            "Step: 777, Loss: 0.000490069855004549\n",
            "0.011811023622047244\n",
            "Step: 778, Loss: 0.00013092567678540945\n",
            "0.02066115702479339\n",
            "Step: 779, Loss: 0.00023306204820983112\n",
            "0.020979020979020983\n",
            "Step: 780, Loss: 0.00033112146775238216\n",
            "0.08695652173913043\n",
            "Step: 781, Loss: 0.0013408423401415348\n",
            "0.0\n",
            "Step: 782, Loss: 0.0023846158292144537\n",
            "0.047619047619047616\n",
            "Step: 783, Loss: 1.3619830951938638e-06\n",
            "0.02654867256637168\n",
            "Step: 784, Loss: 0.00032431347062811255\n",
            "0.11111111111111112\n",
            "Step: 785, Loss: 0.00718835461884737\n",
            "0.0228310502283105\n",
            "Step: 786, Loss: 3.706267671077512e-05\n",
            "0.019801980198019802\n",
            "Step: 787, Loss: 0.00011197746061952785\n",
            "0.013824884792626729\n",
            "Step: 788, Loss: 0.0003235394542571157\n",
            "0.02884615384615385\n",
            "Step: 789, Loss: 1.874760891951155e-05\n",
            "0.023696682464454975\n",
            "Step: 790, Loss: 8.267028169939294e-05\n",
            "0.01932367149758454\n",
            "Step: 791, Loss: 0.00014023894618730992\n",
            "0.06422018348623854\n",
            "Step: 792, Loss: 0.0012336877407506108\n",
            "0.05454545454545454\n",
            "Step: 793, Loss: 3.059482332901098e-05\n",
            "0.0\n",
            "Step: 794, Loss: 0.003379915375262499\n",
            "0.03333333333333333\n",
            "Step: 795, Loss: 0.000493593339342624\n",
            "0.03225806451612903\n",
            "Step: 796, Loss: 0.0004527292330749333\n",
            "0.020512820512820513\n",
            "Step: 797, Loss: 0.00011314205039525405\n",
            "0.04878048780487805\n",
            "Step: 798, Loss: 0.000265034002950415\n",
            "0.0043859649122807015\n",
            "Step: 799, Loss: 0.0008951218333095312\n",
            "0.015503875968992248\n",
            "Step: 800, Loss: 0.0003963520284742117\n",
            "0.01485148514851485\n",
            "Step: 801, Loss: 0.0002237494772998616\n",
            "0.026905829596412554\n",
            "Step: 802, Loss: 3.631296567618847e-05\n",
            "0.008888888888888889\n",
            "Step: 803, Loss: 0.001039600814692676\n",
            "0.008130081300813009\n",
            "Step: 804, Loss: 0.000989008229225874\n",
            "0.0\n",
            "Step: 805, Loss: 0.0\n",
            "0.025\n",
            "Step: 806, Loss: 0.00046788083272986114\n",
            "0.047619047619047616\n",
            "Step: 807, Loss: 0.0016707229660823941\n",
            "0.04347826086956521\n",
            "Step: 808, Loss: 0.0011108239414170384\n",
            "0.018518518518518517\n",
            "Step: 809, Loss: 2.4340457457583398e-05\n",
            "0.0170316301703163\n",
            "Step: 810, Loss: 1.4078347376766942e-09\n",
            "0.015659955257270694\n",
            "Step: 811, Loss: 2.2401411115424708e-05\n",
            "0.017412935323383085\n",
            "Step: 812, Loss: 4.0360635466640815e-05\n",
            "0.01907356948228883\n",
            "Step: 813, Loss: 6.466553168138489e-05\n",
            "0.014925373134328358\n",
            "Step: 814, Loss: 0.00024039392883423716\n",
            "0.03319502074688797\n",
            "Step: 815, Loss: 1.924719583712431e-07\n",
            "0.03684210526315789\n",
            "Step: 816, Loss: 2.2116839915042874e-08\n",
            "0.017543859649122806\n",
            "Step: 817, Loss: 6.077029320294969e-05\n",
            "0.07894736842105263\n",
            "Step: 818, Loss: 0.005380354356020689\n",
            "0.047619047619047616\n",
            "Step: 819, Loss: 0.0017161807045340538\n",
            "0.0392156862745098\n",
            "Step: 820, Loss: 0.0009291812893934548\n",
            "0.015306122448979593\n",
            "Step: 821, Loss: 2.485176628397312e-05\n",
            "0.009324009324009324\n",
            "Step: 822, Loss: 6.606218903471017e-06\n",
            "0.005050505050505051\n",
            "Step: 823, Loss: 7.055694732116535e-05\n",
            "0.00847457627118644\n",
            "Step: 824, Loss: 4.218048707116395e-05\n",
            "0.023923444976076555\n",
            "Step: 825, Loss: 1.1089418876508716e-05\n",
            "0.02864583333333333\n",
            "Step: 826, Loss: 3.8043724543967983e-06\n",
            "0.029411764705882353\n",
            "Step: 827, Loss: 2.0521469195955433e-05\n",
            "0.032467532467532464\n",
            "Step: 828, Loss: 2.29361485253321e-05\n",
            "0.034482758620689655\n",
            "Step: 829, Loss: 0.000224444578634575\n",
            "0.09523809523809523\n",
            "Step: 830, Loss: 0.006004004273563623\n",
            "0.0\n",
            "Step: 831, Loss: 0.00012701985542662442\n",
            "0.017543859649122806\n",
            "Step: 832, Loss: 2.247624252049718e-05\n",
            "0.02425876010781671\n",
            "Step: 833, Loss: 0.00013924328959546983\n",
            "0.006651884700665189\n",
            "Step: 834, Loss: 1.6357484128093347e-05\n",
            "0.016666666666666666\n",
            "Step: 835, Loss: 6.336567821563222e-07\n",
            "0.00804289544235925\n",
            "Step: 836, Loss: 0.00012029201025143266\n",
            "0.008571428571428572\n",
            "Step: 837, Loss: 0.0001423396315658465\n",
            "0.02072538860103627\n",
            "Step: 838, Loss: 1.5162182762651355e-06\n",
            "0.012578616352201259\n",
            "Step: 839, Loss: 0.00011873537732753903\n",
            "0.0661764705882353\n",
            "Step: 840, Loss: 0.0017060874961316586\n",
            "0.042253521126760556\n",
            "Step: 841, Loss: 0.00016399072774220258\n",
            "0.041666666666666664\n",
            "Step: 842, Loss: 0.00019505903765093535\n",
            "0.014285714285714284\n",
            "Step: 843, Loss: 0.00013569930160883814\n",
            "0.02262443438914027\n",
            "Step: 844, Loss: 3.3238280593650416e-05\n",
            "0.010610079575596816\n",
            "Step: 845, Loss: 6.154430593596771e-05\n",
            "0.00746268656716418\n",
            "Step: 846, Loss: 0.00016997662896756083\n",
            "0.0055248618784530384\n",
            "Step: 847, Loss: 0.00017221082816831768\n",
            "0.016867469879518072\n",
            "Step: 848, Loss: 1.0140527528790244e-08\n",
            "0.02710027100271003\n",
            "Step: 849, Loss: 5.6060098359012045e-06\n",
            "0.0\n",
            "Step: 850, Loss: 0.0006928927614353597\n",
            "0.024390243902439025\n",
            "Step: 851, Loss: 1.0929669770121109e-05\n",
            "0.023809523809523808\n",
            "Step: 852, Loss: 2.9264559998409823e-05\n",
            "0.01694915254237288\n",
            "Step: 853, Loss: 0.0005018218071199954\n",
            "0.10638297872340426\n",
            "Step: 854, Loss: 0.00475974939763546\n",
            "0.012658227848101266\n",
            "Step: 855, Loss: 0.0005410760059021413\n",
            "0.017341040462427744\n",
            "Step: 856, Loss: 0.00027694302843883634\n",
            "0.005934718100890208\n",
            "Step: 857, Loss: 0.0002710013068281114\n",
            "0.012376237623762377\n",
            "Step: 858, Loss: 0.00013149730511941016\n",
            "0.05714285714285715\n",
            "Step: 859, Loss: 0.0008249955135397613\n",
            "0.01893939393939394\n",
            "Step: 860, Loss: 6.160712655400857e-05\n",
            "0.034210526315789476\n",
            "Step: 861, Loss: 1.7146239770227112e-05\n",
            "0.023728813559322035\n",
            "Step: 862, Loss: 9.459542343392968e-05\n",
            "0.006535947712418301\n",
            "Step: 863, Loss: 0.0009083882323466241\n",
            "0.04054054054054055\n",
            "Step: 864, Loss: 0.0009409344638697803\n",
            "0.06779661016949153\n",
            "Step: 865, Loss: 0.0038739137817174196\n",
            "0.046511627906976744\n",
            "Step: 866, Loss: 0.001622646115720272\n",
            "0.04861111111111111\n",
            "Step: 867, Loss: 0.0014635486295446754\n",
            "0.01929260450160772\n",
            "Step: 868, Loss: 8.222519682021812e-05\n",
            "0.01288659793814433\n",
            "Step: 869, Loss: 1.2014527328574331e-06\n",
            "0.0199501246882793\n",
            "Step: 870, Loss: 4.361105675343424e-05\n",
            "0.0030303030303030303\n",
            "Step: 871, Loss: 0.0004223671858198941\n",
            "0.0158311345646438\n",
            "Step: 872, Loss: 0.00012029125355184078\n",
            "0.014749262536873156\n",
            "Step: 873, Loss: 0.00023342874192167073\n",
            "0.027700831024930747\n",
            "Step: 874, Loss: 3.0245697416830808e-05\n",
            "0.026845637583892617\n",
            "Step: 875, Loss: 9.167681128019467e-05\n",
            "0.014492753623188408\n",
            "Step: 876, Loss: 2.7703490559360944e-05\n",
            "0.0\n",
            "Step: 877, Loss: 0.00032167424797080457\n",
            "0.016666666666666666\n",
            "Step: 878, Loss: 3.0302946470328607e-05\n",
            "0.02142857142857143\n",
            "Step: 879, Loss: 5.0807258958229795e-05\n",
            "0.017142857142857144\n",
            "Step: 880, Loss: 2.1524776457226835e-05\n",
            "0.03209876543209877\n",
            "Step: 881, Loss: 0.00045750668505206704\n",
            "0.012594458438287154\n",
            "Step: 882, Loss: 1.2967675502295606e-05\n",
            "0.028985507246376812\n",
            "Step: 883, Loss: 0.00010056640894617885\n",
            "0.029490616621983913\n",
            "Step: 884, Loss: 7.967873534653336e-05\n",
            "0.009836065573770493\n",
            "Step: 885, Loss: 0.00015259601059369743\n",
            "0.014285714285714284\n",
            "Step: 886, Loss: 8.773537410888821e-05\n",
            "0.025974025974025976\n",
            "Step: 887, Loss: 7.522282317040663e-07\n",
            "0.19117647058823528\n",
            "Step: 888, Loss: 0.026118487119674683\n",
            "0.5263157894736842\n",
            "Step: 889, Loss: 0.24801886081695557\n",
            "0.08333333333333333\n",
            "Step: 890, Loss: 0.003111038589850068\n",
            "0.013513513513513514\n",
            "Step: 891, Loss: 0.00014475677744485438\n",
            "0.02064896755162242\n",
            "Step: 892, Loss: 7.523028216382954e-06\n",
            "0.011961722488038277\n",
            "Step: 893, Loss: 8.636360871605575e-05\n",
            "0.020942408376963352\n",
            "Step: 894, Loss: 3.430146080063423e-06\n",
            "0.03359173126614987\n",
            "Step: 895, Loss: 0.00027646799571812153\n",
            "0.031161473087818695\n",
            "Step: 896, Loss: 0.00026280098245479167\n",
            "0.043137254901960784\n",
            "Step: 897, Loss: 0.00015011176583357155\n",
            "0.038314176245210725\n",
            "Step: 898, Loss: 1.687325129751116e-05\n",
            "0.025806451612903226\n",
            "Step: 899, Loss: 1.1227872164454311e-05\n",
            "0.012345679012345678\n",
            "Step: 900, Loss: 0.0009081297903321683\n",
            "0.07407407407407407\n",
            "Step: 901, Loss: 0.0011471140896901488\n",
            "0.0\n",
            "Step: 902, Loss: 0.0014661872992292047\n",
            "0.007575757575757577\n",
            "Step: 903, Loss: 0.0008111863862723112\n",
            "0.013986013986013986\n",
            "Step: 904, Loss: 0.00039603898767381907\n",
            "0.02\n",
            "Step: 905, Loss: 0.00013801544264424592\n",
            "0.008955223880597015\n",
            "Step: 906, Loss: 0.00042741093784570694\n",
            "0.009836065573770493\n",
            "Step: 907, Loss: 0.0003102083574049175\n",
            "0.02464788732394366\n",
            "Step: 908, Loss: 1.2978939594177064e-05\n",
            "0.021929824561403508\n",
            "Step: 909, Loss: 1.4793308764637914e-06\n",
            "0.025380710659898477\n",
            "Step: 910, Loss: 8.773835725151002e-05\n",
            "0.0078125\n",
            "Step: 911, Loss: 0.0009075034759007394\n",
            "0.029850746268656712\n",
            "Step: 912, Loss: 0.000515670224558562\n",
            "0.0\n",
            "Step: 913, Loss: 0.0025246276054531336\n",
            "0.04255319148936171\n",
            "Step: 914, Loss: 2.7454172595753334e-05\n",
            "0.08620689655172414\n",
            "Step: 915, Loss: 0.0016384886112064123\n",
            "0.0\n",
            "Step: 916, Loss: 0.0019286760361865163\n",
            "0.01968503937007874\n",
            "Step: 917, Loss: 6.808659964008257e-05\n",
            "0.015822784810126583\n",
            "Step: 918, Loss: 0.00018446825561113656\n",
            "0.024054982817869417\n",
            "Step: 919, Loss: 0.00018285418627783656\n",
            "0.012195121951219513\n",
            "Step: 920, Loss: 0.00026456196792423725\n",
            "0.026515151515151516\n",
            "Step: 921, Loss: 4.743102545035072e-05\n",
            "0.02127659574468085\n",
            "Step: 922, Loss: 0.00010018403554568067\n",
            "0.0457516339869281\n",
            "Step: 923, Loss: 5.9494639572221786e-05\n",
            "0.02127659574468085\n",
            "Step: 924, Loss: 0.0016728099435567856\n",
            "0.054054054054054064\n",
            "Step: 925, Loss: 3.045655284950044e-05\n",
            "0.0\n",
            "Step: 926, Loss: 0.0033064677845686674\n",
            "0.0\n",
            "Step: 927, Loss: 0.003021826734766364\n",
            "0.020512820512820513\n",
            "Step: 928, Loss: 0.0010283039882779121\n",
            "0.03018867924528302\n",
            "Step: 929, Loss: 2.3541390419268282e-06\n",
            "0.012295081967213113\n",
            "Step: 930, Loss: 0.0004392865812405944\n",
            "0.04583333333333333\n",
            "Step: 931, Loss: 0.00012763649283442646\n",
            "0.019083969465648856\n",
            "Step: 932, Loss: 0.0002942787832580507\n",
            "0.036290322580645164\n",
            "Step: 933, Loss: 1.761006933520548e-05\n",
            "0.0034722222222222225\n",
            "Step: 934, Loss: 0.0010195866925641894\n",
            "0.007936507936507936\n",
            "Step: 935, Loss: 0.0009615335729904473\n",
            "0.0\n",
            "Step: 936, Loss: 0.0\n",
            "0.0\n",
            "Step: 937, Loss: 1.05754452306428e-05\n",
            "0.0\n",
            "Step: 938, Loss: 4.229212208883837e-05\n",
            "0.006993006993006993\n",
            "Step: 939, Loss: 7.604216534673469e-06\n",
            "0.010101010101010102\n",
            "Step: 940, Loss: 8.387353773287032e-06\n",
            "0.015037593984962405\n",
            "Step: 941, Loss: 1.445713905923185e-06\n",
            "0.016166281755196306\n",
            "Step: 942, Loss: 1.10074342956068e-05\n",
            "0.014150943396226417\n",
            "Step: 943, Loss: 7.33607157599181e-05\n",
            "0.07235142118863049\n",
            "Step: 944, Loss: 0.0021569158416241407\n",
            "0.18\n",
            "Step: 945, Loss: 0.022647298872470856\n",
            "0.03619909502262444\n",
            "Step: 946, Loss: 3.824774012173293e-06\n",
            "0.05217391304347826\n",
            "Step: 947, Loss: 0.00020977610256522894\n",
            "0.043478260869565216\n",
            "Step: 948, Loss: 0.0011208410141989589\n",
            "0.0\n",
            "Step: 949, Loss: 3.007822306244634e-05\n",
            "0.03773584905660377\n",
            "Step: 950, Loss: 0.0009483008761890233\n",
            "0.006896551724137931\n",
            "Step: 951, Loss: 1.2252578926563729e-05\n",
            "0.016722408026755852\n",
            "Step: 952, Loss: 4.280791472410783e-05\n",
            "0.009925558312655087\n",
            "Step: 953, Loss: 3.3555004392837873e-06\n",
            "0.011764705882352941\n",
            "Step: 954, Loss: 2.419721113255946e-06\n",
            "0.02\n",
            "Step: 955, Loss: 1.73725657077739e-05\n",
            "0.009389671361502348\n",
            "Step: 956, Loss: 0.00033145927591249347\n",
            "0.0215633423180593\n",
            "Step: 957, Loss: 8.803489618003368e-05\n",
            "0.015822784810126583\n",
            "Step: 958, Loss: 0.00034196069464087486\n",
            "0.034482758620689655\n",
            "Step: 959, Loss: 9.644470083003398e-06\n",
            "0.047619047619047616\n",
            "Step: 960, Loss: 0.0007614516653120518\n",
            "0.05128205128205128\n",
            "Step: 961, Loss: 0.0011012665927410126\n",
            "0.0\n",
            "Step: 962, Loss: 0.00012025656906189397\n",
            "0.024390243902439025\n",
            "Step: 963, Loss: 0.00010439105972182006\n",
            "0.019830028328611898\n",
            "Step: 964, Loss: 3.5754506825469434e-05\n",
            "0.0234192037470726\n",
            "Step: 965, Loss: 3.7890044040977955e-05\n",
            "0.02127659574468085\n",
            "Step: 966, Loss: 1.643093492020853e-05\n",
            "0.018518518518518517\n",
            "Step: 967, Loss: 8.18971628291365e-08\n",
            "0.02570694087403599\n",
            "Step: 968, Loss: 2.8442407710826956e-05\n",
            "0.016181229773462782\n",
            "Step: 969, Loss: 3.3599040762055665e-05\n",
            "0.024615384615384615\n",
            "Step: 970, Loss: 1.2345876712061e-06\n",
            "0.03496503496503497\n",
            "Step: 971, Loss: 9.762004629010335e-05\n",
            "0.015873015873015872\n",
            "Step: 972, Loss: 0.0002041648986050859\n",
            "0.0\n",
            "Step: 973, Loss: 0.0007922170334495604\n",
            "0.031746031746031744\n",
            "Step: 974, Loss: 3.161006316076964e-05\n",
            "0.03355704697986577\n",
            "Step: 975, Loss: 0.00029340884066186845\n",
            "0.013422818791946308\n",
            "Step: 976, Loss: 2.153049899789039e-05\n",
            "0.017326732673267328\n",
            "Step: 977, Loss: 5.262155600149754e-09\n",
            "0.013440860215053764\n",
            "Step: 978, Loss: 5.421205423772335e-05\n",
            "0.011142061281337047\n",
            "Step: 979, Loss: 0.00017006602138280869\n",
            "0.026143790849673203\n",
            "Step: 980, Loss: 1.9450849322311115e-06\n",
            "0.015444015444015444\n",
            "Step: 981, Loss: 0.00023997957760002464\n",
            "0.0875\n",
            "Step: 982, Loss: 0.00359714706428349\n",
            "0.0\n",
            "Step: 983, Loss: 0.0008770051063038409\n",
            "0.0\n",
            "Step: 984, Loss: 0.0016047408571466804\n",
            "0.058823529411764705\n",
            "Step: 985, Loss: 0.0004381161998026073\n",
            "0.030303030303030304\n",
            "Step: 986, Loss: 3.3089672797359526e-05\n",
            "0.022222222222222223\n",
            "Step: 987, Loss: 0.00014113663928583264\n",
            "0.018072289156626505\n",
            "Step: 988, Loss: 1.5636553143849596e-05\n",
            "0.020618556701030927\n",
            "Step: 989, Loss: 8.929197974794079e-06\n",
            "0.023026315789473683\n",
            "Step: 990, Loss: 2.7545305783860385e-05\n",
            "0.012698412698412698\n",
            "Step: 991, Loss: 0.0001855389418778941\n",
            "0.2643312101910828\n",
            "Step: 992, Loss: 0.05761377513408661\n",
            "0.011811023622047244\n",
            "Step: 993, Loss: 0.00015581252228002995\n",
            "0.014218009478672983\n",
            "Step: 994, Loss: 6.780472176615149e-05\n",
            "0.014925373134328358\n",
            "Step: 995, Loss: 3.24140528391581e-05\n",
            "0.08695652173913043\n",
            "Step: 996, Loss: 0.0013884336221963167\n",
            "0.0\n",
            "Step: 997, Loss: 0.0023288740776479244\n",
            "0.047619047619047616\n",
            "Step: 998, Loss: 2.1562902929872507e-06\n",
            "0.02654867256637168\n",
            "Step: 999, Loss: 0.00031909573590382934\n",
            "Training completed.\n"
          ]
        }
      ],
      "source": [
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.bandits.agents import lin_ucb_agent\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "\n",
        "# Create the environment\n",
        "env = ABCTestBanditEnv(data)\n",
        "env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "# Define time_step_spec and action_spec\n",
        "time_step_spec = env.time_step_spec()\n",
        "action_spec = env.action_spec()\n",
        "\n",
        "# Initialize the agent\n",
        "agent = lin_ucb_agent.LinearUCBAgent(\n",
        "    time_step_spec=time_step_spec,\n",
        "    action_spec=action_spec,\n",
        "    alpha=0.5,  # Adjust exploration level\n",
        "    tikhonov_weight=1.0,  # Regularization to prevent overfitting\n",
        "    dtype=tf.float32\n",
        ")\n",
        "\n",
        "# Setup Replay Buffer\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.policy.trajectory_spec,\n",
        "    batch_size=env.batch_size,\n",
        "    max_length=1000)\n",
        "\n",
        "actions_taken = []\n",
        "\n",
        "# Function to collect data from interactions\n",
        "def collect_step(env, policy):\n",
        "    time_step = env.current_time_step()\n",
        "    action_step = policy.action(time_step)\n",
        "    # Log\n",
        "    actions_taken.append(action_step.action.numpy())\n",
        "    next_time_step = env.step(action_step.action)\n",
        "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "\n",
        "    # Add trajectory to the replay buffer\n",
        "    replay_buffer.add_batch(traj)\n",
        "\n",
        "# Training Loop\n",
        "num_iterations = 1000 # Number of iterations for training\n",
        "for _ in range(num_iterations):\n",
        "    # Collect data from the environment\n",
        "    collect_step(env, agent.policy)\n",
        "\n",
        "    # Sample a batch of data from the buffer and update the agent's network\n",
        "    experience = replay_buffer.gather_all()\n",
        "    train_loss = agent.train(experience)\n",
        "    replay_buffer.clear()\n",
        "\n",
        "    print(f\"Step: {_}, Loss: {train_loss.loss.numpy()}\")\n",
        "\n",
        "print(\"Training completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "XINMsYZL5-Ft",
        "outputId": "11c14bd0-eadd-4e95-9ee8-8d6ad91dde72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      zakladni  discovery  segment\n",
            "step                              \n",
            "999       31.4       30.5     38.1\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1IAAAIjCAYAAAAJLyrXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAACNvklEQVR4nOzdd3xV9f3H8de5+2YPSEiYIeyNgDJEEFFEwYFaB7Y4fsU9a63WUUGtWxFntSrauqvFDVVqlb33HmGEPbLXXef3x4ULIQFyJeFezPv5eERzzzn3nM+9NyT3fb/LME3TRERERERERGrMEukCRERERERETjYKUiIiIiIiImFSkBIREREREQmTgpSIiIiIiEiYFKRERERERETCpCAlIiIiIiISJgUpERERERGRMClIiYiIiIiIhElBSkREREREJEwKUiIiEpUeeeQRDMOotK1FixZcc801kSnoCAYOHMjAgQNP+HX/97//YRgG//vf/074tUVEREFKRKTemzBhAoZhVPpKS0vjzDPP5Lvvvot0eUe1YsUKHnnkETZu3Fjj+0ybNo2hQ4fSuHFjXC4XzZo1Y/jw4XzwwQd1V+hxePXVV5kwYUKkyxARkcPYIl2AiIhEh7Fjx5KVlYVpmuzcuZMJEyZw3nnn8dVXXzFs2LBIlwfA6tWrsVgOfga4YsUKxowZw8CBA2nRosUx7//pp59y+eWX061bN+644w6Sk5PJycnh559/5s033+Sqq66qw+p/mVdffZUGDRpUaYk744wzKCsrw+FwRKYwEZF6TkFKREQAGDp0KD179gzdvv7660lPT+fDDz+MmiDldDqP6/6PPPIIHTp0YNasWVUCyK5du47r3CeaxWLB5XJFugwRkXpLXftERKRaSUlJuN1ubLbKn7k9++yz9O3bl9TUVNxuNz169OBf//pXlfsbhsGtt97KxIkT6dSpE06nk44dOzJp0qQqx06bNo1evXrhcrnIzs7mb3/7W7U1HTpGasKECVx22WUAnHnmmaFuiUcbM7R+/Xp69epVbStOWlpapduBQIBx48bRsWNHXC4X6enp3HDDDeTl5R3x/AdUVFTwl7/8hVatWuF0OmnatCn33nsvFRUVVY795z//yamnnkpMTAzJycmcccYZ/Oc//wk93uXLl/PTTz+FHt+B8VhHGiP16aef0qNHD9xuNw0aNODqq69m69atlY655ppriIuLY+vWrVx00UXExcXRsGFD7rnnHvx+/zEfn4iIqEVKRET2KygoYM+ePZimya5du3jppZcoLi7m6quvrnTciy++yAUXXMDIkSPxeDx89NFHXHbZZXz99decf/75lY6dNm0an3/+OTfffDPx8fGMHz+eSy65hM2bN5OamgrA0qVLOeecc2jYsCGPPPIIPp+Pv/zlL6Snpx+13jPOOIPbb7+d8ePH8+c//5n27dsDhP5fnebNmzNlyhRyc3Np0qTJUc9/ww03MGHCBK699lpuv/12cnJyePnll1m4cCHTp0/HbrdXe79AIMAFF1zAtGnTGD16NO3bt2fp0qW88MILrFmzhokTJ4aOHTNmDI888gh9+/Zl7NixOBwOZs+ezX//+1/OOeccxo0bx2233UZcXBwPPPAAwFGflwP19urViyeeeIKdO3fy4osvMn36dBYuXEhSUlLoWL/fz5AhQzjttNN49tln+eGHH3juuefIzs7mpptuOupzIyIigCkiIvXaO++8YwJVvpxOpzlhwoQqx5eWlla67fF4zE6dOpmDBg2qtB0wHQ6HuW7dutC2xYsXm4D50ksvhbZddNFFpsvlMjdt2hTatmLFCtNqtZqH/5lq3ry5OWrUqNDtTz/91ATMH3/8sUaP9a233grVdeaZZ5oPPfSQOXXqVNPv91c6burUqSZgvv/++5W2T5o0qcr2AQMGmAMGDAjd/sc//mFaLBZz6tSple77+uuvm4A5ffp00zRNc+3atabFYjEvvvjiKtcPBAKh7zt27Fjp/Af8+OOPlR67x+Mx09LSzE6dOpllZWWh477++msTMB9++OHQtlGjRpmAOXbs2Ern7N69u9mjR48q1xIRkarUtU9ERAB45ZVX+P777/n+++/55z//yZlnnsn//d//8fnnn1c6zu12h77Py8ujoKCA/v37s2DBgirnHDx4MNnZ2aHbXbp0ISEhgQ0bNgDBVpHJkydz0UUX0axZs9Bx7du3Z8iQIbX9ELnuuuuYNGkSAwcOZNq0aTz66KP079+f1q1bM2PGjNBxn376KYmJiZx99tns2bMn9NWjRw/i4uL48ccfj3iNTz/9lPbt29OuXbtK9x00aBBA6L4TJ04kEAjw8MMPV5pAA6gy7XtNzJs3j127dnHzzTdXGjt1/vnn065dO7755psq97nxxhsr3e7fv3/otRERkaNT1z4REQHg1FNPrTTZxJVXXkn37t259dZbGTZsWGhc0ddff81jjz3GokWLKo35qe7N/6Hh6IDk5OTQOKPdu3dTVlZG69atqxzXtm1bvv322+N+XIcbMmQIQ4YMobS0lPnz5/Pxxx/z+uuvM2zYMFatWkVaWhpr166loKCgyripA442McXatWtZuXIlDRs2POp9169fj8VioUOHDsf/oIBNmzYBweftcO3atWPatGmVtrlcrio1HvraiIjI0SlIiYhItSwWC2eeeSYvvvgia9eupWPHjkydOpULLriAM844g1dffZWMjAzsdjvvvPNOteswWa3Was9tmmZdl39MMTEx9O/fn/79+9OgQQPGjBnDd999x6hRowgEAqSlpfH+++9Xe98jhSQIjpHq3Lkzzz//fLX7mzZtWiv1H68jvTYiIlIzClIiInJEPp8PgOLiYgA+++wzXC4XkydPrjQV+TvvvPOLzt+wYUPcbjdr166tsm/16tXHvP8v6QJXnQMtcdu3bwcgOzubH374gX79+lXqylgT2dnZLF68mLPOOuuo9WVnZxMIBFixYgXdunU74nE1fYzNmzcHgs/bgW6EB6xevTq0X0REaofGSImISLW8Xi//+c9/cDgcoZnwrFYrhmFUmiJ748aNlWaiC4fVamXIkCFMnDiRzZs3h7avXLmSyZMnH/P+sbGxAOTn59foelOmTKl2+4EuhAe6xf3mN7/B7/fz6KOPVjnW5/Md9Xq/+c1v2Lp1K2+++WaVfWVlZZSUlABw0UUXYbFYGDt2LIFAoNJxh7bYxcbG1ujx9ezZk7S0NF5//fVKXS6/++47Vq5cWWVGRREROT5qkRIRESD4hnvVqlVAcBzPBx98wNq1a7nvvvtISEgAghMXPP/885x77rlcddVV7Nq1i1deeYVWrVqxZMmSX3TdMWPGMGnSJPr378/NN9+Mz+fjpZdeomPHjsc8Z7du3bBarTz11FMUFBTgdDoZNGjQEcc2XXjhhWRlZTF8+HCys7MpKSnhhx9+4KuvvqJXr14MHz4cgAEDBnDDDTfwxBNPsGjRIs455xzsdjtr167l008/5cUXX+TSSy+t9hq//e1v+eSTT7jxxhv58ccf6devH36/n1WrVvHJJ58wefJkevbsSatWrXjggQdCE16MGDECp9PJ3LlzyczM5IknngCgR48evPbaazz22GO0atWKtLS0Ki1OAHa7naeeeoprr72WAQMGcOWVV4amP2/RogV33XVXOC+LiIgcS6SnDRQRkciqbvpzl8tlduvWzXzttdcqTcVtmsEpxFu3bm06nU6zXbt25jvvvGP+5S9/qTJVOWDecsstVa53+BTmpmmaP/30k9mjRw/T4XCYLVu2NF9//fVqz1ndfd98802zZcuWoenSjzYV+ocffmheccUVZnZ2tul2u02Xy2V26NDBfOCBB8zCwsIqx7/xxhtmjx49TLfbbcbHx5udO3c27733XnPbtm2hYw6f/tw0g1ORP/XUU2bHjh1Np9NpJicnmz169DDHjBljFhQUVDr27bffNrt37x46bsCAAeb3338f2r9jxw7z/PPPN+Pj400gdK3Dpz8/4OOPPw6dLyUlxRw5cqSZm5tb6ZhRo0aZsbGxVR5vdc+5iIhUzzDNKBjxKyIiIiIichLRGCkREREREZEwKUiJiIiIiIiESUFKREREREQkTApSIiIiIiIiYVKQEhERERERCZOClIiIiIiISJi0IC8QCATYtm0b8fHxGIYR6XJERERERCRCTNOkqKiIzMxMLJYjtzspSAHbtm2jadOmkS5DRERERESixJYtW2jSpMkR9ytIAfHx8UDwyUpISIhwNSIiIiIiEimFhYU0bdo0lBGOREEKQt35EhISFKREREREROSYQ3402YSIiIiIiEiYFKRERERERETCpCAlIiIiIiISJo2RqiG/34/X6410GXKc7HY7Vqs10mWIiIiIyElOQaoGiouLyc3NxTTNSJcix8kwDJo0aUJcXFykSxERERGRk5iC1DH4/X5yc3OJiYmhYcOGWrD3JGaaJrt37yY3N5fWrVurZUpEREREfjEFqWPwer2YpknDhg1xu92RLkeOU8OGDdm4cSNer1dBSkRERER+MU02UUNqifp10OsoIiIiIrVBQUpERERERCRMClIiIiIiIiJh0hipX2hr46Yn9HqNt245odcbOHAg3bp1Y9y4cdXuv+aaa8jPz2fixIm/+BobN24kKyuLhQsX0q1btxrd55FHHmHixIksWrToF19XREREROR4qUVKTir33HMPU6ZMiXQZIiIiIlLPqUVKTipxcXFaA0pEREREIk4tUr9SGzduxDCMKl8DBw5k7969XHnllTRu3JiYmBg6d+7Mhx9+eNTzffPNNyQmJvL+++9Xu3/SpEmcfvrpJCUlkZqayrBhw1i/fn2lY+bMmUP37t1xuVz07NmThQsXVtr/v//9D8MwmDJlCj179iQmJoa+ffuyevXq0DGPPPJIjbsBioiIiIjUFQWpX6mmTZuyffv20NfChQtJTU3ljDPOoLy8nB49evDNN9+wbNkyRo8ezW9/+1vmzJlT7bk++OADrrzySt5//31GjhxZ7TElJSXcfffdzJs3jylTpmCxWLj44osJBAIAFBcXM2zYMDp06MD8+fN55JFHuOeee6o91wMPPMBzzz3HvHnzsNlsXHfddbXzpIiIiIiI1JKIBqmff/6Z4cOHk5mZiWEYVSYuME2Thx9+mIyMDNxuN4MHD2bt2rWVjtm3bx8jR44kISGBpKQkrr/+eoqLi0/go4hOVquVRo0a0ahRI5KSkrjxxhvp06cPjzzyCI0bN+aee+6hW7dutGzZkttuu41zzz2XTz75pMp5XnnlFW6++Wa++uorhg0bdsTrXXLJJYwYMYJWrVrRrVs33n77bZYuXcqKFSuAYBgLBAK89dZbdOzYkWHDhvHHP/6x2nM9/vjjDBgwgA4dOnDfffcxY8YMysvLa+eJERERERGpBRENUiUlJXTt2pVXXnml2v1PP/0048eP5/XXX2f27NnExsYyZMiQSm+qR44cyfLly/n+++/5+uuv+fnnnxk9evSJeggnheuuu46ioiI++OADLBYLfr+fRx99lM6dO5OSkkJcXByTJ09m8+bNle73r3/9i7vuuovvv/+eAQMGHPUaa9eu5corr6Rly5YkJCTQokULgNA5V65cSZcuXXC5XKH79OnTp9pzdenSJfR9RkYGALt27Qr7cYuIiIiI1JWITjYxdOhQhg4dWu0+0zQZN24cDz74IBdeeCEA7733Hunp6UycOJErrriClStXMmnSJObOnUvPnj0BeOmllzjvvPN49tlnyczMPGGPJVo99thjTJ48mTlz5hAfHw/AM888w4svvsi4cePo3LkzsbGx3HnnnXg8nkr37d69OwsWLODtt9+mZ8+eGIZxxOsMHz6c5s2b8+abb5KZmUkgEKBTp05VzlkTdrs99P2Bax7oIigiIiIiEg2idta+nJwcduzYweDBg0PbEhMTOe2005g5cyZXXHEFM2fOJCkpKRSiAAYPHozFYmH27NlcfPHF1Z67oqKCioqK0O3CwsK6eyC1pMRbEvZ9Jn4+kbFjx/L5V5/TqFmj0Dl+nvoz5w8/n4svDz4/gUCA1atX0659u9AxftNPs6xmPPrkoww9eygBI8DzLz4fOrcv4MMf8FPiLWHv3r2sXr2a8a+Np/fpvQGYMX0GAOW+ckq8JbRs05L3/vEee4v2hlqlfpr+EwBl3jJKvCWU+cpCj9XutYf2AZR6SynxluDxewiYgV/0fABUeCvw+D0s37Mcw37s40VERESk7jWNb0rDmLRIlxGWqA1SO3bsACA9Pb3S9vT09NC+HTt2kJZW+Qm32WykpKSEjqnOE088wZgxY2q54rq1rXhbWMevWbGG31/3e35/++9JaZ7C4vWLAbA77KQ3T2fyl5P5esrXJCQl8M6r77Bj5w6at2keuo7H76HEU0JsZiwTJk7gtxf8lopABQ888QAApb5SyvxlbCveRsAeICkliVdefwVLgoVtudt4duyzAOwr38e24m30H9Yf8yGT6/7vOm6880Zyt+Ty/HPBYLarbBfbirext2wvADtKdlBqKw3tA9hZuhN7sZ0iTxHegDfs5+MAv8dPfkU+Exa9zV7fnl90DhERERGpXaM738Cw7AsiXUZYojZI1aX777+fu+++O3S7sLCQpk2bhnWOxlu31HZZAHh8fnYU76PCzD+u8yxbtIyy0jJefe5VXn3u1dD2U/udysvvvcyWTVu4/rLrcbldXP67yxl8/mCKCouqPVfL1i15d+K7/PaC32KxWrj/sfsr7bdYLLzw9xd47L7HOL/f+WS1yuKhJx/i6uFXh46JjYvlbx/8jYf/8DAXDryQVm1bcc9f7uG2Ubcd1+MUEREREYkEwzRNM9JFQHAszL///W8uuugiADZs2EB2djYLFy6stG7QgAED6NatGy+++CJvv/02f/jDH8jLywvt9/l8uFwuPv300yN27TtcYWEhiYmJFBQUkJCQUGlfeXk5OTk5ZGVlVZoooa54fQG2F+897iAl1fN7/OzM3cmE7WqREhEREYkW0dQidbRscKioXUcqKyuLRo0aMWXKlNC2wsJCZs+eHZrtrU+fPuTn5zN//vzQMf/9738JBAKcdtppJ7zm2mECR57UQUREREREIi+iXfuKi4tZt25d6HZOTg6LFi0iJSWFZs2aceedd/LYY4/RunVrsrKyeOihh8jMzAy1WrVv355zzz2X3//+97z++ut4vV5uvfVWrrjiipN7xr6oaCMUEREREZEjiWiQmjdvHmeeeWbo9oFxS6NGjWLChAnce++9lJSUMHr0aPLz8zn99NOZNGlSpS5277//PrfeeitnnXUWFouFSy65hPHjx5/wxyIiIiIiIvVHRIPUwIEDOdoQLcMwGDt2LGPHjj3iMSkpKXzwwQd1UZ6IiIiIiEi1onaMlIiIiIiISLRSkBIREREREQmTglSU0Xx9IiIiIiLRT0FKREREREQkTApSUUdzn4uIiIiIRLuIztp3Muv9l8kn9Hr/uLNlrZzn6uFX075Tex544gHO7Homo24cxTU3XVMr5xYRERERqS8UpOqxz6Z8hjvGHekyREREREROOgpS9VhKg5RIl3BcvF4vdrs90mWIiIiISD2kMVK/YqUlpfzxpj/SrWk3+rXvx1svv1Vp/5ldz2TCaxMAME2T8U+OZ0DnAXRs1JHTO5zOo/c9GjrWU+HhmUee4YxOZ9CxUUcG9xjMp//4NLR/zvQ5XDL4Ejo26ki/9v14Zswz+Hw+AD6a8BGndzidQCBQ6fo3jbyJ+2+9P3T7h29/4KKBF9EpoxODug/ipadeCp0DoE1KGz54+wNuvOpGujbpyqvPvsrgHoN566XKj2vF0hW0SWnDpg2bju8JFBERERE5ArVI/Yo9/ZenmTt9Lq/+81VSG6by/KPPs3zxctp3al/l2MlfTmbCaxN44e8v0Lp9a3bv3M2qZatC+/940x9ZNHcRDz75IO06tSN3Uy55e/MA2LFtB7+//PdcfOXFPP3q02xYu4EH73wQp9PJ7ffdztCLhvLofY8ya+os+g7oC0B+Xj4/T/mZNz9+E4C5M+dy70338uCTD9KzT0+25GzhobseAuC2P90WquOlp17inofv4c9//TM2mw2H08FnH3zG9bddHzrm8/c/p1ffXjRv2bz2n1QRERERERSkfrVKikv49J+f8uzrz4bCy1OvPsUZnc6o9vhtudtomN6QvgP7YrfbyWySSdceXQHIWZfDdxO/453P36HfwH4ANGvRLHTfD97+gEaNG/GXp/+CYRhkt8lm145dPDvmWW6991YSkxI5Y/AZfP2vr0O1TPpiEsmpyfTu3xuAl59+mdF3jmbElSNC57/jz3fwzCPPVApSwy8ZziUjLwndHnHlCMY/MZ7F8xfTtUdXvF4vX332FX8a+6faeipFRERERKpQ175fqc0bN+P1eOnas2toW1JyElmtsqo9fuiFQykvK+es7mfxwB0P8J+v/xPqVrdy6UqsViun9ju12vuuX7Oe7j27YxgHlxM+5bRTKCkuYce2HQBccOkFTP5qMp4KDwBf/esrzr/4fCyW4I/gqmWreOWZV+jWtFvo68E7H2TXjl2UlZaFztupe6dK107PSGfgOQP57P3PAPjvpP/iqfAw9MKhYT1fIiIiIiLhUJASADKaZDB5zmQeefYRXG4XY/44hpHnj8Tr9eJyu477/IPOHYRpmvz4nx/ZnrudeTPnccFlF4T2l5aUcvt9t/PFT1+Evr6e9jXfz/sep8sZOq66WQYv++1lfPP5N5SXlfP5B59z3sXnaTZCEREREalT6tr3K9WsRTPsdjuL5y0ms0kmAAX5BWxcv5FT+1bfsuRyuxh07iAGnTuIkdeP5NzTzmXNijW06dCGQCDAnOlzQl37DpXdJpvJX03GNM1Qq9SC2QuIjYulUWYjAJwuJ+cMO4evPv2KzTmbyWqVRceuHUPn6NilIzlrc37RuKYBZw/AHevmg7c/YOqUqbz/9fthn0NEREREJBwKUr9SsXGxXHr1pTz9l6dJSkkitUEqLzz+Qqgr3eE+/+Bz/H4/XXt0xRXj4stPvsTldpHZNJPklGQuvuJi/nzbn0OTTWzbso29u/dy3sXncdV1V/Hu6+8y9k9jufr/riZnXQ7jnxzPtTdfW+l6wy8bzg1X3sDa1Wu58LILK13/lj/ewg1X3kBGkwzOveBcDIvBqmWrWLtqLXc9cNdRH6vVamXElSN47tHnaN6yOd1P7X78T6CIiIiIyFEoSP1Cs8YMqZPz+r1ecosK8Rj7jvtc9465l5KSEm686kZi42K57ubrKCosqvbY+MR43hj3Bk88+ASBQIA27dvw+gevk5ySDMCY58bw3KPPMeaPY8jbl0dmk0xuvOtGABplNuLNj9/kqb88xQVnXEBSchKXXn0pN99zc6Vr9DmjD0nJSeSszWHYpcMq7et/Vn/+9uHfeOWZV3hz/JvYbDZatm7JZb+9rEaP9dKrL+X151/nkqsuOfbBIiIiIiLHyTBN04x0EZFWWFhIYmIiBQUFJCQkVNpXXl5OTk4OWVlZuFzHP1boWGozSNUnc2fO5ZqLruGnpT/RIK3BEY/ze/zszN3JhO1vs9e35wRWKCIiIiJHMrrzDQzLvuDYB54AR8sGh1KLlJzUPBUe9u3Zx0tPvcS5F5571BAlIiIiIlJbNGtfNKr3bYQ19/VnXzOw60CKCor44yN/jHQ5IiIiIlJPqEUqCilH1dyIq0Yw4qoRkS5DREREROoZtUhFJePYh4iIiIiISMQoSEUZRSgRERERkeinIBVt1K9PRERERCTqKUiJiIiIiIiESUFKREREREQkTApSUUd9+0REREREop2mP/+FLph4/gm93nMDxp3Q64mIiIiIyJGpRUpOOuOfHM8FZ1wQ6TJEREREpB5TkBIREREREQmTgtSv2KQvJjGs3zA6Z3bm1OxTGXXxKEpLSgH45L1POPe0c+mU0Ykhpw3h/bfer3TfBbMXcMEZF9ApoxMjBo3g+2++p01KG1YsXQHA7GmzaZPShqlTpnLhgAvpnNmZ3134O/bu3stP3//EuaedS/dm3bn793dTVloWOm8gEOD1F15nULdBdM7szPD+w5n0xaTQ/gPnnfHTDEYMGkGXxl24fMjlbFi7AYDPP/icl59+mVXLVtEmpQ1tUtrw+Qef1/VTKSIiIiJSicZI/Urt2rGLu39/N3985I+cPexsSopLmDdzHqZp8uWnXzL+yfE89NRDdOjSgRVLVvDQnQ/hjnEz4soRFBcWc+NVN3LG2Wfw/BvPs3XLVv76wF+rvc5LT73Ew089jDvGzR3X3cEd192Bw+Hg+Tefp6SkhFt+ewv/ePMfjL5jNAB/e+FvfPHJF4x5bgzNs5szb8Y87rnxHlIapHBqv1ND533hsRe479H7SElN4eE/PMyfb/szH036iPMuPo81K9cwdcpUJvx7AgDxCfF1/nyKiIiIiBxKQepXavfO3fh8Ps4Zfg6NmzYGoG2HtkBwjNF9j97HkOFDAGjavCnrV6/n4wkfM+LKEXz12VdgwOPjHsfpctKqXSt2bt/Jg3c+WOU6dz5wJz169wDg0qsv5bmxz/HDgh9o1qIZAOdecC6zp85m9B2j8VR4eP2F15nw+QS6n9odgGYtmjFv1jw+mvBRpSB114N3hW6PvnM0oy8fTUV5BS63i5jYGKw2Kw3TG9bRsyciIiIicnQKUr9S7Tq1o8+APgzrN4z+g/rT78x+nHvhudjtdjbnbObPt/+5UjDy+Xyhlp2ctTm069gOp8sZ2t+lR5fqr9OxXej7Bg0b4I5xh0IUQGrDVJYsWALApg2bKCst49pLrq10Dq/HS/vO7Y943rT0NAD27tlLZpPMsJ4HEREREZG6oCD1K2W1Wpnw+QQWzF7AtB+n8c83/8kLj7/A3z74GwCPjXuMrj26VrqPxRr+kDmb/eCPkGEY2GyVf6QMwyAQCACExme98dEbpGekVzrO4XAc8bwYwf8dOI+IiIiISKQpSP2KGYZBj9496NG7B7feeysDuwxk/uz5pGWksWXjFi64rPopxLNaZ/HFp1/gqfDgcAYDztIFS4+7nuy22TicDrblbqvUjS9cdoedgF+hSkREREQiR0HqV2rxvMXM+HkGp595OqkNU1k8bzH79u4ju002t//pdh67/zHiE+Lpf1Z/PB4PyxYuoyC/gOtuuY7hlwznhcde4MG7HmT0HaPZnrudt15+CwiGs18qLj6O62+9nr8+8FcCgQA9e/ekqLCI+bPnExcfx4grR9ToPE2aNSF3cy4rlq6gUWYj4uLiQoFPREREROREUJD6hb686Js6Oa/p8bCxsASfdc9xnSc2PpZ5M+bx7uvvUlxUTOOmjbnv0fsYcPYAANwxbv7+0t956i9PERMTQ5sObRh14ygA4hLieP2D1/nLPX/hwgEX0rZDW26991bu/v3dOJ3Oo132mO78852kpKbwt3F/46GNDxGfGE/HLh258e4ba3yOIcOH8J+v/sPvLvgdhQWFPPnyk4y4qmYhTERERESkNhimaZqRLiLSCgsLSUxMpKCggISEhEr7ysvLycnJISsrC5fLVee11FaQqm1ffvol9996P/M3zsflrvvnoa74PX525u5kwva32euLrudYREREpL4a3fkGhmVXP+zkRDtaNjiUWqSkWv/+6N80bdGU9Ix0Vi1bxTOPPMPQi4ae1CFKRERERKS2KEhFoyhoI9yzaw/jnxjP7l27SUtPY+iFQ7nrwbsiXZaIiIiISFRQkJJq/f723/P7238f6TJERERERKJS+AsHiYiIiIiI1HMKUjWkOTl+TUzMaOg/KSIiIiInLQWpY7BarQB4PJ4IVyK1wfSZ+M0A5YGySJciIiIiIicxjZE6BpvNRkxMDLt378Zut2Ox1G32ND0e/F4v/oC/Tq9TL5lQnFfMhtL1lCpIiYiIiMhxUJA6BsMwyMjIICcnh02bNtX59Uyfj71lHgLW4jq/Vn1jYlLkLeLHgilExdSIIiIiInLSUpCqAYfDQevWrU9I9z7fjh28vmA5Ramf1Pm16psAAQp8BQRQa5+IiIiIHB8FqRqyWCy4XHW/GK3PZiOvwk+Bb0+dX0tERERERH4ZTTYRjTRDoIiIiIhIVFOQikpGpAsQEREREZGjUJCKSgpSIiIiIiLRTEEqCqlnn4iIiIhIdFOQEhERERERCZOClIiIiIiISJgUpKKRqTFSIiIiIiLRTEEqKilIiYiIiIhEMwUpERERERGRMClIRSHN2iciIiIiEt0UpKKQqTFSIiIiIiJRTUFKREREREQkTApS0UgtUiIiIiIiUU1BKippkJSIiIiISDRTkIpCGiMlIiIiIhLdFKSikKF1pEREREREopqClIiIiIiISJgUpKKSWqRERERERKKZglQ0OmSuCbvFHrk6RERERESkWgpSUcg8JEi5bTGRK0RERERERKqlIBWFzEO69rmsClIiIiIiItFGQSrKOSwKUiIiIiIi0UZBKso5DHekSxARERERkcMoSEWhQ8dIWVGQEhERERGJNgpSUengGCnDdEawDhERERERqY6CVDQ6pEXKCLgiV4eIiIiIiFRLQSoamcEWKQsWTL8jwsWIiIiIiMjhFKSimMPqwDSNYx8oIiIiIiInlIJUFDqwjpTT6gZTL5GIiIiISLSJ6nfpfr+fhx56iKysLNxuN9nZ2Tz66KOYh0xrZ5omDz/8MBkZGbjdbgYPHszatWsjWHXtcVhcapESEREREYlCUR2knnrqKV577TVefvllVq5cyVNPPcXTTz/NSy+9FDrm6aefZvz48bz++uvMnj2b2NhYhgwZQnl5eQQrP077c6Ld4iTKXyIRERERkXrJFukCjmbGjBlceOGFnH/++QC0aNGCDz/8kDlz5gDB1qhx48bx4IMPcuGFFwLw3nvvkZ6ezsSJE7niiisiVvvxCbZC2Q2nWqRERERERKJQVDd39O3blylTprBmzRoAFi9ezLRp0xg6dCgAOTk57Nixg8GDB4fuk5iYyGmnncbMmTOPeN6KigoKCwsrfUUTAzAwsClIiYiIiIhEpahukbrvvvsoLCykXbt2WK1W/H4/jz/+OCNHjgRgx44dAKSnp1e6X3p6emhfdZ544gnGjBlTd4UfLyMYpAzsBBSkRERERESiTlS3SH3yySe8//77fPDBByxYsIB3332XZ599lnffffe4znv//fdTUFAQ+tqyZUstVVw7jP3/MUwbpmbtExERERGJOlHdIvXHP/6R++67LzTWqXPnzmzatIknnniCUaNG0ahRIwB27txJRkZG6H47d+6kW7duRzyv0+nE6XTWae3H40DXPgNbsGufGqVERERERKJKVDd3lJaWYrFULtFqtRIIBADIysqiUaNGTJkyJbS/sLCQ2bNn06dPnxNaa20zDCPYIhVQihIRERERiTZR3SI1fPhwHn/8cZo1a0bHjh1ZuHAhzz//PNdddx0QDBt33nknjz32GK1btyYrK4uHHnqIzMxMLrroosgWfxwOtEhh2kBjpEREREREok5UB6mXXnqJhx56iJtvvpldu3aRmZnJDTfcwMMPPxw65t5776WkpITRo0eTn5/P6aefzqRJk3C5XBGsvDYYmAEbfgUpEREREZGoE9VBKj4+nnHjxjFu3LgjHmMYBmPHjmXs2LEnrrA6Zuz/Mk2rWqRERERERKJQVI+Rqq8MIxgQzYCVgMZIiYiIiIhEHQWpKHRgjJQZsGr6cxERERGRKKR36VHKwCAQsLF/gkIseqlERERERKKG3p1HK8Mg4LcQ2N8iFWOPjXBBIiIiIiJygIJUFDow2YTPb8Xc3yIVa4uPZEkiIiIiInIIBakoFAxSFvx+a6hFymmJi2xRIiIiIiISoiAVhQxMDAP8fktojJTdUNc+EREREZFooSAVtQx8PiuBQPAlspoxEa5HREREREQOiOoFeestIzhrn9dnCS3Ia/gVpEREREREooVapKLQgSV4vV4L/v1d+/w+V8TqERERERGRyhSkopABGIYFr88S6trn8ypIiYiIiIhECwWpKHRg+nOv1yAQCHbz83gckS5LRERERET2U5CKWgYer4WAaeCyufH79FKJiIiIiEQLvTuPUsb+IOX3G7itMfgDeqlERERERKKF3p1HIQOwGBY8PgMzYOCwuAgEjGPeT0RERERETgwFqSgUDFJWTNPAFwC7xYXfHwxScfa4yBYnIiIiIiIKUtHKalgBCAQMrDjw+4MvVbwtOZJliYiIiIgIWpA3KhmYWAw7AP4A2AxnaIyUy5oQydJERERERAS1SEUlA7BwsEXKwB7q2mc3FaRERERERCJNQSpKWYxgY6E/YGCYDnz7g5Thj41kWSIiIiIigoJUVDow2QQEu/YZAUeoRcrvVZASEREREYk0BakoZRzo2uc3MAP2UItURYU7kmWJiIiIiAgKUlEpOEYq2LUvYAaD1IEWqdISJwBuqwKViIiIiEikKEhFIcM42CIFEPA58O4PUoXFwSCV5GgQkdpERERERERBKjqZJoZ5MEh5vXZ8PgOX1UVZRXC726r1pEREREREIkVBKhod1iLl8QbHSMXY4vB690+DHkiKUHEiIiIiIqIgFYUMwDhkreSKCjumaeCyxuLxBV+ygDc+QtWJiIiIiIiCVBQyAMyDL015RTBUOS0HW6Q85XERqExEREREREBBKioZgHnIGKmysmCQshFLwDSwGlaKijVrn4iIiIhIpChIRaXKk00UlwdboSyBYHiyWezkF7qw6OUTEREREYkIvROPRiaYh3TtKysPhirTHwxSdouN/EIbdqsjIuWJiIiIiNR3ClJRyDAA8+BkE6YZbJHye4NBym2NpdxjwWFRkBIRERERiQQFqShkAGag6kvj8QQX43VbEwGwK0iJiIiIiESEglQUMoCAWfWlKS8PBieHkQAEx0pVJ96eUGe1iYiIiIiIglTUqq5FqqQ0GKSsgeAaUjajaotUvD2BOHtSndYmIiIiIlLfKUhFIcM0K01/fkBRcbAFKuCNAcBqVG2RSrQ3wG64ALVMiYiIiIjUFQWpKBWopkWqoDgYrjwVVYOUEVzGF5clBZsRnBq9obPZCahURERERKT+UZCKQgbVBymfP7itrDQ4e5+Vg0EqyZka3OZPwoqLJGcqNjOu7osVEREREamHFKSikQEB/5FfmsJi5/7DDgapRFsaAL6KeIyAkwRbQwzTVbd1ioiIiIjUUwpSUcgA/NW0SB2wNz84yYTFDAYpi2HBSbBFqrQkBsN04jBTwe+s81pFREREROojBakoZJjmUVukSsoOTEQRXLQ3wZ4EgWBoKihyE/A7ML2J+H0KUiIiIiIidUFBKkr5/cYxjzHMYJCKsyVhBoLf78lzEPA5KCuNx+9z4LbF1GmdIiIiIiL1kYJUFDIAn7/q9OdV7A9PTksCAX8wNBWX2vD77BQUxODxOEh3tajTWkVERERE6iMFqShVkxYp9o+RspkJ+H02Eu0pAHi9DnbtdVLhseP0NanLMkVERERE6iUFqShkAP6jjJE6IOAPtkiZvhj8Pjsx1mQACgpdFJXaKC+3U1zQoC5LrXMxtljaxPWOdBkiIiIiIpUoSEUhAxNfDVqkAvu79vk9MXh9NmxmIgBbdgTXmSoqsZG7LbHuCj0BmlpPpzy/BfH2hEiXcsLYLLZIlyAiIiIix6AgFaW8vmO/NP7946gqyl14vTbwxVW67+48B/sKbRjUoJtgFHJZXSxems2m3CRiCy84aR9HOCxYaFR+Gc1j20a6FBERERE5CgWpqFSzFin//q59JWVOPB4rPk9s1WMCBi6bu9YrPBGaO/pRWGJjd56DJatSSHc3jnRJdS7LuIgFyxvizzsFgIyYZhGuSERERESqoyAVhQzA5zt2kPL5gkGqsNhBhcdGaWn1gSnGVjVgRTubxcaqFW0qbUu0tIxQNSdGG9cQZi4ITg6yaEUDWsedypp5F9AkNjvClYmIiIjI4RSkopABeLzHfml8vmDXvrwCO+UVFoqKXNUe57JEf5A6fFxQS/ep7M63V9pWXtjoRJZ0QrWJ7cO0mQe78wVMg+lTe1NWYcGS3yuClYmIiIhIdRSkopTXd+xjPB4rLquLco+Fsgor+woc1R7niPIglexMpaX7tNBtA4Mt6ztWOW7TltTQ963iup+ULW3VaRHXnlkzexxx/8IVDWngSjuBFYmIiIjIsShIRSEDE3/g2C+N12sh3h6cla+0zEJBcfWzvdmIqdX6altD8zS8JQdbm7LiOrNpe9Vuirvy7DRwpWNgsG1DDzJcbaocc7JJc2eycv6Z+I4y3b0/YJDo6XsCqxIRERGRY1GQikZmzQ7zeC24rPFAsCvYkVgC0TvZhM2wsWJVc3buTA5tK9nZ7YjHp1izaRV3Chu3xmCUBSdisFvsRzw+msXZ48hbN5yi0mNPd75waRPi7PGh25bD/ulaDWut1yciIiIiR6YgFYWMGs7yXeGx4DDijnmcGcVBKiu2O/sK7Gze7sZtddPI3YSla5KOeLynqAnbNnQHYOu2YHe3ZubwkyZIOK1O2roHYzNsuPZdwtZdzhrdr9xjIdPSD4Amsdk09V8W2pfibEDcnlE4rTU7l4iIiIgcPwWpKGTUsEWq3GPFah57nFDAG71vsAt2tAeCLWrprhbElJ961OMXLEsjZ2uwq+KmbW6y4joya2FjWsb2ICuu6riqaJNecTGz5rSjse8SVqwLb7Hkpcta0iSmJesXDWHe0jQyYpqR4EiiKGcEazfH0cJxeh1VLSIiIiKHU5A6iZVXWDD8x25t8nqrn80v0hq6M1i2Nil02+7JZtGyzKPe5/CxRGsWD8A0DRbO7YWlrFVdlFlr2roHM29pGl6fhblL0sO+f0GxjaWzz6Og2IZpGhh5/QlsvZStO4Ov7/LlbXBYqp9w5HAt4zqFfX0REREROUhBKgoZNRwk5Q8Y+L3H7tpXUVGzN9cnWpKv8kx1cxa0oNwT3o9kXmFwfFFpuZXc3PDDyYnSIq49M2a1P+7zeH0Hn59FK1LZkHtwIpF9BXaynP1pGdf5qOdo4xrCrKmDtD6ViIiIyHFQkIpCNRwiBUBZ6bFn5Csri74gZTWsrFrdtNK2Q0PCL7Flh4tUZ8PjOkddSHamsHbxQPyBcF7ZX2b6zA4smXsGLY0RpFYzZXpb55DQelXlO9QVUEREROSXUpCKQqZZw0FSQEHhsYNUadmxZ4U70VrEdmZvQe3PtpdqbVfr5zweVsOKsesC8gpPzMyCAdOgtNzKrAVNSCg/o9K+tq5zmDrr4KK/K9Ylkh3X9YTUJSIiIvJroyAVhWo6ax/A7n3Hbm0qPML6UpFUvrdDnZy3NK/psQ86gVrahrJqQ0JErj1vcQZp7gwgOD5r6syqIXPb+lOxGFV/DbSNGUib2N51XqOIiIjIyUpBKgrVdNY+oEZrEBWXWaNqevB4ewJLV6fUyblX5SRjM4LPSSN3E5rFHly09/C1l9LcwYktmsS0JN6eQNPYbJrHtuVwiY7gGleNY7KqnONosuO6MmNuVtiPobb4AwauojNoGzOQqTOqD66btrlpFVN5sd+2rrOZOr0Lq5d1x2l10dZ1TiiQiYiIiEiQglQUMsJJUjVgmgax9mNPSnGiZNq7V5l9r7aUlllpEtMaAGv+GTg9bWgdF5xSPds6vNKx3u1DaBKbTVHuQDLt3cjbNBBraTBwHAhMdosDz5ZLSXWlsWvt2TSLq1nXwSRHCisX9cU8ykLJJ8L8ZWlMm3H0ySdWLeuM0xqc+a+N81ymzgxOirE7345129VMndkOy76z6rxWERERkZNJ9PX5Emo4aV9YYqzxFFJQ+yf+BXbk1u1scXZPNs1ifcxb2ID4mCQslmZ06F7B9KnN6dgnnd3lO2kddyrTF8bTtOwstuxwsXVXZ0rLrOQVOHG1spLtHMw+FpES6Mm0bW4yvRezbbeTxpkdwVhR5ZpuWwxlvlIgGMLs+4aTX3RixkUdTU2C3J58O/1tZ2JaDabNqjyF/MZtwTF4S1al0Kd/N9YXL6qLMkVEREROOmqRikJ10YbhtBx74d4TIc2dwdpNdds6lpvbiLKdfYBg18eCYhvLF/QDIMXohMPiYO2y4NTrW3YEW2JKy4JdH/MKbbRy92PuvNbElgxk1rxg17xtu4OLGi9dlUbruJ6kOBuErtcmti9NLQNJdzcGoLXrLJavDW+x3UibPrsV02YffR2uTatOw26JfDgUkehkNazE2I48AZLL6qJVXPcar3cnIhLt1CIVlWq/ScpuxGHBQoBArZ87HEmBup8lbtO2GKDyH/PCkuCP+p4dTWmRfibT8o4cCKbP6ErANKpdNLes3Mqs6X047ZTGNHB62MNcFs7vjsNuEgi0oG33/zFzRtVxVtEuUIOWq227nfTPHsTqisknoCKpK41jsthamlPtPrvFQUvHAPy2fawrnn+CK5OTVeOYFrgrurJyTWPKgU6dl+Mta4DdUUyO90eaxLTFX9CBpcsbstljoXFaL1q2nYUVF4ZpY23JTMxa+LtnYNA0thUuGrCudDYAdoudCn9FpeNshg2f6Tvu64mIKEhFobpokbIEYkiPacL20s11cPaay8lpFtHrr86Jx7n16C1ixwoV/oDBjHnNiYvx0ywzjZIyKyVlwX2LZg2qs/Ff0WDOgtZk91jInvJdWAwLATOywVxqrnFMFmXbB7BwYQL9+y7Ha92JpbwlOf5vCJgBWsWdyrrl3Zm6z4HVYtK7n41NpQvJcgzAU57IFmNirbz5tGAhK64Lm0qX4QvozezJwm1108TdmXzfJjwBDyYBMqw9yN3UkoVbK39wNXX6wQ/M7LbW5B62RuDWXS627hoYut2uZRscjX4g37OHZu5umARYWzynSg0pzgY0tHTDZ92D36xgY8lKADJjmhPr7cy6dU2Yt38m26aNOlNcasNrmLTvspCc0gU0j+mEJ78Ny9c0oF32PioSp7CnfGdtPUUiUg8ZZjiLFv1KFRYWkpiYSEFBAQkJkZmq+gBfbi5PfDKfbwpctXre0/usAksZa0qn1up5w9E4JouF04cf+0CJat3a74OUaRRuPgtf+vuU+8sjXZIcRYIjiVTPWcxZlFntmLmu7fZRWuao0uXWYpgkJ/hC6711a7+P3TGf4A14jnlNA6NKC4OBQau4U9iy7hQ2b3fToVUBqemb2LU1C2fm9+woyz2ORyl1pXlsW4yiLixZmU65JxiI3M4AHq9Rq4uM26wBbFZC1+jUOh8afk+pr4jG9u7s2ZHNqg3xlX6GO7fJJ7/QFeqifazzH/4hl8Me4NRTctjkn0KFfo9FNZfVRWN3W/Z6tpDv2YfT6qzU0pjqbEiqrS2GP4Zt/lkUeQtrvYZ0dyZJRmt8lgI2liwFwG/6a/069dnozjcwLPuCSJcB1DwbqEUqGtVBtg143fh8icds7oqzx1HsLa716wPEeDvWyXnlxFq0MgWnYxgVHgt9G53DOr6MdElSDathpZVrAAsWtmdd2ZGXP1i8qvqlCAKmUWnR7EUrU+jQ6kpSG25jy+bmxDX/ll1l2yrdJ82dgatwAAVFMSQnFVMS8xO7y3fSMq4Luzb2YvrCg2M1V6xLhHVdAIjfdQHte/zIxuKVJDlSSPWdQcBvI9f6Bd6A93ieBvkFEh3JNLL0ZMP6lszd5ayyv6yi9lvdfX4LvkPeky5bm4Q95xIANviqv97SNUlhnf9wHq+FabOzaZjclLYdF7G2ZHboAwCHxYGnBh8aSO2xWWyVWqkzY5oTH2jD3t2ZrMmJZ7Pfgt0WoEl6OZu2uenUJp/YuCK25KaxcufBMO10tKFty3xyt8fTvHEB1pS5eM0KEszWFOZlkJC8nc2eGZT5yypdP8YWQwNnY/K9uyj1lRBriyfd3hZPcVNyNqWyNO/g2D677XSS4/20bb+WLd6ZeAMeGrtb4fC1oLwkEVvSCnKKl9ZKl1WJbmqRIvpapJ78eB5fF7pr9byd2uTj9VrIS5pQ7X6n1UWFv5w2sb1ZUzKrVq99gLHlBnbuqfpHWU5eVotJlz5fsbV0Y6RLEYKtvoZhYMPJzvWns2l77f4eOVRSvJeWXSaTW7qBREcSDbyDmLOocaVWiuQEL40alLGyBotS26wBunfczZKVDanwBt/0tm9ZiKfB53Xy6XK0sBgWUl3p7C7bXu3+JEcKacapVNhy2FSyOrS9WWxrjKLuVJQ78aVMxm/6yKvYV+m+cfY4GttOpby4Ia7YfRQYK6u0/BkYNIttyz7PNtKcWZTv7ciSVam12tp0smjboogGabvJyWlMSZmVLl2Xs65s2knd6tA4JguvWc6uI/x8RZLd4qBpTFssZS3ZvCWdvfkOUhI9pCaXsXVHXKUPcmpbXIyfzu22U1AYj9PppbTUybpNcaGf+xi3PzQJ1bG4nQECJlR4Kof1ZhllNG+5io0Vc6qM06svnFYnVsNK6f5ZjY/lZGyRUpCifgSpWLefpHgv3sZvVNp+YAKK7Ljg1NaNPVex1fFBrV4bgoveLpg+rNbPK5HXqlkJxQ3eOWnHS9ktdtJcjU/qMBhjiyUjcA6zFzTBZjPxeE/MOL0Yl59uHbeycFmTOmmlAGicVk5Kq6/ZeVjr18kuGHJ6s2ZVK3bn2el76nrWeyeT7m6Cs7QXu/ckkpJYwsIVDfEHDCyGSd9eG8BRwLZNbdmwpfJMrA57gNN6rWRt2Y80ic3GUtSNxSvSQ6E0dEzPVayt+C+xtjgybaeyYX1Ltu50YTHMGk06U980yygjs9VsNhQviXQpVaS7M0myZLPVs4hSbwnxjgRKvMU0jW2LtbQ16zZksCvPjtVi0rPrduyxO/CWNCIQuzQUyjNjmhNnNCW3YkHoza7d4qBZTHsobYnNWURFaTIOVzFb/VMp8R3ssZLhbkqC2Q7TWsaG0lk1GkPZwJVGqqUdRfuasWp9Uqgr569ZQqyPzh03sdcyG7vFSZKRjceyh5ziqkupnCxsho2MmBbEmE0oZENo/H2cPY50Zyss5c3YvbsBG7bEYLVC1/a7CMQvwmdW4LYksal0ebVjZBWkTlLRFqSe+nguXxUeeQrZXyrW7Se23SuVtjWJaUlu6QZaGiPYbHzJvqU3ktTx1Vqf3a+1fRjT57Ss1XNK9OjfbymrS3+MdBlha+hKpyx3GDv3uGjZ/d/EWhtgxcna4rmhY+wWO1m2c9hjzGZfxZ4IVlu9NnG9Wb64O3mFv96p6eNjfHTo8RM5xcsjXUrY4uxxVPgrQl0U092ZxJb3ZtGyzCpvIlMTvcf9KXxygo+8wqP32m+cVsGufXa8R+gyJ1V1aZuHkfo/tpdtiVgNBgaNY7OI8bZn06bGobFhsW4/DnuAgiIbbleAkhq0pLRrWUheoTPUSyQh1kfnDlsoLUlg5brkagNOrNtP984bCfjtrN+UXqmHSXqDCtq0XcmG8pl4A14sWGgcm4Xb14aiglTiYovYsjWNzXXYSn6yaZFZStOWq9lYHl0tVmnuTJIt2ZQXZuLz2YhJXYuJD59ZhtPXjLy96azblFDpZ6R182I8Xgubt7trtH5lUryXDu22UmxfUKmVXEHqJFVfghRA4x6v4Q14ibcnUOQtpI3rHLb4pmHmjiSl5fcsmn4BbXu/T17F3iOew26x13jcQro7k51l27Dl3hBai0l+fdzOAC26f8zeit2RLqXGWsf1YNG83qE3HckJPvKLrNhtJp1Pm8SWknWkuzMp2TyUDbmxZDUuxZ/xT8r95SQ4kij05Ee0/jR3Bubuc066Nct+KavFpG+fJawu/YlERzLpllPZ5p9eZ2M6j1dDdwYJFb1ZuLQxmWnlZGQtoWh3G5auTq7RGw2JPlaLSa9u2/DHLMfpy6LUuprc0g21fp00dybJZie25TbFNMHl9JOQWMiGjWns2hfda3A1TPbQomke6zam/Ko/3KlNwRarzZTYl+IzvfgDXuJsKWwuWVOnY6ySHCkUevNp6MogycimrDCTnM3Jddqlsjpts4po2HgNm8rnM6rDqJMuSGmyiShk1GG2jbMnkFexl3Rna2AtZUWpNExqwvJ9TpplZwIQb0shr2JvpfVmDkx13cCVjssSe8w/Hk1js9lbsZ24itOwxixgkULUr1pZhQV73rkQ849Il3JMNsNGluV8pk9tXmn7gU/xPV6DjUvPpkPHZsyb3yHUZS1nawxd4i/FMGD7xniS235cZUxKdayG9ZhjLKqb5e5QDVxp7CnfFazfYqOlfTBz5rY6YV34ooE/YDB1elc6tW7O2k0JrPVYaJzWnEatvo2qGf+axbbBv68nixalhgLThtwYNuT2jnBlcrz8AYNZCxoDwcXXDaMVPbvspNj9I3srdpPkSCHd1gXT8LG+dCYGRo2XDMhwNyU+0JEtm5uybFt1rTaR/ZC3pnbnOdidV3UNRjmywhLb/h47lXvtNE4bQMtW69jinY3f9BNri6PEV/SLW69SnA1oYGuNp7gJm3JTWLPHicMeYEeE/46szolndU4PYlzdoEVWRGv5JRSkolBdflYZY03AZ/di9TYgxVFM4S4XDRPTKauw4ClqAoDTSA4e6+0IBINUZkwW+Z5dpFhbgmECVYOUy+oi1ZlBAD9ub1sau1LZvjGZxo071OEjkmixZHUyp5/ehzUlM4Fg+AYiOnbKYlhoZR9KmX01AdNPia8AY9eFTM+JP+r99hbYmTqjU5XtSw6Z4S4udwTuRu9T5i8j0ZFEkr0Rm0pWVTq+ddxprF12Ck06Tia3pPoPH5rHtmXHugE0a7YNi72Y1aX/C+1LcCSSVDqUhbMb0qJxKenpe8jdmsa0etw9ZtnapND3W3e5yCu6kG49Z7CueGFE6klypFDoyadlXHf2bunCvIVH/9mSXw/TNJi7uBEO++W0aFzK2k2xrNkfnmNcXYl1+2nbcRnrSmdU+j2Y6kqjgdEF01IC3mQ2bWrC4hpM4S71S3C9tU44HR3w+4NB3gBaNS8hObkQR8weNnvmUHbYRA6xtjgauppS5NtLii0LX3ETtuQ2YFU1H2hH04dxpeVWSkvqpjdWXVKQqmdclnisDju+shhctnQ2FTpJSU0FYPX6BgAYvkQSHcns2dWQzMzmbCvdRGygGYbDwFvSGLuj8j/aVFcaAdNHir0Z1kA8RsBFYWEKDns8W7bH4PM1OeGPUyJjycLupHVaSamvmPTy3xCXsIvV5T9EpJY4ezwxeRczbW0ShtGS+Bg/NpvJvlrqtrAhN4YusZfSOG0ZKxf3YXO5hU6n+thSso44exwppcOYPjUNAM/iISQleGmYNS0UthwWB82Mocyc3gzTNNiyI/hpZP/TA6wu+Zk2sb1ZuugU1hUHf02v3xLL+sMmGBAoLbMyc9rpnH5aGmsq/nNCphs2MMiO68beLd1YuymOhqkeZmhG0nrL47WwZmPlddhKy62UllvZPe0UmmW0p2mrxVj88ezY2pyVm46+KLzIoQ6dDdCE4M/axjggE7ezE906bsOI3YRRkcGO7enk5MaQo+7DJ4yCVFSquzcCVjMOK/GUlrkxjGT25ttJzU8CoGD/GzZfRTwNYpuzaq+LTo3aEmPbTVlhOu7kMjZuSyG9oR3skBHTjJ2luaRa2+A18qE0Cz+Ql5fInn1uPF4Dr8+iwaX1SGGJjeyi84nxW1i4OhmHPYV2vVad8K5XGTHN2LVmKBv2fwJnmgaFJbX/627J6mRY3T90e92ic+jcLZ3Vyzuz4ZA1R/KL7OQX2dm97yzanFKBxbCyc+0gZuys+in0tOldadMim2lq2agx0zSYOqst2U2b4HD4CTT8kiR7BkCtTlBhM2y0jOnF5vWdmHHI7zUt6yBHs3m7m83b1bVTal9ZhYWZC5oA+sA6UhSkolBdfo5g+GMJ+B0Ul7rw+qz4AwbrDvt0rKQ4lkRHI/IK7eTtaURyWjqb1iXRypFJ7k4XNmsA0iEh0Bavq4KygibExDvYsKUhNptJ7g5XvVyDRIIWrkgNfe/xWijLPQdrg3dP2FosreJ6sHBOnzqbjvtoCoptTJvW44j7S8qsrF14HmXlliP+GzFNg9XH6Hoo1TvQYufcfBUVXgsWw6Rf70zWlP9QqaXqwGQ7NWW3OGjp6sOale2ZFuWD/UVE5MQJ+53G7bffzvjx46tsf/nll7nzzjtro6ZKtm7dytVXX01qaiput5vOnTszb9680H7TNHn44YfJyMjA7XYzePBg1q5dW+t1nEhGHfZM8XtjKCtJoKDQwc49wTcdh0+Bu6/ATVFB8M3wyvWJxJlZ7M5zsGxVIwC27XZhwULBvkYk2jJYvzGZvTubsXWXi03b3ApRUsnazXG0cgyudp/bWrutlW2dQ5g5rW9EQlRNFZda9W+kjh1YOylgGkyd2Z7GFVfitsXQPLYdDYpGsXnB72gbM4DWcT1DY/kOZTEstIntTSvrhbR1D6Zs3fVMnd6VnQpRIiJyiLBbpD777DO+/PLLKtv79u3Lk08+ybhx42qjLgDy8vLo168fZ555Jt999x0NGzZk7dq1JCcnh455+umnGT9+PO+++y5ZWVk89NBDDBkyhBUrVuBynZyDN4067NrnqXCxLz+WPfl2Ake4zO59DioqglNC+wMG23ODs6gUlR6Y1cxCA3cj1q1MoIOjbajbksiRzJzbms6917C1NIfGMVk4y7pjjc1lzbJTSGzz0XFPJW63OMjwXMzUhZotSqpasKIBcTHXsqn04Po6U6d3BaBDdie8Db6gwJOH1bCSHdObnDWdmLZL3fVEROTowg5Se/fuJTGx6rolCQkJ7NlTu4tVPvXUUzRt2pR33nkntC0r6+DUiKZpMm7cOB588EEuvPBCAN577z3S09OZOHEiV1xxRa3W82tQWuZi++6jd73zeC3syT/4Ke26zVUHuCcHulJabmXRioZ1Uqf8uvj8FvZtGExWqxksmzdw/9pNwT7dmQXDKHT/M+xz2gwbAQLE2ROw7BzBvI0awC1HVlxa/SKlK9YnkLz7Cjp12MC69S2YpqUaRESkhsLu/9KqVSsmTZpUZft3331Hy5Ytq7nHL/fll1/Ss2dPLrvsMtLS0ujevTtvvvlmaH9OTg47duxg8OCD3YYSExM57bTTmDlz5hHPW1FRQWFhYaWvaFKX60ht3hZXaQaYX2rV6uAaPD5/9Hahkuiyabub2VPPCi2Ae8DiVSm0jRkYum01rGRbLqaR+8iDZ2NssSQXjaS5OYKi9ZdXmTFLJBx5hXamzmrLdoUoEREJQ9gtUnfffTe33noru3fvZtCgQQBMmTKF5557rla79QFs2LCB1157jbvvvps///nPzJ07l9tvvx2Hw8GoUaPYsWMHAOnplbvzpKenh/ZV54knnmDMmDG1WmtdsFsNvP7aDVVH+lQ2XLvzNFZAas/sOZ3ocGoOe8q306DsMmauSKV5ZgNaZc9jn3cL+yoOtnanOhtSuukilm93A1Vbx0VEREROhLCD1HXXXUdFRQWPP/44jz76KAAtWrTgtdde43e/+12tFhcIBOjZsyd//etfAejevTvLli3j9ddfZ9SoUb/4vPfffz9333136HZhYSFNmzY97nprm8Nqwes/MTOdiUSSx2th7/ohxMd6WLQ+AYBN29xs2tafti2KsKX8A5/po3FMC7YsH8qefI3JExERkcj6RdOf33TTTdx0003s3r0bt9tNXFzddKvJyMigQ4cOlba1b9+ezz77DIBGjYKzyO3cuZOMjIzQMTt37qRbt25HPK/T6cTpjN4uHAcmm3BYDUoiXIvIibJlhwuoOkHM6o3x9G14Hn7XepbOG0hpWe20qoqIiIgcj+Ma4NKwYcM6C1EA/fr1Y/Xq1ZW2rVmzhubNg+NzsrKyaNSoEVOmTAntLywsZPbs2fTp06fO6qprhwYpEYEZc1swf8aZClEiIiISNWrUInXKKacwZcoUkpOT6d69O4Zx5Df4CxYsqLXi7rrrLvr27ctf//pXfvOb3zBnzhzeeOMN3njjDQAMw+DOO+/kscceo3Xr1qHpzzMzM7noootqrY4T7cCz69A8DiIhmthEREREokmNgtSFF14Y6gp3IgNKr169+Pe//83999/P2LFjycrKYty4cYwcOTJ0zL333ktJSQmjR48mPz+f008/nUmTJp20a0gBHFhGyqEGKRERERGRqFSjIJWcnIzFEvw0+Nprr6VJkyah23Vt2LBhDBs27Ij7DcNg7NixjB079oTUcyI5jLqbBl1ERERERH65GqWhu+++O7TWUlZWVq0vvCuVHRgj5VSQEhERERGJSjVqkcrMzOSzzz7jvPPOwzRNcnNzKS8vr/bYZs2a1WqB9dGB/GQnENlCRERERESkWjUKUg8++CC33XYbt956K4Zh0KtXryrHmKaJYRj4te5RLdjfIkWAg1NPiIiIiIhItKhRkBo9ejRXXnklmzZtokuXLvzwww+kpqbWdW311oGufXbTzy9c6ktEREREROpQjd6ljx8/ntGjR9OpUyfeeecd+vTpg9vtruva6q8Ds/YpSImIiIiIRKWwJ5u47rrrKCoqqtOi6rsDnfnsAXWTFBERERGJRppsIioFm6QcAV+E6xARERERkeposokoZJgKUiIiIiIi0UyTTUShUNc+vzeidYiIiIiISPVqPJNBfHx8aLKJfv364XQ667IuARwKUiIiIiIiUSnsKeFGjRpVF3XIofZ37bN7PTWcDkRERERERE6ksIOU3+/nhRde4JNPPmHz5s14PJ5K+/ft21drxdVXxv6+fXafBxyRrUVERERERKoKu71jzJgxPP/881x++eUUFBRw9913M2LECCwWC4888kgdlFj/HJhswur3YT2QqkREREREJGqEHaTef/993nzzTf7whz9gs9m48sor+fvf/87DDz/MrFmz6qLGessa8GNV1z4RERERkagT9tv0HTt20LlzZwDi4uIoKCgAYNiwYXzzzTe1W109ValFyqIWKRERERGRaBN2kGrSpAnbt28HIDs7m//85z8AzJ07VzP51ZpgkLIoSImIiIiIRKWwg9TFF1/MlClTALjtttt46KGHaN26Nb/73e+47rrrar3A+uhAdNIYKRERERGR6BT2rH1PPvlk6PvLL7+c5s2bM2PGDFq3bs3w4cNrtbh6K9S1z49VOUpEREREJOqEFaS8Xi833HADDz30EFlZWQD07t2b3r1710lx9dWB7GQJ+LCpa5+IiIiISNQJq2uf3W7ns88+q6taZL9Q1z6fTy1SIiIiIiJRKOwxUhdddBETJ06sg1IkZH/XPouClIiIiIhIVAp7jFTr1q0ZO3Ys06dPp0ePHsTGxlbaf/vtt9dacfXXgTFSXgUpEREREZEoVOMg1bJlS+bOnctbb71FUlIS8+fPZ/78+ZWOMQxDQaoWhMZI+dUiJSIiIiISjWocpDZu3Ijf7ycnJ6cu6xEOXZBXLVIiIiIiItEo7DFScuJYvT6s+7v5iYiIiIhI9AhrjNTkyZNJTEw86jEXXHDBcRUkB1ukLD4Pmv1cRERERCT6hBWkRo0addT9hmHg9/uPqyAB48BkEz4vNrVIiYiIiIhEnbC69u3YsYNAIHDEL4Wo2mGGWqS8WCNci4iIiIiIVFXjIGUY6mN2ooRm7fNpjJSIiIiISDSqcZA60Eoide9A1z6L16MgJSIiIiIShWocpEaNGoXb7a7LWmQ/Y392svq8ClIiIiIiIlGoxpNNvPPOO3VZh1Syf7KJUIuUulWKiIiIiEQTrSMVjfY3Qll8XiwEIluLiIiIiIhUoSAVhUKTTXg9WDU2TUREREQk6tQoSC1ZsoRAQC0jJ8rBySY0RkpEREREJBrVKEh1796dPXv2ANCyZUv27t1bp0XVe+aBMVIVWE0FWBERERGRaFOjIJWUlEROTg4AGzduVOtUHTvQImX1+bApSImIiIiIRJ0azdp3ySWXMGDAADIyMjAMg549e2K1Wqs9dsOGDbVaYH1kCZhYbcGRUmqREhERERGJPjUKUm+88QYjRoxg3bp13H777fz+978nPj6+rmurv8wAxv4ZJ2wBf2RrERERERGRKmq8jtS5554LwPz587njjjsUpOqQ1QxgsQSTlM1UkBIRERERiTY1DlIHHLowb25uLgBNmjSpvYoEI+Bnf47C6leQEhERERGJNmGvIxUIBBg7diyJiYk0b96c5s2bk5SUxKOPPqpJKGqJETCx7O/bZwv4IlyNiIiIiIgcLuwWqQceeIC33nqLJ598kn79+gEwbdo0HnnkEcrLy3n88cdrvcj6xmIGQi1SClIiIiIiItEn7CD17rvv8ve//50LLrggtK1Lly40btyYm2++WUGqFhhmgP05CptfQUpEREREJNqE3bVv3759tGvXrsr2du3asW/fvlopqr6zBPxYD0w2oSAlIiIiIhJ1wg5SXbt25eWXX66y/eWXX6Zr1661UlR9ZwQOtkhZFaRERERERKJO2F37nn76ac4//3x++OEH+vTpA8DMmTPZsmUL3377ba0XWB8dOkbK7vOCI7L1iIiIiIhIZWG3SA0YMIA1a9Zw8cUXk5+fT35+PiNGjGD16tX079+/LmqsdywBf2jWPqvfG+FqRERERETkcGG3SAFkZmZqUok6ZAkcbJGy+hSkRERERESiTdgtUlL3DH8g9MLYfJ6I1iIiIiIiIlUpSEUhi+k/uI6UWqRERERERKKOglQUOnTWPptXQUpEREREJNooSEUhSyCA9cAYKW9FZIsREREREZEqftFkEwfs2bOH2bNn4/f76dWrFxkZGbVVV71mBPwYoSClMVIiIiIiItHmFwepzz77jOuvv542bdrg9XpZvXo1r7zyCtdee21t1lcvGQF/qKnQ7lGLlIiIiIhItKlx177i4uJKt8eMGcOcOXOYM2cOCxcu5NNPP+WBBx6o9QLrI0vg4Kx96tonIiIiIhJ9ahykevTowRdffBG6bbPZ2LVrV+j2zp07cTgctVtdPRVckDf4vc2jrn0iIiIiItGmxl37Jk+ezC233MKECRN45ZVXePHFF7n88svx+/34fD4sFgsTJkyow1Lrj+A6UiagFikRERERkWhU4yDVokULvvnmGz788EMGDBjA7bffzrp161i3bh1+v5927drhcrnqstZ6w2L6QtOfWz3lEa1FRERERESqCnv68yuvvJK5c+eyePFiBg4cSCAQoFu3bgpRtcjiDxzs2leuICUiIiIiEm3CmrXv22+/ZeXKlXTt2pW///3v/PTTT4wcOZKhQ4cyduxY3G53XdVZrxh+v7r2iYiIiIhEsRq3SP3hD3/g2muvZe7cudxwww08+uijDBgwgAULFuByuejevTvfffddXdZabxjmwenPbeVlAFgPNFGJiIiIiEjE1ThITZgwgW+//ZaPPvqIuXPn8o9//AMAh8PBo48+yueff85f//rXOiu0PrH4/QfHSFUEu/bFOayRK0hERERERCqpcZCKjY0lJycHgC1btlQZE9WhQwemTp1au9XVU0YggMXY37XP58ViQKxdLVIiIiIiItGixkHqiSee4He/+x2ZmZkMGDCARx99tC7rqtcsfj8W8+Btm8UgTg1SIiIiIiJRo8aTTYwcOZJzzz2XDRs20Lp1a5KSkuqwrPrNCPhCLVIQDFKxRiCCFYmIiIiIyKHCmrUvNTWV1NTUuqpF9rMEApVbpKwGsfgAde8TEREREYkGYa8jJXXP8PlC058D2C0GMaY3ghWJiIiIiMihFKSikCXgxzgsSMX6tJ6UiIiIiEi0UJCKQobfV+mFsVsg1lsesXpERERERKQyBakoZAkEKnXtcxgQ4ymNYEUiIiIiInIoBakoVGWMlGESU14SwYpERERERORQClJRyPD7MMxDW6RM3KVFEaxIREREREQOpSAVhSwBf6UXxmGYxBQXRKweERERERGpTEEqChl+P4Z5cAFeuxnAVVyoVaRERERERKKEglQUsvj9lSebIIDNU47DppdLRERERCQa6J15NAocNv25GcDmrcBpU5uUiIiIiEg0OKmC1JNPPolhGNx5552hbeXl5dxyyy2kpqYSFxfHJZdcws6dOyNXZC2w+P1YDplswm76sFaU47SeVC+XiIiIiMiv1knzznzu3Ln87W9/o0uXLpW233XXXXz11Vd8+umn/PTTT2zbto0RI0ZEqMraYfH5sHBwjJQj4MfmqcB50rxaIiIiIiK/bifFW/Pi4mJGjhzJm2++SXJycmh7QUEBb731Fs8//zyDBg2iR48evPPOO8yYMYNZs2ZFsOLjYzls+nNbwIdVQUpEREREJGqcFG/Nb7nlFs4//3wGDx5cafv8+fPxer2Vtrdr145mzZoxc+bMI56voqKCwsLCSl/RxDBNrIeuI+X3Yasow2kxj3IvERERERE5UWyRLuBYPvroIxYsWMDcuXOr7NuxYwcOh4OkpKRK29PT09mxY8cRz/nEE08wZsyY2i61VllNf+h7e8CHraIcl6EgJSIiIiISDaK6RWrLli3ccccdvP/++7hcrlo77/33309BQUHoa8uWLbV27tpiCxyyjpTPi7WiHNch4UpERERERCInqoPU/Pnz2bVrF6eccgo2mw2bzcZPP/3E+PHjsdlspKen4/F4yM/Pr3S/nTt30qhRoyOe1+l0kpCQUOkr2lgDvtD3Dp8n2LXP9B3lHiIiIiIicqJEdde+s846i6VLl1badu2119KuXTv+9Kc/0bRpU+x2O1OmTOGSSy4BYPXq1WzevJk+ffpEouRaYzUPtki5KkoxTBO36SPKXzIRERERkXohqt+Vx8fH06lTp0rbYmNjSU1NDW2//vrrufvuu0lJSSEhIYHbbruNPn360Lt370iUXGusgYPd+FxlxcH/+71A7XVxFBERERGRXyaqg1RNvPDCC1gsFi655BIqKioYMmQIr776aqTLOm5W/8FufO6yIgCcPk+kyhERERERkUOcdEHqf//7X6XbLpeLV155hVdeeSUyBdUR2yEtUu7iYJBy+SoiVY6IiIiIiBwiqiebqM8ObZFyFucH/+8tj1A1IiIiIiJyKAWpKFWpa19hPgAuj4KUiIiIiEg0UJCKUrZDpj93FuwL/r+8NFLliIiIiIjIIRSkotShLVKu/D3B/ytIiYiIiIhEBQWpKGX1eQ/5PhiqHPunQRcRERERkchSkIpSh7ZIHeBUkBIRERERiQoKUlHK5qsmSJUURaASERERERE5nIJUlLL4qy6+6ywujEAlIiIiIiJyOAWpKGX1VtMiVZR/4gsREREREZEqFKSilNVXTYtUQV4EKhERERERkcMpSEWpQ2ftO8BeXordakSgGhEREREROZSCVJSqrkUKIMZuBcCiPCUiIiIiEjEKUlHK6qnaIgUQYw8mqCS37USWIyIiIiIih1CQilJHbJGygNVikGRXk5SIiIiISKQoSEUpq6ei2u0xFhOn1SDOEqi0XbFKREREROTEUZCKUtVNNgEQgx+H1UK84a+0vWGsHdDYKRERERGRE0FBKkpZvUdokcKHw2oQd9iCvZmOYAtVo7hgoHLY9NKKiIiIiNQVvduOUlZP9WOk3H4vDivE+corbW8cKAGguS3YkpUVZ63bAkVERERE6jEFqSh1pBYpt9+Dw4BYT2ml7RlFuwFoWhFctDfLLK7bAkVERERE6jEFqSh1xBYpbzkOwySuvHJQSt+1GYAmu7cAkJWXW7cFioiIiIjUYwpSUcpypFn7vGU4DJPY0sJK29O2rAWg8cYVwf/nrsNu1cwTIiIiIiJ1QUEqSlk95dVud5WX4jRMYovzQ9ucNgtJm9cB0CBnFS67hdiCvaTHaNFeEREREZG6oCAVpax+f7XbY0uLsJsB4vL3hLa57Rbidu/AYbPgzN9LmtuKq2AvGfbqzyEiIiIiIsdHQeok4y4pwGH6id278+C2/V34MmKs2MvLSLP6idm3m0a+k3/CiXhncPbBJLda10REREQkeihInWRiivJw4Cdu9/bQNvf+mc6bWYMTVKQHSnHv20V68d5IlFirfle6igSXlRHezbRKske6HBERERERQEHqpOPO24sj4Cdm7y6slmBLlNtiAtCkLBicmuRvx+rzkbYnembu+6XTXnSaNZmLAttou3IOl22YWqs1iYiISP0R57Risxx8R+KyW0hwHX3dzWYJ9hpP3pXosnF+Yjmdk204bHqLXR+ov9RJJiZvN46AD4BEp5V9ZT7cBABosjs4BXrTTasASNu8DlL6RKbQw5yV5OOH/PB+3FJibKStXMTZu7djLy4ku6Kc7Lv7s77AW0dVioiIyMnKahj4zeCHy2mxdgKY7Cnxkey2cUlFDoPeexEj4Cen92Bym7Wl17cfEr99M9u79mZNl36sSsvGbxjE+j0UWZ2cN3Mi2dO+w+uOZdOpZ7K6XS9WJTZmn2kjgwpWeF3sLPbSINbGiNL1DHz3JZxF+QB4XW429xzIuvY9WZ3UlOUeB3tKfBiAGbmnSGqZgtRJxp23G4d/f5ByGOwrAzfBSSUyc1YC0HTpLAAarl0C3SJSZhXn//wxS/tew87imoegDs7gsTF7Do4Hu3L9jzzW4PRar09EROqHU1MMLMCCQgOPLxDpcqQWtEi0c+XG6XT99n229BzAvrQmdHvvXWyeCna360Ji7kYcxQeXjWk75d+0PeT+mYtmkrloJgOPcH57WQmtfvqaVj99zfmH7ctr0Zq4HbnYy8sq36e8jOxp35E97TuG7N+2L7sDAZsNV8E+NvQaxJoWHVntTmNVEZR6NUHYyUhB6iRjLy8jriI4iUSCJfgHwGUGg1WjFfMBiNu5Nbi9II/UGBt7S30RqPQgm8UgY8kszu55Nv8krcb3a1dQtWti94kTaHfPQFblR/YxiYhIdOmQbCMVL/OLbZR6/dgsBr7Awc/+U2NsjN46nZ5/fxMAT2w8qwZdzILWPZntT2RXiXo7QLArfqdkGxtLTYoqjvzmvk2SnTMK1rM2qQlLyh3klR38u5waY+PKghVkrVnA6k59WJbcnCYV+ey1x7HE62Z3DZ/rU1MMMrzFLLcmsaHAy4GXs3G8nQJPgBSnhStzZ9Lz7TexBIK1tpz6LS0POUfDVUvCfQrCkrxxbY2PTVm/IvR9l6/eo8v+7/02G9u79WVdx9NY27AFK804Nhd4K7VcHf7zLNFBQeoklFgYHAuVgA8wcAWCv5AO/bTlgMZOk72ltXdtiwHh/jtuFm/DXl7GgEnv8eFZfww1ux9L69Xzq90+ctm3PNTknPCK+JXrnWKwpNhCqcf/i14jkROla7KVDuW7+NZMJ8lp4Dcht/Dgm6p4pxW3zUIfSz5TPAkUH+WNnJz8Du/m5LBZwm4lctstjPKs56znnsQS8ON1x7Kh7zlkzfqBrd36sKjbmZTaXVz8jyeI2bvr4LVKiva/mX2Pa4DcHqezuOfZLEhoxtKCAP568ou0fbKNrWUmheV+Tkmx8tuf/kGz2f8lYLGS2/MMVnY9nWVJzVlSaqW4wk9GvJ2rt83mtLdewzBNhu4/z/auvVnV/Qw8didnvjUOR0kRAC1m/hBqkTlgd7surOlxJssbtWVpIJZtRZWDVbdkK1fO+YxWP30d2lbaIJ11p52N326n21sTMI3gGKQDAepkZvX5aDLvZ5rM+znUKlbaIJ2NPQewLqsLWZtX0mHyx2zv2pucDqeyNq0lq4hnY6Ev9J6qUZydi4rW0HrlHDa27k6Z001S4V7WZbRirS2JtUWBWm2BbZVkZ+ju5ZS6YtiQkEGHPRtos2QGm9t0Z32jlmy0JdK1eCtZm1eysWlb1sdnkOEp4JTFP7G9aRs2NGzOOmsC62u5rhPNMM0avqv9FSssLCQxMZGCggISEhIiWosvN5edpx19XNOKIb+hw+RPePvul/mm0MVvYvO5/MV7qj32zT+8wqQCZ63V1y8Fpu87+jFJbhv5ZT46JttYnufjrCQfNz97IwDP/vHvzMw79nVsFoN//PN2HKUl1e5//N53WLDv5P/lWRvsVoPXJz1BXpOWPNr9Kv608gve7n4R6/IP/mEanOTjvwU2Ambwuf2NYzeznRkabyaVxDutXOnZwOSEVmwq8GK1GLX6ZrJnioU/vHIHjpIiPLHx+B1OXPl7WTrst3zTfiDbvVYe/vY5UtYuxxLwU56YzIwRo/k6pQOb9LN60otzWvld6WryY5OZ62rE2ftWcdo377Fk0AjmNulCs9I9DHtrLFt6DGD+KYOY40ing1nI8G/eZHXPM5mb2SnU2hTnDL6pPyXFwg1fjKPB6tptdShNTWPFwAtZ1KIbc32x7Ck5OXtB9E2GlqW7WRLTCLthMmLB1+xNb8rSjHbstbi4dP4XtJ0ykYDFyp62nUlbueiI5zrQapK+bG6VbmzHK69Fa9aeehaxRXkk7tpGk/maXKomKuKT2NzzDIoTUujy7QdYvZ4jHutzONl6yunktO3B2gbNWUMcu8sDNIsx2FoOA2wFnDPlfcrjEslp0511DZqzxowjt9hH83gbW0r8VHgD9EmG8xd+S7sfPq+Vx+C3O9jetTcb255Cm6Fn0P3MXrVy3uNV02ygIMXJF6RyT+lHkwXT+ezWp/igPJWrHbu4+NU/V3vsNzeO5W1fZq3V99iW//Bg06O3Bv2fZQvv0IwbvGv5T2p7zijexPmvPwzAkvOvZkzGwGNep12yjcefueaI+zf2Gcw9Ha/QgE3gvMRyrn/uVgAKmrYkccsGShuk89S1T7Esz0ebJDtPPDuKuZeO5sPm/fjDj2+QtmIBPncsL/z+aebvO3k/CZLaE2O3MmbJx7ScPgnTMFh04TU4y0uIy9vDl2dczs/5thq3Jlend4rBnS/djr2s+g9HIPgH9UhvBFadfSmTug1hRr7luOqQyOiRYuGmT58iOWf1cZ3H645lW9feNJv9X3Z0OY2MxbNqqcKjy+3RnyU9z2JBQnOWFQbw+qP7Z7Blop3rln5N+/98GulS5CQXsFixBPz4bTZKG2YSv31znV0r8dGxxF13bZ2dPxw1zQbq2ncSStq2CYCEskIwUnH5Ko54bKNdmyDl+IOU1QhO/dnqp69oecv5bDjCp8MG0H7ZdHqd3owm81cx2GIhfeeW0P5O331I5h1nV2nGP1w7X/5R97eY+QMD+13Kj2HOBPhrYzFg2KQJoduJWzYAwQk6Hnjtdsbd/AJnr/kfAL3+9QadEz/FVRBsErSXl3HvuJt4+7YXmFzgqHLeAw0RA5IDZJTn81FZSp0/HokMh83Cg2u+pOX0SQAYpkn3ie+E9t86+7/8pn03vjv3WtJKgk3SX7hbhsY5DE8opdjqPGLYOj3Z5Nbxtx3zU+yjfZra7vt/0e77fzEqqy0/nncNk6wZER//+WuQ4LJSWH58rfuJLhtZbpOl+YHQ6++2W0hwWskv93Otdx2Dn3kCoxYCsL2shOazpgCcsBAF0GT+VJrMn8p5QHliMqsHDGdxdg/mG0nkFnqxGDA8roRzJ71DTuc+LG7amQW+2CpjgXqnGLhMHwvLnBSUB2eTO3RsUU2kxdqp8JsUlFe9X5Lbxm8Ll9P/xRew+vTvQ47fga6TVp+vTkPUyap+vws9ScXun0wisWgvJGTh9JYf8diMDSuOOAV6vNN61IGkQOiXfHqcjRY2L/byMnpX7GADqaFj7FaDWIeVwnIf/ZMCNFo+n4GZLWm8fB6Nl8zGFxMbOtYS8HNeyQb+TtOjXrf1tjVH3Q9w5cSXmD74Dyd139pjuSS2kG2OhCN2h+yXZJK+fF61+xzFhdw97uZKb04PhKgDbJ4KRj93M41ueoz3vI1w2S3EO6zcN/s93uj/W9YV+Lni61dJW7mI5pfdwIsNTqPc++t9vn/NYuxWusX7mZUXDMkOq8FvbLuY5spk1OofaDvl30e9f9rKRYxaeUfo9tkOJ3MuGc2+xIYMez7Y4nxVdgcmD7ueSb4U2sea7PBZybJ6uHXcLUcNSeFIzlnNiFfu50K7g0XDf8fkVn1ZsC9Q71qnY+xWPP7AEQefO6wGvRJNFhZbKfVU/T1vMeAK514uev0BdnXsydy+wyi3O2mUt4OZGR1YWECNWl16pFi4+ePHSNq0juL0xiwefCkrG7Xhgm//TtrKhZQ0bETs7h3H/Xijiasgj65fvkdX3uN3wJ7WnfDExZO5cCYQ/Ldy2v5jt3Xrw/Ieg1iZ0pyBK6fS5e//AIKf8u9u15X0FQvY07oTK3ufw+JGbVlU7jxisHLbLVzOVs5940ls5aVs6TWQFV37szS5OU0r8hjy5Zu48vfizttzAp4FEQF17QNOvq59B6wcfAkPthjK3aVL6PfB+GqP8brcjPzty5U+JT7wCeSZyX5+zKu8EN2Bgb8tE+1sKPDSO9lgR8BGisXPRcsm0/G7j9nY92z+0OFyWiXZKfBCjBW6mAXMMxP580+vk7lwJj6HE5un+paykoaNGH3pX4/6hvy16S8fta/2AZ/e9gwflSUf87iThcNqcE1gI3+nOQYGr//3OeK35jDu9peZtS/4GhrAXWVLeTulOw8s+CjUinC85l46mh0NmjDkn8/iKC7EExPLkqFX0fOzN0PH5PY8gyf7X0+W3YvFNJmW90uXWpYTyW41eHjTZDpM+oTtXXvz5dmj6LlhHj0+f6tOrleemIyjqBDDDBCwWuv8k/Fd7bvz49kj+d5sEPan+yej1kl27vnqOVz7drPg3KuYkdGRBflmaDxb43g790yfQLM5P+KJiWX52ZeRkLcbq8/D7J5DWOJswNVLvqX95E+OeI3S1DSWnH0Zs5p2Y16xlbLDfl/brQbXBDYx5O+P1kpLkxyU2+N0VnQfwJLkLJaWWmkTCw0C5fzms3HH3TVSJJqdjF37FKQ4eYPU1u59ub3HddyfN7vSm93D3X7Xu2wt8uK2WyjzBhiQ7OenPCuj7Dv4j7Mp24u9oUXsuiZbWZzn5+K4YqabyZxqKcAZ8FFic/J/z9+KYZqYhsFNt0+gg6OCFG8pO+1xXDDvSz7veSF/eub6GtX+zt0v8XWhG4fVwHPYp55JbhtvvXRNjc5TnpTKbb97jn2/gi4+g5N8dN26gr4fjufn3/6BRY3acvszo4FgIB5/20t4DAsXL/yWdt//i30t25GyYdUJr7O4YQbeuASSNq7h29GP8G6gSbXduQzAcsjiiBIZBvDHwvmc9slrkS6lzvkcThYNH8X32b3Z6rFyirWI/5XFUrK/RSY1xkaWK8CCvEBUzGxpABaLQc8kWFFioajCz6WxBTTZm8u0zM4szDPxmybnJ5aTXF7EVGcm7S3FjHr13ioT8RSnN2b+kCvZnNqES977a6XZ6Y6XJzaeFYNHMKfVaeRaYuhSvpNTZ0+i2Zwfa+0aUj3TMBRUpd44GYOUuvadxBJ2BNdZcpQffX7zZjYvW4FO8SZz90GXXeuZ7mxLQkk+beMash0rXZMMFuSZ9CrYyFp7C1JL8+ge76BhST4tNy5jbVaX0C9zwzTpY8knpryC7kt+Zm63M2n1v6/olt3lqHUcaugP/+CbU0fTJ8HPdtPJmkNmmGvnrnl/fVf+Xq7Zu5Dn3Z1rfJ9ode7Mz8ma/h8AzvjHc5wWnxTaZy8v444Xb2FPu640WjIHICIhCiBu93bYvR2A8//2F1qddRHPdbyoyniVs5J8nD33a57vfllYCzFLkMWAi2MKmWakcn7ZRhYlNmXhvuAHITOK7DXu0nqDmVMvQhQEu6r2/OwNevIGfpsNq8/HyPgk5l0winmZnRg58VkarF3GvuwOTB0yku+dTdh+yHjNtFg7t6/8mnUtOzMlpjlbCr3EOYNd42o7eMXYrdy5cxodp3yOqyAPr8vNzk69aDLvZwD6A0UZzdjasWdodqyLj3K+uJ1bGfDes7Vb5H6OkiK6ffEu3Xi3Ts4vR6YQJRLdFKROYrE7t2I1DJzlR54FC6CpJ5+ZJNG5IJe5NCZ53w6y2nYkceceWjtc/I8W9Nu1igX2NjTYt52OLZuRmrOT5ILdgEHrn76h3B1X6ZynLZvK7vSmZP/8DWVxiRimyZkfvljj2hstmcNp595Ay8Id9N67lacSD0532bo4vP70fT8Yz+R73mF53tFbpZol2Nlc6KV/sslKjz2qprNtm2QPhagDnEX5lW7bPBWhEBVN2k6ZyLOrF/PSVQ+HpqSPsVu58tMnSdq0jmdWzOOta8byU54lwpWeWBfGlzDbTGLHUULk1c7dbI1JIbtgG6Zh8C97i9AA8uuMLQx9cQxX7J8x6XyCa680XLWE3zVtybcjbuEbT3KVLleH+k1MPmePf7y2H9pJ4UB3QmdRPv3ef5F+h+xLWb+CC199gAsMgzWDLuJ/3c9muxHDnR+PJWnzetrzKcOB9acPJXlbDha/j5lDRvJjfMtjLhnQINZGsSdQbdflhrF2yn0B4hwW7vv5TZrMnxbaZy8vC4WoA+K3b6adBneLiEQtBamTmCXgJ9FtxVlafNTjmuzeAjFJtFk9D3vbJsTl7SbbLCZh304S8nZBmxY0zVlOs54dSV67jc6pW0jevZVGqxezs00X7GUldP7uw0rnbP3jRBI698IwTTp/8z5AaPG9mrpg/td4HU46TP6UJnf2DS3Kmb1peVjnMUyT66f+g3s6X3nUT42v3vgzM7NPpff6uQwuL+aRzMFRM0B96M7FkS7huCTk5nD/s//HVzeO5X1vI35jbiFp0zoAYnfv4PZnRtP96rt4I74Lg9zFrLUksDrfS/tkG6vzfVHRzSoc1XVJPXRh0eEJpfzu+Tu40uXmfyPv5qOY1uQfNnbn/MRyLn7u/krbBiWl8v2Vd1LgjGfouDFA5cUmG64KrpWTuGUDV774B4ZlNGPSpbfylb9hqPvaAYOTfPzmuT/WwqP99TJMk7ZT/n3EiTayp30X+n7oG2MYSnAa7OmnX8iPNKwyI9spKVbufOc+bOXlLDzvKlZntuW0ZVOZ1/F09thj+b/3/oyzYB8+d6wmBBAR+RVQkDrJJduNYwapxhtXQIfOJO7MpUVPG/F7t9N6zyYSdm8ladN67O0vIWHnFjpRRNK2jXT0lBO3Zwfx2zcTsyfYOnT4tMVWny80Q9Ev1XbKv/G6Y7EE/Fy6ezHjnB0AyJr/8zHuWVXzWVMY3ucCviiKrXa/1WLQ/qevaDv1W6w+L+68PVx4V28mFsVVe/yh03/XtTinlVM/fvvEXKwOWQJ+Lnz1ATr3P48m83+qsr//P1+gbftuJG9ci+H38cOoPzHolefY2Hsw47uOYHuRlxi7lVS3hUt2L+athM5VZpVsnmDnulWTeLPduaHgXZdSY4KzVjaNt1PiN9lT4qNtkp0/v3sfUy67lX+ZmZR6/bRLtnHXxGf5cvgNbLbGM/KlPwHBfzdnv/U4/ZNSmXzV3XxuZFJc4adTso3fvlQ15Ljy9zL8tYdqXF/89s1c9tK9nN8gnSmX3cpEW1Pyy3yckmLh9+PuULegOtBk/lQunz+VyyxW1gy6kGldz8Jm+mm9cz29X3gtNDth749fpff++7T7/l+VzlHbi5mKiEhkKEid5FKsPhzFhUc9JmPpXIwOlxOzZzstzRLidm2jxZr5xG/bjL2shNYJVhJzc+i4bSWJW9aTsn4lpjU4m19tTVl8JAcW5+zz8St8ePMbWAyI2z/2K1yXvvc400Y+Xe3aMh0SrVUGX1/+twdYctOr1a6JdWqSQaq/lG8K3b+olnCc5Sis0o3vZNZy6rdH3HfoTIznvjkWgDb//YJn5//MR9eP4bTl02i24Gdid++gc/NWvHX5fcwILluEAdw8/1Na/fQ1z/zvS764/i/8qyIV04T0ONsx1yarCbfdwqlxPn7Os+C2W3n0h/F43bG4iguI37aZSb/9I/2++pi43du58NUHODOzOV/95k7O/fffSV23nOuev63aweGu/L1c+OoDnNUwg+9/czuDPnqzVt9Mx+zZyfDXHmJIXAIzLrmB095854izZkrtsAT8tPvh89D4JRERqX8UpE5yKf4KnCVHD1LOonwy4+3E7N1Nmz0bidm7i6bzpoZCUs+SrThKiujyw78OvrkLHN8CjeGyeSq4LG85y9Kyf/E5YvbuYvTWGTyRfGql7U3i7ZxSvKXK8Y7SEu788Q3+eOr1VBw2cP+0HSvo86/XWX/rK6w6xtgrCE4F/EtXuh/087+OfdCvnKsgj2uev73StqRN6/jD0/9H/8tu4I1GfTjdXkirn74Ggq/dZS/dS7/ufVjTuR/9/vka31z3AB/70qt0uTuaGLuVi6y72BDTgFn7TEYXL+eM155jWL9zKU5JI33FgkrHX/jqA5VuJ2zbxMhxd1XadrRWoLjd27n4lfuPuP94OYoLGfjuM3V2fhERETlIQeokl+wpxlFUcMzjOluLsQT8dJgTnNDg0JamXjOCb05j9uysmyJr6PSPXyJu2G+P6xw9P3uDvn88lRmHrDt768J/0TBnZbXHN14wndGd+/KSvS0G0CDWzt5SL92m/At7WQl/+OxJ7j3/vkpr07jtFiyGERqTkuCyct/KL3mk9fCwFwfukGyjyfypYT/O+uTUT/9Gh4ZfYqumdTRz4cxQF9OLXvkzfdt3Z8LwW5m978hh5pQUC5fP/pw53QYx4t3HceXvBWBj37NpMeN7gFpbm0tERER+verXNFq/QqnFe2s0yUPHrcGpsqtb5DZz0fGNdaot9vIyev3rjeM+z3WfPkWc00qM3coV7jxaTvsuNPFBdQa++wxnJvtpk2Tjvrn/pFuylYRtm4DgFOP3Lv83NotBn2TomWKhd5yXezf+B6thMDjJx/V5i2k75d/cs2s6FiM40UD75Jp9RnHOjiXH/Xjrg7jd20OB52jSVi7k3qev58E908mIs1c9j9PKTf96mlY/fc1VL95d6ZwHQpSIiIhITahF6iSXtnNTjY5rN2dKHVcSPZI3ruX6wmXMbtSey56p2axl//fmn1l8zm9oMeN77li/otK+Nv/9gpuatsG1uYQuUz5je7tTyJ72HTdd05RGSzfSfvInAPT4/C1uurYBbX6eyaVb1vPw1X9l01GmSo5x/DommYhG3Se+w/Mxn/DdNX/m40AGFb4ATeLtXL5jXsTW3xIREZFfF8M0Na1TTVcvPhF8ubnsPK1PjY/f1b5bta1MUrmrVm04sMBnTeW1aM1DFz3I9iOsI3R+YjnXPXdrbZUnR7CvZTtmnX0FQ98Yo1nsREREolTio2OJu+7aSJcB1DwbqGvfSS513YpjH1RP1XZXrXBCFARbxv4y6TkaxFbf8HvmrC9royw5hpQNqzjvb48oRImIiEitUpA6ydX19ORyfBquWsKYH18l2W2jQYyNq1zBMTktE+1kTf9PhKsTERERkV9KY6RE6lijJXMYY7OztMdZnPvmWIxbniC2oCTSZYmIiIjIcVCQEjkBGi+YTuMF0wEY8cr9+BzOCFckIiIiIsdDXftEIsDmqYh0CSIiIiJyHBSkREREREREwqQgJSIiIiIiEiYFKRERERERkTApSImIiIiIiIRJQUpERERERCRMClIiIiIiIiJhUpASEREREREJk4KUiIiIiIhImBSkREREREREwqQgJSIiIiIiEiYFKRERERERkTApSImIiIiIiIRJQUpERERERCRMClIiIiIiIiJhUpASEREREREJk4KUiIiIiIhImBSkREREREREwqQgJSIiIiIiEiYFKRERERERkTApSImIiIiIiIRJQUpERERERCRMClIiIiIiIiJhUpASEREREREJk4KUiIiIiIhImBSkREREREREwqQgJSIiIiIiEqaoDlJPPPEEvXr1Ij4+nrS0NC666CJWr15d6Zjy8nJuueUWUlNTiYuL45JLLmHnzp0RqlhEREREROqDqA5SP/30E7fccguzZs3i+++/x+v1cs4551BSUhI65q677uKrr77i008/5aeffmLbtm2MGDEiglWLiIiIiMivnS3SBRzNpEmTKt2eMGECaWlpzJ8/nzPOOIOCggLeeustPvjgAwYNGgTAO++8Q/v27Zk1axa9e/eORNkiIiIiIvIrF9UtUocrKCgAICUlBYD58+fj9XoZPHhw6Jh27drRrFkzZs6cecTzVFRUUFhYWOlLRERERESkpk6aIBUIBLjzzjvp168fnTp1AmDHjh04HA6SkpIqHZuens6OHTuOeK4nnniCxMTE0FfTpk3rsnQREREREfmVOWmC1C233MKyZcv46KOPjvtc999/PwUFBaGvLVu21EKFIiIiIiJSX0T1GKkDbr31Vr7++mt+/vlnmjRpEtreqFEjPB4P+fn5lVqldu7cSaNGjY54PqfTidPprMuSRURERETkVyyqW6RM0+TWW2/l3//+N//973/JysqqtL9Hjx7Y7XamTJkS2rZ69Wo2b95Mnz59TnS5IiIiIiJST0R1i9Qtt9zCBx98wBdffEF8fHxo3FNiYiJut5vExESuv/567r77blJSUkhISOC2226jT58+mrFPRERERETqTFQHqddeew2AgQMHVtr+zjvvcM011wDwwgsvYLFYuOSSS6ioqGDIkCG8+uqrJ7hSERERERGpT6I6SJmmecxjXC4Xr7zyCq+88soJqEhERERERCTKx0iJiIiIiIhEIwUpERERERGRMClIiYiIiIiIhElBSkREREREJEwKUiIiIiIiImFSkBIREREREQmTgpSIiIiIiEiYFKRERERERETCpCAlIiIiIiISJgUpERERERGRMClIiYiIiIiIhElBSkREREREJEwKUiIiIiIiImFSkBIREREREQmTgpSIiIiIiEiYFKRERERERETCpCAlIiIiIiISJgUpERERERGRMClIiYiIiIiIhElBSkREREREJEwKUiIiIiIiImFSkBIREREREQmTgpSIiIiIiEiYFKRERERERETCpCAlIiIiIiISJgUpERERERGRMClIiYiIiIiIhElBSkREREREJEwKUiIiIiIiImFSkBIREREREQmTgpSIiIiIiEiYFKRERERERETCpCAlIiIiIiISJgUpERERERGRMClIiYiIiIiIhElBSkREREREJEwKUiIiIiIiImFSkBIREREREQmTgpSIiIiIiEiYFKRERERERETCpCAlIiIiIiISJgUpERERERGRMClIiYiIiIiIhElBSkREREREJEwKUiIiIiIiImFSkBIREREREQmTgpSIiIiIiEiYFKRERERERETCpCAlIiIiIiISJgUpERERERGRMClIiYiIiIiIhElBSkREREREJEwKUiIiIiIiImFSkBIREREREQmTgpSIiIiIiEiYFKRERERERETCpCAlIiIiIiISJgUpEREREfn/9u49KMry/eP4B0QWGERIEsTAQzaaeUgkFbX8Q4zKytIsGyrUorF00mg85ahpmWYzTYdRsya1mUzLb0nWdHIwMxsURSTxQJaajQqWhuAhQPb6/eH05Ar57fn+lAV8v2Z2xn3u+1mve/gMuxfP7r0AXKKRAgAAAACXaKQAAAAAwCUaKQAAAABwiUYKAAAAAFyikQIAAAAAl2ikAAAAAMAlGikAAAAAcIlGCgAAAABcopECAAAAAJdopAAAAADAJRopAAAAAHCJRgoAAAAAXKKRAgAAAACXaKQAAAAAwCUaKQAAAABwiUYKAAAAAFyikQIAAAAAl2ikAAAAAMAlGikAAAAAcKnRNFILFixQ27ZtFRISot69eys3N9ffJQEAAABopBpFI/XBBx8oMzNTM2fO1LZt29S9e3elpqbq6NGj/i4NAAAAQCPUKBqpV155RRkZGRo1apQ6d+6sN998U2FhYVqyZIm/SwMAAADQCAX5u4D/r8rKSuXl5Wnq1KnOscDAQKWkpCgnJ6fWcyoqKlRRUeHcP3HihCSprKzs8hb7L1RXVamyb7K/ywAAAADqzKmoSHnrwWtx6e+ewMwuOq/BN1K///67qqurFRMT43M8JiZGe/bsqfWcuXPnatasWTWOx8fHX5YaAQAAAFzEf1b5u4IaysvL1bx5838cb/CN1P9i6tSpyszMdO57vV4dP35cLVq0UEBAgB8rO9cBx8fH69dff1VERIRfa0HDQGbgFpmBW2QGbpEZuFWfMmNmKi8vV1xc3EXnNfhGKjo6Wk2aNFFJSYnP8ZKSEsXGxtZ6jsfjkcfj8TkWGRl5uUr8n0RERPg9RGhYyAzcIjNwi8zALTIDt+pLZi52JeovDX6zieDgYPXs2VPZ2dnOMa/Xq+zsbCUn81kjAAAAAJdeg78iJUmZmZlKT09XUlKSevXqpVdffVWnTp3SqFGj/F0aAAAAgEaoUTRSDzzwgH777TfNmDFDxcXFuvHGG/Xll1/W2ICiIfB4PJo5c2aNtx4C/4TMwC0yA7fIDNwiM3CrIWYmwP7bvn4AAAAAAB8N/jNSAAAAAFDXaKQAAAAAwCUaKQAAAABwiUYKAAAAAFyikapHFixYoLZt2yokJES9e/dWbm6uv0uCn8ydO1c33XSTmjVrppYtW+qee+5RUVGRz5w///xTY8eOVYsWLRQeHq5hw4bV+GLqgwcPavDgwQoLC1PLli01ceJEnT17ti6XAj+ZN2+eAgICNGHCBOcYmcGFDh06pIceekgtWrRQaGiounbtqq1btzrjZqYZM2aoVatWCg0NVUpKivbu3evzGMePH1daWpoiIiIUGRmpRx99VCdPnqzrpaAOVFdXa/r06WrXrp1CQ0N17bXX6vnnn9f5+5aRmSvbhg0bdNdddykuLk4BAQHKysryGb9U+fjhhx908803KyQkRPHx8Zo/f/7lXlrtDPXCypUrLTg42JYsWWI7d+60jIwMi4yMtJKSEn+XBj9ITU21pUuXWmFhoW3fvt3uuOMOS0hIsJMnTzpzxowZY/Hx8ZadnW1bt261Pn36WN++fZ3xs2fPWpcuXSwlJcXy8/Pt888/t+joaJs6dao/loQ6lJuba23btrVu3brZ+PHjneNkBuc7fvy4tWnTxkaOHGmbN2+2ffv22VdffWU//fSTM2fevHnWvHlzy8rKsoKCArv77rutXbt2dubMGWfObbfdZt27d7dNmzbZd999Zx06dLAHH3zQH0vCZTZnzhxr0aKFffbZZ7Z//35btWqVhYeH22uvvebMITNXts8//9ymTZtmH3/8sUmy1atX+4xfinycOHHCYmJiLC0tzQoLC23FihUWGhpqixcvrqtlOmik6olevXrZ2LFjnfvV1dUWFxdnc+fO9WNVqC+OHj1qkuzbb781M7PS0lJr2rSprVq1ypmze/duk2Q5OTlmdu6XWWBgoBUXFztzFi1aZBEREVZRUVG3C0CdKS8vt+uuu87Wrl1rAwYMcBopMoMLTZ482fr37/+P416v12JjY+3ll192jpWWlprH47EVK1aYmdmuXbtMkm3ZssWZ88UXX1hAQIAdOnTo8hUPvxg8eLCNHj3a59jQoUMtLS3NzMgMfF3YSF2qfCxcuNCioqJ8npcmT55sHTt2vMwrqom39tUDlZWVysvLU0pKinMsMDBQKSkpysnJ8WNlqC9OnDghSbrqqqskSXl5eaqqqvLJTKdOnZSQkOBkJicnR127dvX5YurU1FSVlZVp586ddVg96tLYsWM1ePBgn2xIZAY1rVmzRklJSRo+fLhatmypHj166O2333bG9+/fr+LiYp/MNG/eXL179/bJTGRkpJKSkpw5KSkpCgwM1ObNm+tuMagTffv2VXZ2tn788UdJUkFBgTZu3Kjbb79dEpnBxV2qfOTk5OiWW25RcHCwMyc1NVVFRUX6448/6mg15wTV6f+GWv3++++qrq72efEiSTExMdqzZ4+fqkJ94fV6NWHCBPXr109dunSRJBUXFys4OFiRkZE+c2NiYlRcXOzMqS1Tf42h8Vm5cqW2bdumLVu21BgjM7jQvn37tGjRImVmZurZZ5/Vli1b9NRTTyk4OFjp6enOz7y2TJyfmZYtW/qMBwUF6aqrriIzjdCUKVNUVlamTp06qUmTJqqurtacOXOUlpYmSWQGF3Wp8lFcXKx27drVeIy/xqKioi5L/bWhkQLqubFjx6qwsFAbN270dymox3799VeNHz9ea9euVUhIiL/LQQPg9XqVlJSkF198UZLUo0cPFRYW6s0331R6erqfq0N99OGHH2r58uV6//33dcMNN2j79u2aMGGC4uLiyAyuSLy1rx6Ijo5WkyZNauyeVVJSotjYWD9Vhfpg3Lhx+uyzz/TNN9/ommuucY7HxsaqsrJSpaWlPvPPz0xsbGytmfprDI1LXl6ejh49qsTERAUFBSkoKEjffvutXn/9dQUFBSkmJobMwEerVq3UuXNnn2PXX3+9Dh48KOnvn/nFnptiY2N19OhRn/GzZ8/q+PHjZKYRmjhxoqZMmaIRI0aoa9euevjhh/X0009r7ty5ksgMLu5S5aM+PVfRSNUDwcHB6tmzp7Kzs51jXq9X2dnZSk5O9mNl8Bcz07hx47R69WqtW7euxiXsnj17qmnTpj6ZKSoq0sGDB53MJCcna8eOHT6/kNauXauIiIgaL57Q8A0cOFA7duzQ9u3bnVtSUpLS0tKcf5MZnK9fv341vlbhxx9/VJs2bSRJ7dq1U2xsrE9mysrKtHnzZp/MlJaWKi8vz5mzbt06eb1e9e7duw5Wgbp0+vRpBQb6vnRs0qSJvF6vJDKDi7tU+UhOTtaGDRtUVVXlzFm7dq06duxYp2/rk8T25/XFypUrzePx2LJly2zXrl32+OOPW2RkpM/uWbhyPPHEE9a8eXNbv369HTlyxLmdPn3amTNmzBhLSEiwdevW2datWy05OdmSk5Od8b+2sr711ltt+/bt9uWXX9rVV1/NVtZXkPN37TMjM/CVm5trQUFBNmfOHNu7d68tX77cwsLC7L333nPmzJs3zyIjI+2TTz6xH374wYYMGVLrVsU9evSwzZs328aNG+26665jK+tGKj093Vq3bu1sf/7xxx9bdHS0TZo0yZlDZq5s5eXllp+fb/n5+SbJXnnlFcvPz7dffvnFzC5NPkpLSy0mJsYefvhhKywstJUrV1pYWBjbn1/p3njjDUtISLDg4GDr1auXbdq0yd8lwU8k1XpbunSpM+fMmTP25JNPWlRUlIWFhdm9995rR44c8XmcAwcO2O23326hoaEWHR1tzzzzjFVVVdXxauAvFzZSZAYX+vTTT61Lly7m8XisU6dO9tZbb/mMe71emz59usXExJjH47GBAwdaUVGRz5xjx47Zgw8+aOHh4RYREWGjRo2y8vLyulwG6khZWZmNHz/eEhISLCQkxNq3b2/Tpk3z2YaazFzZvvnmm1pfv6Snp5vZpctHQUGB9e/f3zwej7Vu3drmzZtXV0v0EWB23tdRAwAAAAD+Kz4jBQAAAAAu0UgBAAAAgEs0UgAAAADgEo0UAAAAALhEIwUAAAAALtFIAQAAAIBLNFIAAAAA4BKNFAAAAAC4RCMFAAAAAC7RSAEAGp3ffvtNTzzxhBISEuTxeBQbG6vU1FR9//33kqSAgABlZWX5t0gAQIMW5O8CAAC41IYNG6bKykq9++67at++vUpKSpSdna1jx475uzQAQCMRYGbm7yIAALhUSktLFRUVpfXr12vAgAE1xtu2batffvnFud+mTRsdOHBAkvTJJ59o1qxZ2rVrl+Li4pSenq5p06YpKOjc3x0DAgK0cOFCrVmzRuvXr1erVq00f/583XfffXWyNgBA/cFb+wAAjUp4eLjCw8OVlZWlioqKGuNbtmyRJC1dulRHjhxx7n/33Xd65JFHNH78eO3atUuLFy/WsmXLNGfOHJ/zp0+frmHDhqmgoEBpaWkaMWKEdu/effkXBgCoV7giBQBodD766CNlZGTozJkzSkxM1IABAzRixAh169ZN0rkrS6tXr9Y999zjnJOSkqKBAwdq6tSpzrH33ntPkyZN0uHDh53zxowZo0WLFjlz+vTpo8TERC1cuLBuFgcAqBe4IgUAaHSGDRumw4cPa82aNbrtttu0fv16JSYmatmyZf94TkFBgWbPnu1c0QoPD1dGRoaOHDmi06dPO/OSk5N9zktOTuaKFABcgdhsAgDQKIWEhGjQoEEaNGiQpk+frscee0wzZ87UyJEja51/8uRJzZo1S0OHDq31sQAAOB9XpAAAV4TOnTvr1KlTkqSmTZuqurraZzwxMVFFRUXq0KFDjVtg4N9Pl5s2bfI5b9OmTbr++usv/wIAAPUKV6QAAI3KsWPHNHz4cI0ePVrdunVTs2bNtHXrVs2fP19DhgyRdG7nvuzsbPXr108ej0dRUVGaMWOG7rzzTiUkJOi+++5TYGCgCgoKVFhYqBdeeMF5/FWrVikpKUn9+/fX8uXLlZubq3feecdfywUA+AmbTQAAGpWKigo999xz+vrrr/Xzzz+rqqpK8fHxGj58uJ599lmFhobq008/VWZmpg4cOKDWrVs7259/9dVXmj17tvLz89W0aVN16tRJjz32mDIyMiSd22xiwYIFysrK0oYNG9SqVSu99NJLuv/++/24YgCAP9BIAQDwL9W22x8A4MrEZ6QAAAAAwCUaKQAAAABwic0mAAD4l3g3PADgL1yRAgAAAACXaKQAAAAAwCUaKQAAAABwiUYKAAAAAFyikQIAAAAAl2ikAAAAAMAlGikAAAAAcIlGCgAAAABc+j9GpYHlvf7fDgAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "actions_df = pd.DataFrame(actions_taken, columns=['action'])\n",
        "actions_df['step'] = actions_df.index\n",
        "\n",
        "# Count the occurrences of each action at each step\n",
        "actions_count_df = actions_df.groupby(['step', 'action']).size().unstack(fill_value=0)\n",
        "\n",
        "# Calculate the cumulative sum to get the traffic over time\n",
        "cumulative_counts = actions_count_df.cumsum()\n",
        "\n",
        "# Calculate the percentage of traffic allocated to each variant\n",
        "cumulative_percents = cumulative_counts.div(cumulative_counts.sum(axis=1), axis=0) * 100\n",
        "\n",
        "cumulative_percents.columns = data['variant'].unique()\n",
        "print(cumulative_percents.tail(1), )\n",
        "# Plotting the area chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.stackplot(cumulative_percents.index, cumulative_percents.T, labels=cumulative_percents.columns, colors=['#e41a1c', '#377eb8', '#4daf4a'])\n",
        "plt.legend(loc='upper left')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('% of Traffic')\n",
        "plt.title('Bandit Selection')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gxE2om7TX2o"
      },
      "outputs": [],
      "source": [
        "def simulate_clicks(row):\n",
        "    visits = int(row['visits'])\n",
        "    clicks = int(row['clicks'])\n",
        "    if visits == 0:\n",
        "        probability\n",
        "    else:\n",
        "        probability = clicks / visits\n",
        "    # Bernoulliho schma\n",
        "    clicks = np.random.binomial(1, probability, visits)\n",
        "    return clicks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdEm5o6dUZsN"
      },
      "source": [
        "Add rewards simulation"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
